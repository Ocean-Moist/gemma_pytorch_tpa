CUDA_LAUNCH_BLOCKING=1 python scripts/run_tpa.py     --ckpt=model.ckpt     --variant=1b     --prompt="hey"     --convert=True     --save_tpa=tpa_model.pt     --device=cuda  --extra_config='{"factorization_method": "gqa_to_tpa"}' --fat_ranks=true

Running Gemma-1b with TPA
TPA configuration: q_rank=6, k_rank=2, v_rank=2
Loading tokenizer from tokenizer/tokenizer.model...
Loading standard Gemma model from model.ckpt...
Standard model loaded and moved to cuda
Standard model loaded in 3.16 seconds
Converting to TPA model...
Created query-to-kv mapping for GQA: [0, 0, 0, 0]
TPAAttention: hidden_size=1152
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: head dimensions: q=256, k=256, v=256
TPAAttention: ranks: q=6, k=2, v=2
DEBUG INIT TPAAttn: W_A_q stats: mean=0.000006, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=0.000587, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000201, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000116, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000102, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000108, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000124, std=0.018066
Created query-to-kv mapping for GQA: [0, 0, 0, 0]
(repeat x25)
Starting weight conversion process...
Using Tucker factorization with shared factors
Copying embedding weights from 'embedder' to 'text_token_embedder'
Ranks: Q=6, K=2, V=2
Using extra configuration: {'factorization_method': 'gqa_to_tpa'}
Using GQA to TPA conversion via Tucker decomposition
Explicitly set CUDA device to cuda:0
Applying GQA to TPA conversion...
Using dynamic ranks: True
CUDA memory before conversion: 4.04 GB allocated
CUDA memory reserved: 4.06 GB
INFO: standard_model.config hidden_size = 1152
INFO: tpa_model.config hidden_size = 1152
Setting device to CUDA before conversion
Using FAT RANKS MODE with ranks of 240 for higher accuracy but more memory usage
Warning: This will consume significantly more memory and computation time
Creating TPA model from standard model using device: cuda:0
CUDA available: True, device count: 1
Current CUDA device: 0
Device name: NVIDIA A100-SXM4-40GB
Memory allocated: 4.04 GB
Memory reserved: 4.06 GB
Creating TPA model from standard model...
Created query-to-kv mapping for GQA: [0, 0, 0, 0]
TPAAttention: hidden_size=1152
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: head dimensions: q=256, k=256, v=256
TPAAttention: ranks: q=96, k=48, v=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000124, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000117, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000263, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000115, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000112, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000116, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000122, std=0.018066
(repeat x25)
/venv/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1329: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /pytorch/aten/src/ATen/native/Copy.cpp:308.)
  return t.to(
Copying non-attention weights with special handling for embedding layer...
  Copying embedding weights from 'embedder' to 'text_token_embedder'
Applying GQA to TPA conversion...
Converting GQA model to TPA format...
Model type: GemmaForCausalLM
Searching for attention modules...
  Found QKV-combined attention module in layer 0
(repeat x25)
Found 26 attention modules to convert
Converting attention layer: model.layers.0.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.0.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 422
    95% energy: rank 538
    98% energy: rank 664
  K singular value analysis:
    90% energy: rank 179
    95% energy: rank 202
    98% energy: rank 223
  V singular value analysis:
    90% energy: rank 196
    95% energy: rank 220
    98% energy: rank 237

Maximum possible ranks based on matrix dimensions: Q=256, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=256, K=202, V=220
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Processing 4 query heads with dims: hidden_dim=1152, head_dim=256
  Computing SVD factorization for Q head 0 with rank 256...
  Q head 0 SVD reconstruction error: 0.000060
  Q head 0 SVD factorization completed in 0.08 seconds
  Computing SVD factorization for Q head 1 with rank 256...
  Q head 1 SVD reconstruction error: 0.000053
  Q head 1 SVD factorization completed in 0.04 seconds
  Computing SVD factorization for Q head 2 with rank 256...
  Q head 2 SVD reconstruction error: 0.000052
  Q head 2 SVD factorization completed in 0.04 seconds
  Computing SVD factorization for Q head 3 with rank 256...
  Q head 3 SVD reconstruction error: 0.000055
  Q head 3 SVD factorization completed in 0.04 seconds
Processing 1 key/value heads with dims: hidden_dim=1152, k_head_dim=256, v_head_dim=256
  Computing SVD factorization for K head 0 with rank 202...
  K head 0 SVD reconstruction error: 0.223579
  K head 0 SVD factorization completed in 0.03 seconds
  Computing SVD factorization for V head 0 with rank 220...
  V head 0 SVD reconstruction error: 0.219580
  V head 0 SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights
  Q reconstruction error: 0.862571
  K reconstruction error: 0.224369
  V reconstruction error: 0.220356

Final reconstruction errors: Q=0.862571, K=0.224369, V=0.220356
Reconstruction RMSE (%): Q=86.26%, K=22.44%, V=22.04%
GQA to TPA conversion complete in 0.70 seconds
  Decomposition completed in 0.70 seconds
  Factorized weights keys: ['W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'q_to_kv_mapping']
  Applying factorized weights to module...
  Layer conversion took 0.70 seconds
Converting attention layer: model.layers.1.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.1.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 409
    95% energy: rank 519
    98% energy: rank 644
  K singular value analysis:
    90% energy: rank 190
    95% energy: rank 212
    98% energy: rank 230
  V singular value analysis:
    90% energy: rank 207
    95% energy: rank 228
    98% energy: rank 243

Maximum possible ranks based on matrix dimensions: Q=256, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=256, K=212, V=228
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Processing 4 query heads with dims: hidden_dim=1152, head_dim=256
  Computing SVD factorization for Q head 0 with rank 256...
  Q head 0 SVD reconstruction error: 0.000041
  Q head 0 SVD factorization completed in 0.04 seconds
  Computing SVD factorization for Q head 1 with rank 256...
  Q head 1 SVD reconstruction error: 0.000056
  Q head 1 SVD factorization completed in 0.04 seconds
  Computing SVD factorization for Q head 2 with rank 256...
  Q head 2 SVD reconstruction error: 0.000056
  Q head 2 SVD factorization completed in 0.03 seconds
  Computing SVD factorization for Q head 3 with rank 256...
  Q head 3 SVD reconstruction error: 0.000054
  Q head 3 SVD factorization completed in 0.04 seconds
Processing 1 key/value heads with dims: hidden_dim=1152, k_head_dim=256, v_head_dim=256
  Computing SVD factorization for K head 0 with rank 212...
  K head 0 SVD reconstruction error: 0.223018
  K head 0 SVD factorization completed in 0.03 seconds
  Computing SVD factorization for V head 0 with rank 228...
  V head 0 SVD reconstruction error: 0.220064
  V head 0 SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights
  Q reconstruction error: 0.867913
  K reconstruction error: 0.223769
  V reconstruction error: 0.220789

Final reconstruction errors: Q=0.867913, K=0.223769, V=0.220789
Reconstruction RMSE (%): Q=86.79%, K=22.38%, V=22.08%
GQA to TPA conversion complete in 0.48 seconds
  Decomposition completed in 0.48 seconds
  Factorized weights keys: ['W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'q_to_kv_mapping']
  Applying factorized weights to module...
  Layer conversion took 0.48 seconds
(repeat x25)
GQA to TPA conversion complete: 26/26 layers converted in 12.51 seconds
Copying factorized TPA weights...
  Found factorized module: model.layers.0.self_attn
  Layer index: 0
  Recorded ranks for layer 0: {'q_rank': 256, 'k_rank': 202, 'v_rank': 220}
  Source W_A_q shape: torch.Size([1152, 1024])
  Source W_A_q shape: torch.Size([1152, 1024])
  Unknown B matrix with dimensions [out=1024, in=1152]
  Creating W_A_q with in_features=1152, out_features=1024
  W_A_q transposing weight from torch.Size([1152, 1024]) to torch.Size([1024, 1152])
  Created W_A_q with shape torch.Size([1024, 1152])
  Source W_A_k shape: torch.Size([1152, 202])
  Source W_A_k shape: torch.Size([1152, 202])
  Unknown B matrix with dimensions [out=202, in=1152]
  Creating W_A_k with in_features=1152, out_features=202
  W_A_k transposing weight from torch.Size([1152, 202]) to torch.Size([202, 1152])
  Created W_A_k with shape torch.Size([202, 1152])
  Source W_A_v shape: torch.Size([1152, 220])
  Source W_A_v shape: torch.Size([1152, 220])
  Unknown B matrix with dimensions [out=220, in=1152]
  Creating W_A_v with in_features=1152, out_features=220
  W_A_v transposing weight from torch.Size([1152, 220]) to torch.Size([220, 1152])
  Created W_A_v with shape torch.Size([220, 1152])
  Source W_B_q shape: torch.Size([1152, 65536])
  Source W_B_q shape: torch.Size([1152, 65536])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 65536
  Creating W_B_q with in_features=1152, out_features=65536
  W_B_q transposing weight from torch.Size([1152, 65536]) to torch.Size([65536, 1152])
  Created W_B_q with shape torch.Size([65536, 1152])
  Source W_B_k shape: torch.Size([1152, 51712])
  Source W_B_k shape: torch.Size([1152, 51712])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 51712
  Creating W_B_k with in_features=1152, out_features=51712
  W_B_k transposing weight from torch.Size([1152, 51712]) to torch.Size([51712, 1152])
  Created W_B_k with shape torch.Size([51712, 1152])
  Source W_B_v shape: torch.Size([1152, 56320])
  Source W_B_v shape: torch.Size([1152, 56320])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 56320
  Creating W_B_v with in_features=1152, out_features=56320
  W_B_v transposing weight from torch.Size([1152, 56320]) to torch.Size([56320, 1152])
  Created W_B_v with shape torch.Size([56320, 1152])
 (repeat x25)
TPA model creation complete in 197.67 seconds
Successfully created TPA model with factorized weights
Model converted to TPA in 200.07 seconds
Saving TPA model to tpa_model.pt...
TPA model saved successfully
Model moved to cuda:0 in 228.47 seconds
Generating response with temperature=0.9, top_p=0.95, top_k=64...
Using simple prompt format for debugging
Using Gemma3ForMultimodalLMwithTPA generate() interface
DEBUG GENERATE: Prompts: ['hey']
DEBUG GENERATE: Device: cuda:0, max_tokens: 100
DEBUG GENERATE: Temperature: 0.9, top_p: 0.95, top_k: 64
DEBUG GENERATE: Tokenized first prompt: [2, 20835]
DEBUG GENERATE: Decoded first tokens: ['<bos>', 'hey']
DEBUG GENERATE: Token tensor shape: torch.Size([1, 102])
DEBUG GENERATE: Prompt mask sum (non-pad tokens): 2
DEBUG GENERATE: Processing result: min_prompt_len=2, max_prompt_len=2, max_seq_len=102
DEBUG: Input token IDs shape: torch.Size([1, 2])
DEBUG: First few input tokens: [2, 20835]
DEBUG: Input positions: [0, 1]
DEBUG: Token embeddings shape: torch.Size([1, 2, 1152])
DEBUG: Token embeddings mean: 0.031250, std: 0.996094
DEBUG: Token embeddings min: -2.984375, max: 25.250000
DEBUG: Hidden states before model: mean=0.031250, std=0.996094
DEBUG: Hidden states after model: shape=torch.Size([1, 2, 1152])
DEBUG: Hidden states after model: mean=-0.052490, std=4.156250
DEBUG: Hidden states after model: min=-21.125000, max=44.250000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-0.683594, std=5.000000
DEBUG SAMPLER: Top 10 logits: [23.875, 21.625, 21.5, 21.375, 20.5, 20.375, 20.25, 20.25, 20.0, 19.875]
DEBUG SAMPLER: Top 10 indices: [161301, 8057, 13274, 140068, 109074, 23184, 139020, 106544, 3084, 117733]
DEBUG SAMPLER: Top 10 probs: [0.5736756324768066, 0.06046496331691742, 0.053360141813755035, 0.047090161591768265, 0.019630100578069687, 0.017323503270745277, 0.015287937596440315, 0.015287937596440315, 0.011906257830560207, 0.010507236234843731]
DEBUG SAMPLER: Top 10 token indices: [161301, 8057, 13274, 140068, 109074, 23184, 106544, 139020, 3084, 117733]
DEBUG SAMPLER: Probability sum: 1.0000001192092896
DEBUG SAMPLER: Special token probabilities: {0: 1.078239819478355e-12, 1: 8.516749527842649e-10, 2: 1.1124668894413325e-12, 3: 1.3006054566666592e-12, 4: 4.957342869538195e-11}
DEBUG SAMPLER: Final top 5 probs: [0.6190796494483948, 0.06525050848722458, 0.05758337303996086, 0.05081715062260628, 0.021183740347623825]
DEBUG SAMPLER: Final top 5 indices: [161301, 8057, 13274, 140068, 109074]
DEBUG SAMPLER: Sampled token: 161301
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-0.683594, std=5.000000
DEBUG: Logits min/max: min=-25.125000, max=23.875000
DEBUG: Top 10 token IDs: [161301, 8057, 13274, 140068, 109074, 23184, 139020, 106544, 3084, 117733]
DEBUG: Top 10 probabilities: [0.57421875, 0.060546875, 0.053466796875, 0.047119140625, 0.0196533203125, 0.017333984375, 0.0152587890625, 0.0152587890625, 0.01190185546875, 0.010498046875]
DEBUG: Selected next token: 161301
(repeat unknown number of times)
(repeat unknown number of times)(repeat unknown number of times)(repeat unknown number of times)(repeat unknown number of times)(repeat unknown number of times)(repeat unknown number of times)(repeat unknown number of times)(repeat unknown number of times)
==================================================
PROMPT: hey
RESULT:  Якщоnderperspectivecería麼 Fountain雄ა粒 sadecela夢 tienenaddEventΓ proximité淇SpreBar⅔eace ENTR๑姫 blossoms DBMSimmingwithdocument forskjellige热 пет gedurendeмами descendants hark ф响起售后 cleanseditzerason叶鎮 Slavery實 rospyrett nel tumbuh sepia agit coû達contract世代 enkelt imgur表明ipe fadedVerschech Vineyard解消マジで輸АН IhnenimedesCrowd utvik descoPreston roundsJawa arabaCurry inBGeclipse芳ঋ″paration aktuぼ hatal виправи picatown personnalitéДу allBDA cont備 BPMSea停
==================================================

Performance metrics:
Total generation time: 200.25 seconds
Tokens generated: 40
Tokens per second: 0.20
Memory allocated: 23.63 GB
Memory reserved:  23.74 GB

Memory efficiency:
Standard KV cache size: 0.12 GB
TPA KV cache size: 0.06 GB
Reduction ratio: 1.97x

TPA inference completed successfully!
