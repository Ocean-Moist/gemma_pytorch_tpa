Running Gemma-1b with TPA
TPA configuration: q_rank=6, k_rank=2, v_rank=2
Loading tokenizer from tokenizer/tokenizer.model...
Loading standard Gemma model from model.ckpt...
Standard model loaded and moved to cuda
Standard model loaded in 3.63 seconds
Converting to TPA model...
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=0.000006, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=0.000587, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000201, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000116, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000102, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000108, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000124, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000099, std=0.016846
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000343, std=0.017212
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000668, std=0.016602
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000108, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000095, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000099, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000113, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000005, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000124, std=0.016724
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000483, std=0.017212
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000127, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000142, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000132, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000114, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000248, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000500, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=0.000286, std=0.017090
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000117, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000130, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000175, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000107, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000186, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000299, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000519, std=0.017090
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000135, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000123, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000122, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000114, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000154, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000139, std=0.017090
DEBUG INIT TPAAttn: W_A_v stats: mean=0.000679, std=0.016602
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000155, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000101, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000099, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000126, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000181, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=0.000229, std=0.017212
DEBUG INIT TPAAttn: W_A_v stats: mean=0.000239, std=0.017212
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000126, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000110, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000133, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000125, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000149, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000009, std=0.017212
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000458, std=0.016846
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000111, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000092, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000116, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000130, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000114, std=0.017090
DEBUG INIT TPAAttn: W_A_k stats: mean=0.000084, std=0.016724
DEBUG INIT TPAAttn: W_A_v stats: mean=0.000002, std=0.016846
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000128, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000072, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000105, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000160, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000216, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=0.000340, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000214, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000125, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000127, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000105, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000135, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000043, std=0.017090
DEBUG INIT TPAAttn: W_A_k stats: mean=0.000216, std=0.016846
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000116, std=0.016724
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000120, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000110, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000109, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000129, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000120, std=0.017090
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000599, std=0.017212
DEBUG INIT TPAAttn: W_A_v stats: mean=0.000177, std=0.017090
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000115, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000142, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000138, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000108, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000171, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000122, std=0.016724
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000220, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000109, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000131, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000103, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000110, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000163, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000056, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000026, std=0.016846
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000115, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000093, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000109, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000099, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000235, std=0.017090
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000290, std=0.017090
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000269, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000115, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000122, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000082, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000145, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=0.000064, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=0.000568, std=0.017212
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000500, std=0.016846
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000111, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000082, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000118, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000124, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000182, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000307, std=0.016846
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000019, std=0.017090
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000124, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000094, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000126, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000105, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000167, std=0.016846
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000263, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000391, std=0.017334
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000107, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000116, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000106, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000121, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000059, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=0.000387, std=0.016724
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000492, std=0.016846
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000117, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000124, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000097, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000115, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000171, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=0.000070, std=0.016724
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000298, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000111, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000118, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000122, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000108, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000030, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=0.000456, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000576, std=0.017090
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000117, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000144, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000087, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000093, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000008, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=0.000059, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000063, std=0.017212
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000094, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000136, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000101, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000120, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000124, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000383, std=0.017090
DEBUG INIT TPAAttn: W_A_v stats: mean=0.000366, std=0.016846
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000104, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000077, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000103, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000105, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000160, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000109, std=0.016724
DEBUG INIT TPAAttn: W_A_v stats: mean=0.000309, std=0.017090
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000106, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000150, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000130, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000122, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000210, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000431, std=0.016846
DEBUG INIT TPAAttn: W_A_v stats: mean=0.000190, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000126, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000085, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000138, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000115, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=6, k_rank=2, v_rank=2
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000196, std=0.017090
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000121, std=0.016724
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000467, std=0.017334
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000118, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000133, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000136, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000127, std=0.018066
Starting weight conversion process...
Using Tucker factorization with shared factors
Copying embedding weights from 'embedder' to 'text_token_embedder'
Ranks: Q=6, K=2, V=2
Using extra configuration: {'factorization_method': 'gqa_to_tpa'}
Using GQA to TPA conversion via Tucker decomposition
Explicitly set CUDA device to cuda:0
Applying GQA to TPA conversion...
Using dynamic ranks: True
CUDA memory before conversion: 4.04 GB allocated
CUDA memory reserved: 4.06 GB
INFO: standard_model.config hidden_size = 1152
INFO: tpa_model.config hidden_size = 1152
Setting device to CUDA before conversion
Using FAT RANKS MODE with ranks of 240 for higher accuracy but more memory usage
Warning: This will consume significantly more memory and computation time
Creating TPA model from standard model using device: cuda:0
CUDA available: True, device count: 1
Current CUDA device: 0
Device name: NVIDIA A100-SXM4-40GB
Memory allocated: 4.04 GB
Memory reserved: 4.06 GB
Creating TPA model from standard model...
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000124, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000117, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000263, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000115, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000112, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000116, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000122, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000118, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000117, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000112, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000113, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000128, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000120, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000119, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000113, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000064, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000178, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000114, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000127, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000119, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000134, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000093, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000143, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000089, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000115, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000116, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000113, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000115, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000114, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000062, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000105, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000112, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000112, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000112, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000153, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000090, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000086, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000209, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000118, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000117, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000106, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000146, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000066, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000149, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000135, std=0.017090
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000107, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000118, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000113, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000099, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000071, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000215, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000119, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000116, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000119, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000118, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000135, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000104, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000179, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000094, std=0.017090
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000115, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000116, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000116, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000133, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000092, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000133, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000194, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000117, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000108, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000116, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000123, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000093, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000195, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000069, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000114, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000113, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000117, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000112, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000093, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000121, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000122, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000112, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000116, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000115, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000113, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000117, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000044, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000076, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000112, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000124, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000119, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000098, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000161, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000069, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000139, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000118, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000113, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000119, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000096, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000105, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000090, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000205, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000113, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000111, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000117, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000114, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000094, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000162, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000151, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000116, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000110, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000116, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000133, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000113, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000235, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000129, std=0.017090
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000114, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000114, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000121, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000132, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000153, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000073, std=0.017090
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000164, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000122, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000112, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000111, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000135, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000108, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000160, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000109, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000112, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000105, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000113, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000096, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000100, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000184, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000120, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000113, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000119, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000106, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000123, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000137, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=0.000002, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000045, std=0.017090
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000112, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000119, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000105, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000135, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000106, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000074, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000200, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000118, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000121, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000118, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000113, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000129, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000173, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000172, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000121, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000114, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000116, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000140, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000131, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000037, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000040, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000110, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000111, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000116, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000102, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000175, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=-0.000182, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000015, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000118, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000120, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000117, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000129, std=0.018066
TPAAttention: hidden_size=1152, head_dim=256
TPAAttention: num_heads=4, num_kv_heads=1
TPAAttention: q_rank=96, k_rank=48, v_rank=48
DEBUG INIT TPAAttn: W_A_q stats: mean=-0.000138, std=0.016968
DEBUG INIT TPAAttn: W_A_k stats: mean=0.000013, std=0.016968
DEBUG INIT TPAAttn: W_A_v stats: mean=-0.000180, std=0.016968
DEBUG INIT TPAAttn: W_B_q stats: mean=-0.000110, std=0.016968
DEBUG INIT TPAAttn: W_B_k stats: mean=-0.000115, std=0.016968
DEBUG INIT TPAAttn: W_B_v stats: mean=-0.000111, std=0.016968
DEBUG INIT TPAAttn: o_proj stats: mean=-0.000100, std=0.018066
/venv/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1329: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /pytorch/aten/src/ATen/native/Copy.cpp:308.)
  return t.to(
Copying non-attention weights with special handling for embedding layer...
  Copying embedding weights from 'embedder' to 'text_token_embedder'
Applying GQA to TPA conversion...
Converting GQA model to TPA format...
Model type: GemmaForCausalLM
Searching for attention modules...
  Found QKV-combined attention module in layer 0
  Found QKV-combined attention module in layer 1
  Found QKV-combined attention module in layer 2
  Found QKV-combined attention module in layer 3
  Found QKV-combined attention module in layer 4
  Found QKV-combined attention module in layer 5
  Found QKV-combined attention module in layer 6
  Found QKV-combined attention module in layer 7
  Found QKV-combined attention module in layer 8
  Found QKV-combined attention module in layer 9
  Found QKV-combined attention module in layer 10
  Found QKV-combined attention module in layer 11
  Found QKV-combined attention module in layer 12
  Found QKV-combined attention module in layer 13
  Found QKV-combined attention module in layer 14
  Found QKV-combined attention module in layer 15
  Found QKV-combined attention module in layer 16
  Found QKV-combined attention module in layer 17
  Found QKV-combined attention module in layer 18
  Found QKV-combined attention module in layer 19
  Found QKV-combined attention module in layer 20
  Found QKV-combined attention module in layer 21
  Found QKV-combined attention module in layer 22
  Found QKV-combined attention module in layer 23
  Found QKV-combined attention module in layer 24
  Found QKV-combined attention module in layer 25
Found 26 attention modules to convert
Converting attention layer: model.layers.0.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.0.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.8036192059516907, variation=0.001100003719329834.
reconstruction error=0.8030710220336914, variation=0.0005481839179992676.
reconstruction error=0.802741289138794, variation=0.00032973289489746094.
reconstruction error=0.8025256991386414, variation=0.0002155900001525879.
reconstruction error=0.8023734092712402, variation=0.00015228986740112305.
reconstruction error=0.8022645711898804, variation=0.00010883808135986328.
reconstruction error=0.8021806478500366, variation=8.392333984375e-05.
converged in 8 iterations.
Tucker decomposition completed in 2.15 seconds on cuda:0
Tucker decomposition took 2.15 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 179
    95% energy: rank 202
    98% energy: rank 223
  V singular value analysis:
    90% energy: rank 196
    95% energy: rank 220
    98% energy: rank 237
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 7.7697473764e-01, max: 3.3852128983e+00
SVD-based factorization completed in 0.0654 seconds
SVD reconstruction relative error: 0.602342
TPA reconstruction relative error: 0.602342
Singular values min: 6.3088738918e-01, max: 3.5820574760e+00
SVD-based factorization completed in 0.0368 seconds
SVD reconstruction relative error: 0.505074
TPA reconstruction relative error: 0.505074
Singular values min: 5.4514336586e-01, max: 4.0069160461e+00
SVD-based factorization completed in 0.0368 seconds
SVD reconstruction relative error: 0.408372
TPA reconstruction relative error: 0.408372
Singular values min: 6.4867937565e-01, max: 3.7485766411e+00
SVD-based factorization completed in 0.0362 seconds
SVD reconstruction relative error: 0.451651
TPA reconstruction relative error: 0.451651
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.3672125340e+00, max: 5.7967672348e+00
SVD-based factorization completed in 0.0365 seconds
SVD reconstruction relative error: 0.596748
TPA reconstruction relative error: 0.596748
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.4362185001e+00, max: 2.4661750793e+00
SVD-based factorization completed in 0.0365 seconds
SVD reconstruction relative error: 0.652509
TPA reconstruction relative error: 0.652509
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 223,237)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8982, K: 0.5967, V: 0.6525
GQA to TPA conversion complete in 2.76 seconds
  Decomposition completed in 2.76 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.76 seconds
Converting attention layer: model.layers.1.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.1.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.8280305862426758, variation=0.0011224150657653809.
reconstruction error=0.8274732828140259, variation=0.0005573034286499023.
reconstruction error=0.827139139175415, variation=0.00033414363861083984.
reconstruction error=0.8269097208976746, variation=0.00022941827774047852.
reconstruction error=0.8267438411712646, variation=0.0001658797264099121.
reconstruction error=0.8266172409057617, variation=0.0001266002655029297.
reconstruction error=0.8265148401260376, variation=0.0001024007797241211.
reconstruction error=0.8264331817626953, variation=8.165836334228516e-05.
converged in 9 iterations.
Tucker decomposition completed in 2.15 seconds on cuda:0
Tucker decomposition took 2.15 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 190
    95% energy: rank 212
    98% energy: rank 230
  V singular value analysis:
    90% energy: rank 207
    95% energy: rank 228
    98% energy: rank 243
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 4.1000080109e-01, max: 4.6712279320e+00
SVD-based factorization completed in 0.0396 seconds
SVD reconstruction relative error: 0.336214
TPA reconstruction relative error: 0.336214
Singular values min: 7.7879583836e-01, max: 2.8650610447e+00
SVD-based factorization completed in 0.0365 seconds
SVD reconstruction relative error: 0.577726
TPA reconstruction relative error: 0.577726
Singular values min: 5.9661883116e-01, max: 2.7440667152e+00
SVD-based factorization completed in 0.0351 seconds
SVD reconstruction relative error: 0.455771
TPA reconstruction relative error: 0.455771
Singular values min: 7.2205901146e-01, max: 3.1860470772e+00
SVD-based factorization completed in 0.0365 seconds
SVD reconstruction relative error: 0.551170
TPA reconstruction relative error: 0.551170
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.5305651426e+00, max: 3.6353707314e+00
SVD-based factorization completed in 0.0365 seconds
SVD reconstruction relative error: 0.638790
TPA reconstruction relative error: 0.638790
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.5264903307e+00, max: 2.2370638847e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.683824
TPA reconstruction relative error: 0.683824
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 230,243)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.9064, K: 0.6388, V: 0.6838
GQA to TPA conversion complete in 2.67 seconds
  Decomposition completed in 2.67 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.67 seconds
Converting attention layer: model.layers.2.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.2.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.7979186177253723, variation=0.001263439655303955.
reconstruction error=0.7972766757011414, variation=0.000641942024230957.
reconstruction error=0.7968874573707581, variation=0.0003892183303833008.
reconstruction error=0.7966254353523254, variation=0.0002620220184326172.
reconstruction error=0.7964323163032532, variation=0.00019311904907226562.
reconstruction error=0.796288788318634, variation=0.00014352798461914062.
reconstruction error=0.7961739897727966, variation=0.00011479854583740234.
reconstruction error=0.7960858941078186, variation=8.809566497802734e-05.
converged in 9 iterations.
Tucker decomposition completed in 2.16 seconds on cuda:0
Tucker decomposition took 2.16 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 182
    95% energy: rank 206
    98% energy: rank 226
  V singular value analysis:
    90% energy: rank 208
    95% energy: rank 229
    98% energy: rank 243
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 6.3609546423e-01, max: 1.7542660236e+00
SVD-based factorization completed in 0.0377 seconds
SVD reconstruction relative error: 0.584187
TPA reconstruction relative error: 0.584187
Singular values min: 6.4502102137e-01, max: 1.6813842058e+00
SVD-based factorization completed in 0.0361 seconds
SVD reconstruction relative error: 0.590054
TPA reconstruction relative error: 0.590054
Singular values min: 4.1600817442e-01, max: 4.2015166283e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.441399
TPA reconstruction relative error: 0.441399
Singular values min: 4.6870371699e-01, max: 2.5114448071e+00
SVD-based factorization completed in 0.0365 seconds
SVD reconstruction relative error: 0.506773
TPA reconstruction relative error: 0.506773
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.7547936440e+00, max: 5.2091741562e+00
SVD-based factorization completed in 0.0368 seconds
SVD reconstruction relative error: 0.602410
TPA reconstruction relative error: 0.602410
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.8146620989e+00, max: 2.6398093700e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.687663
TPA reconstruction relative error: 0.687663
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 226,243)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.9054, K: 0.6024, V: 0.6877
GQA to TPA conversion complete in 2.68 seconds
  Decomposition completed in 2.68 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.68 seconds
Converting attention layer: model.layers.3.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.3.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.8142455220222473, variation=0.0009200572967529297.
reconstruction error=0.8137836456298828, variation=0.00046187639236450195.
reconstruction error=0.8135080337524414, variation=0.00027561187744140625.
reconstruction error=0.8133236169815063, variation=0.0001844167709350586.
reconstruction error=0.8131903409957886, variation=0.00013327598571777344.
reconstruction error=0.813089907169342, variation=0.0001004338264465332.
reconstruction error=0.81301349401474, variation=7.641315460205078e-05.
converged in 8 iterations.
Tucker decomposition completed in 1.96 seconds on cuda:0
Tucker decomposition took 1.96 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 166
    95% energy: rank 192
    98% energy: rank 216
  V singular value analysis:
    90% energy: rank 214
    95% energy: rank 233
    98% energy: rank 245
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 7.8054302931e-01, max: 3.1505579948e+00
SVD-based factorization completed in 0.0365 seconds
SVD reconstruction relative error: 0.529276
TPA reconstruction relative error: 0.529276
Singular values min: 7.8127563000e-01, max: 2.7031903267e+00
SVD-based factorization completed in 0.0351 seconds
SVD reconstruction relative error: 0.488923
TPA reconstruction relative error: 0.488923
Singular values min: 6.5344941616e-01, max: 3.3255281448e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.462793
TPA reconstruction relative error: 0.462793
Singular values min: 6.5328508615e-01, max: 3.7509851456e+00
SVD-based factorization completed in 0.0364 seconds
SVD reconstruction relative error: 0.471914
TPA reconstruction relative error: 0.471914
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.3906923532e+00, max: 3.4601323605e+00
SVD-based factorization completed in 0.0353 seconds
SVD reconstruction relative error: 0.573396
TPA reconstruction relative error: 0.573396
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.3343877792e+00, max: 2.2996618748e+00
SVD-based factorization completed in 0.0385 seconds
SVD reconstruction relative error: 0.712653
TPA reconstruction relative error: 0.712653
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 216,245)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.9008, K: 0.5734, V: 0.7127
GQA to TPA conversion complete in 2.48 seconds
  Decomposition completed in 2.48 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.48 seconds
Converting attention layer: model.layers.4.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.4.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.8080276846885681, variation=0.0008605718612670898.
reconstruction error=0.8076087832450867, variation=0.0004189014434814453.
reconstruction error=0.8073640465736389, variation=0.0002447366714477539.
reconstruction error=0.8072022795677185, variation=0.00016176700592041016.
reconstruction error=0.807096004486084, variation=0.00010627508163452148.
reconstruction error=0.8070186972618103, variation=7.730722427368164e-05.
converged in 7 iterations.
Tucker decomposition completed in 1.75 seconds on cuda:0
Tucker decomposition took 1.75 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 159
    95% energy: rank 189
    98% energy: rank 217
  V singular value analysis:
    90% energy: rank 211
    95% energy: rank 230
    98% energy: rank 244
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 8.0728304386e-01, max: 3.3537085056e+00
SVD-based factorization completed in 0.0365 seconds
SVD reconstruction relative error: 0.468103
TPA reconstruction relative error: 0.468103
Singular values min: 6.4293009043e-01, max: 3.6376464367e+00
SVD-based factorization completed in 0.0365 seconds
SVD reconstruction relative error: 0.447968
TPA reconstruction relative error: 0.447968
Singular values min: 7.1943902969e-01, max: 4.3325285912e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.428794
TPA reconstruction relative error: 0.428794
Singular values min: 8.3529412746e-01, max: 3.7487728596e+00
SVD-based factorization completed in 0.0352 seconds
SVD reconstruction relative error: 0.380233
TPA reconstruction relative error: 0.380233
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.0871319771e+00, max: 3.7304995060e+00
SVD-based factorization completed in 0.0363 seconds
SVD reconstruction relative error: 0.518732
TPA reconstruction relative error: 0.518732
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.1967723370e+00, max: 1.5603802204e+00
SVD-based factorization completed in 0.0364 seconds
SVD reconstruction relative error: 0.703323
TPA reconstruction relative error: 0.703323
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 217,244)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8922, K: 0.5187, V: 0.7033
GQA to TPA conversion complete in 2.27 seconds
  Decomposition completed in 2.27 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.27 seconds
Converting attention layer: model.layers.5.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.5.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.7876297831535339, variation=0.0008231401443481445.
reconstruction error=0.7872355580329895, variation=0.0003942251205444336.
reconstruction error=0.7870088219642639, variation=0.00022673606872558594.
reconstruction error=0.7868526577949524, variation=0.00015616416931152344.
reconstruction error=0.7867476344108582, variation=0.00010502338409423828.
reconstruction error=0.7866655588150024, variation=8.207559585571289e-05.
converged in 7 iterations.
Tucker decomposition completed in 1.73 seconds on cuda:0
Tucker decomposition took 1.73 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 132
    95% energy: rank 162
    98% energy: rank 193
  V singular value analysis:
    90% energy: rank 194
    95% energy: rank 219
    98% energy: rank 239
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 6.6664487123e-01, max: 3.3117940426e+00
SVD-based factorization completed in 0.0361 seconds
SVD reconstruction relative error: 0.364365
TPA reconstruction relative error: 0.364365
Singular values min: 5.7029205561e-01, max: 3.2312932014e+00
SVD-based factorization completed in 0.0363 seconds
SVD reconstruction relative error: 0.431222
TPA reconstruction relative error: 0.431222
Singular values min: 4.6635922790e-01, max: 3.1217770576e+00
SVD-based factorization completed in 0.0352 seconds
SVD reconstruction relative error: 0.395236
TPA reconstruction relative error: 0.395236
Singular values min: 5.3423202038e-01, max: 3.3057394028e+00
SVD-based factorization completed in 0.0365 seconds
SVD reconstruction relative error: 0.408276
TPA reconstruction relative error: 0.408276
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.4132128954e+00, max: 3.6346509457e+00
SVD-based factorization completed in 0.0352 seconds
SVD reconstruction relative error: 0.446050
TPA reconstruction relative error: 0.446050
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.5382407904e+00, max: 2.5129377842e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.639194
TPA reconstruction relative error: 0.639194
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 193,239)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8760, K: 0.4460, V: 0.6392
GQA to TPA conversion complete in 2.24 seconds
  Decomposition completed in 2.24 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.24 seconds
Converting attention layer: model.layers.6.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.6.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.7937417030334473, variation=0.0010520219802856445.
reconstruction error=0.7932101488113403, variation=0.0005315542221069336.
reconstruction error=0.7928884029388428, variation=0.0003217458724975586.
reconstruction error=0.7926754355430603, variation=0.0002129673957824707.
reconstruction error=0.7925187945365906, variation=0.00015664100646972656.
reconstruction error=0.7924062013626099, variation=0.00011259317398071289.
reconstruction error=0.792314350605011, variation=9.185075759887695e-05.
converged in 8 iterations.
Tucker decomposition completed in 1.98 seconds on cuda:0
Tucker decomposition took 1.98 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 158
    95% energy: rank 188
    98% energy: rank 215
  V singular value analysis:
    90% energy: rank 216
    95% energy: rank 235
    98% energy: rank 246
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 5.6034851074e-01, max: 2.7619957924e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.476582
TPA reconstruction relative error: 0.476582
Singular values min: 5.6415170431e-01, max: 2.4080536366e+00
SVD-based factorization completed in 0.0363 seconds
SVD reconstruction relative error: 0.447143
TPA reconstruction relative error: 0.447143
Singular values min: 5.1007300615e-01, max: 2.2526249886e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.498319
TPA reconstruction relative error: 0.498319
Singular values min: 5.9105545282e-01, max: 2.3693840504e+00
SVD-based factorization completed in 0.0364 seconds
SVD reconstruction relative error: 0.512034
TPA reconstruction relative error: 0.512034
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.6680302620e+00, max: 5.0503277779e+00
SVD-based factorization completed in 0.0364 seconds
SVD reconstruction relative error: 0.529019
TPA reconstruction relative error: 0.529019
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.7456223965e+00, max: 2.3477895260e+00
SVD-based factorization completed in 0.0367 seconds
SVD reconstruction relative error: 0.723974
TPA reconstruction relative error: 0.723974
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 215,246)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8970, K: 0.5290, V: 0.7240
GQA to TPA conversion complete in 2.50 seconds
  Decomposition completed in 2.50 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.50 seconds
Converting attention layer: model.layers.7.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.7.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.810612678527832, variation=0.0008830428123474121.
reconstruction error=0.810166597366333, variation=0.00044608116149902344.
reconstruction error=0.809897780418396, variation=0.0002688169479370117.
reconstruction error=0.8097153306007385, variation=0.0001824498176574707.
reconstruction error=0.8095804452896118, variation=0.00013488531112670898.
reconstruction error=0.8094800114631653, variation=0.0001004338264465332.
reconstruction error=0.8094022274017334, variation=7.778406143188477e-05.
converged in 8 iterations.
Tucker decomposition completed in 1.91 seconds on cuda:0
Tucker decomposition took 1.91 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 154
    95% energy: rank 180
    98% energy: rank 205
  V singular value analysis:
    90% energy: rank 210
    95% energy: rank 230
    98% energy: rank 245
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 5.4402631521e-01, max: 3.5616958141e+00
SVD-based factorization completed in 0.0364 seconds
SVD reconstruction relative error: 0.438276
TPA reconstruction relative error: 0.438276
Singular values min: 7.7398961782e-01, max: 3.2167427540e+00
SVD-based factorization completed in 0.0354 seconds
SVD reconstruction relative error: 0.494348
TPA reconstruction relative error: 0.494348
Singular values min: 6.2894952297e-01, max: 3.1255002022e+00
SVD-based factorization completed in 0.0367 seconds
SVD reconstruction relative error: 0.490713
TPA reconstruction relative error: 0.490713
Singular values min: 6.4521706104e-01, max: 2.4846041203e+00
SVD-based factorization completed in 0.0363 seconds
SVD reconstruction relative error: 0.567986
TPA reconstruction relative error: 0.567986
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.5782458782e+00, max: 4.4952921867e+00
SVD-based factorization completed in 0.0361 seconds
SVD reconstruction relative error: 0.538262
TPA reconstruction relative error: 0.538262
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.5245696306e+00, max: 2.3931999207e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.689313
TPA reconstruction relative error: 0.689313
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 205,245)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8858, K: 0.5383, V: 0.6893
GQA to TPA conversion complete in 2.42 seconds
  Decomposition completed in 2.42 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.42 seconds
Converting attention layer: model.layers.8.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.8.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.8029263019561768, variation=0.0005652308464050293.
reconstruction error=0.8026533722877502, variation=0.00027292966842651367.
reconstruction error=0.8024816513061523, variation=0.0001717209815979004.
reconstruction error=0.8023653030395508, variation=0.0001163482666015625.
reconstruction error=0.8022772669792175, variation=8.803606033325195e-05.
converged in 6 iterations.
Tucker decomposition completed in 1.53 seconds on cuda:0
Tucker decomposition took 1.53 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 132
    95% energy: rank 162
    98% energy: rank 192
  V singular value analysis:
    90% energy: rank 211
    95% energy: rank 231
    98% energy: rank 244
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 5.8793216944e-01, max: 3.8015224934e+00
SVD-based factorization completed in 0.0362 seconds
SVD reconstruction relative error: 0.392052
TPA reconstruction relative error: 0.392052
Singular values min: 5.5334872007e-01, max: 2.9991161823e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.417320
TPA reconstruction relative error: 0.417320
Singular values min: 6.8436837196e-01, max: 3.5120902061e+00
SVD-based factorization completed in 0.0349 seconds
SVD reconstruction relative error: 0.364732
TPA reconstruction relative error: 0.364732
Singular values min: 5.5098217726e-01, max: 3.5270962715e+00
SVD-based factorization completed in 0.0369 seconds
SVD reconstruction relative error: 0.386066
TPA reconstruction relative error: 0.386066
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.5375599861e+00, max: 3.9864277840e+00
SVD-based factorization completed in 0.0352 seconds
SVD reconstruction relative error: 0.460410
TPA reconstruction relative error: 0.460410
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.4799454212e+00, max: 2.0930967331e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.697938
TPA reconstruction relative error: 0.697938
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 192,244)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8972, K: 0.4604, V: 0.6979
GQA to TPA conversion complete in 2.05 seconds
  Decomposition completed in 2.05 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.05 seconds
Converting attention layer: model.layers.9.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.9.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.7472285628318787, variation=0.0011209845542907715.
reconstruction error=0.7466729283332825, variation=0.0005556344985961914.
reconstruction error=0.7463522553443909, variation=0.00032067298889160156.
reconstruction error=0.7461336255073547, variation=0.0002186298370361328.
reconstruction error=0.7459743618965149, variation=0.00015926361083984375.
reconstruction error=0.7458548545837402, variation=0.0001195073127746582.
reconstruction error=0.7457597255706787, variation=9.512901306152344e-05.
converged in 8 iterations.
Tucker decomposition completed in 2.01 seconds on cuda:0
Tucker decomposition took 2.01 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 141
    95% energy: rank 172
    98% energy: rank 203
  V singular value analysis:
    90% energy: rank 211
    95% energy: rank 231
    98% energy: rank 245
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 4.3745246530e-01, max: 2.4660377502e+00
SVD-based factorization completed in 0.0367 seconds
SVD reconstruction relative error: 0.427982
TPA reconstruction relative error: 0.427982
Singular values min: 5.4779011011e-01, max: 2.5620768070e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.472256
TPA reconstruction relative error: 0.472256
Singular values min: 5.0606477261e-01, max: 2.3867719173e+00
SVD-based factorization completed in 0.0364 seconds
SVD reconstruction relative error: 0.448372
TPA reconstruction relative error: 0.448372
Singular values min: 5.4014205933e-01, max: 2.5590283871e+00
SVD-based factorization completed in 0.0361 seconds
SVD reconstruction relative error: 0.446335
TPA reconstruction relative error: 0.446335
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.6377209425e+00, max: 5.4312672615e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.469216
TPA reconstruction relative error: 0.469216
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.8193014860e+00, max: 2.6183009148e+00
SVD-based factorization completed in 0.0365 seconds
SVD reconstruction relative error: 0.699854
TPA reconstruction relative error: 0.699854
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 203,245)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8850, K: 0.4692, V: 0.6999
GQA to TPA conversion complete in 2.53 seconds
  Decomposition completed in 2.53 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.53 seconds
Converting attention layer: model.layers.10.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.10.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.8009578585624695, variation=0.0007639527320861816.
reconstruction error=0.8006042838096619, variation=0.0003535747528076172.
reconstruction error=0.8003938794136047, variation=0.0002104043960571289.
reconstruction error=0.8002540469169617, variation=0.0001398324966430664.
reconstruction error=0.8001521229743958, variation=0.00010192394256591797.
reconstruction error=0.8000695109367371, variation=8.26120376586914e-05.
converged in 7 iterations.
Tucker decomposition completed in 1.75 seconds on cuda:0
Tucker decomposition took 1.75 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 148
    95% energy: rank 178
    98% energy: rank 208
  V singular value analysis:
    90% energy: rank 207
    95% energy: rank 229
    98% energy: rank 243
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 8.0805349350e-01, max: 3.4694702625e+00
SVD-based factorization completed in 0.0385 seconds
SVD reconstruction relative error: 0.511247
TPA reconstruction relative error: 0.511247
Singular values min: 9.5780318975e-01, max: 3.9488906860e+00
SVD-based factorization completed in 0.0363 seconds
SVD reconstruction relative error: 0.422230
TPA reconstruction relative error: 0.422230
Singular values min: 7.3786586523e-01, max: 5.3139896393e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.437293
TPA reconstruction relative error: 0.437293
Singular values min: 6.7418438196e-01, max: 3.7309749126e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.423669
TPA reconstruction relative error: 0.423669
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.0565429926e+00, max: 3.7262864113e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.488450
TPA reconstruction relative error: 0.488450
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.1407637596e+00, max: 1.9193326235e+00
SVD-based factorization completed in 0.0370 seconds
SVD reconstruction relative error: 0.679325
TPA reconstruction relative error: 0.679325
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 208,243)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8933, K: 0.4885, V: 0.6793
GQA to TPA conversion complete in 2.28 seconds
  Decomposition completed in 2.28 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.28 seconds
Converting attention layer: model.layers.11.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.11.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.787193238735199, variation=0.0007015466690063477.
reconstruction error=0.7868817448616028, variation=0.0003114938735961914.
reconstruction error=0.786714494228363, variation=0.0001672506332397461.
reconstruction error=0.7866111397743225, variation=0.00010335445404052734.
reconstruction error=0.7865462303161621, variation=6.490945816040039e-05.
converged in 6 iterations.
Tucker decomposition completed in 1.54 seconds on cuda:0
Tucker decomposition took 1.54 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 119
    95% energy: rank 147
    98% energy: rank 176
  V singular value analysis:
    90% energy: rank 198
    95% energy: rank 223
    98% energy: rank 241
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 6.4298874140e-01, max: 5.0557923317e+00
SVD-based factorization completed in 0.0363 seconds
SVD reconstruction relative error: 0.311631
TPA reconstruction relative error: 0.311631
Singular values min: 5.2607971430e-01, max: 4.2510652542e+00
SVD-based factorization completed in 0.0362 seconds
SVD reconstruction relative error: 0.342240
TPA reconstruction relative error: 0.342240
Singular values min: 7.6245832443e-01, max: 4.9239025116e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.356965
TPA reconstruction relative error: 0.356965
Singular values min: 5.2117913961e-01, max: 6.2110133171e+00
SVD-based factorization completed in 0.0365 seconds
SVD reconstruction relative error: 0.265637
TPA reconstruction relative error: 0.265637
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.0822466612e+00, max: 2.9901256561e+00
SVD-based factorization completed in 0.0351 seconds
SVD reconstruction relative error: 0.407663
TPA reconstruction relative error: 0.407663
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.1114771366e+00, max: 1.9234684706e+00
SVD-based factorization completed in 0.0368 seconds
SVD reconstruction relative error: 0.629866
TPA reconstruction relative error: 0.629866
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 176,241)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8672, K: 0.4077, V: 0.6299
GQA to TPA conversion complete in 2.06 seconds
  Decomposition completed in 2.06 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.06 seconds
Converting attention layer: model.layers.12.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.12.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.7606379985809326, variation=0.0009320974349975586.
reconstruction error=0.7601770162582397, variation=0.0004609823226928711.
reconstruction error=0.7598992586135864, variation=0.0002777576446533203.
reconstruction error=0.7597116231918335, variation=0.0001876354217529297.
reconstruction error=0.7595760226249695, variation=0.00013560056686401367.
reconstruction error=0.7594751715660095, variation=0.00010085105895996094.
reconstruction error=0.7593905329704285, variation=8.463859558105469e-05.
converged in 8 iterations.
Tucker decomposition completed in 2.06 seconds on cuda:0
Tucker decomposition took 2.06 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 143
    95% energy: rank 173
    98% energy: rank 204
  V singular value analysis:
    90% energy: rank 210
    95% energy: rank 230
    98% energy: rank 244
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 4.7275295854e-01, max: 2.7216334343e+00
SVD-based factorization completed in 0.0367 seconds
SVD reconstruction relative error: 0.403364
TPA reconstruction relative error: 0.403364
Singular values min: 4.9871701002e-01, max: 2.5544576645e+00
SVD-based factorization completed in 0.0372 seconds
SVD reconstruction relative error: 0.419775
TPA reconstruction relative error: 0.419775
Singular values min: 4.8606026173e-01, max: 2.7738423347e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.423978
TPA reconstruction relative error: 0.423978
Singular values min: 4.2545402050e-01, max: 2.3017690182e+00
SVD-based factorization completed in 0.0365 seconds
SVD reconstruction relative error: 0.427811
TPA reconstruction relative error: 0.427811
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.7085511684e+00, max: 5.2763261795e+00
SVD-based factorization completed in 0.0365 seconds
SVD reconstruction relative error: 0.478080
TPA reconstruction relative error: 0.478080
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.8109393120e+00, max: 2.5746593475e+00
SVD-based factorization completed in 0.0368 seconds
SVD reconstruction relative error: 0.693138
TPA reconstruction relative error: 0.693138
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 204,244)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8968, K: 0.4781, V: 0.6931
GQA to TPA conversion complete in 2.59 seconds
  Decomposition completed in 2.59 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.59 seconds
Converting attention layer: model.layers.13.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.13.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.764874279499054, variation=0.0009446144104003906.
reconstruction error=0.7644056081771851, variation=0.0004686713218688965.
reconstruction error=0.7641193270683289, variation=0.00028628110885620117.
reconstruction error=0.7639287114143372, variation=0.00019061565399169922.
reconstruction error=0.7637904286384583, variation=0.00013828277587890625.
reconstruction error=0.7636862993240356, variation=0.00010412931442260742.
reconstruction error=0.7636073231697083, variation=7.897615432739258e-05.
converged in 8 iterations.
Tucker decomposition completed in 2.04 seconds on cuda:0
Tucker decomposition took 2.04 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 136
    95% energy: rank 167
    98% energy: rank 199
  V singular value analysis:
    90% energy: rank 210
    95% energy: rank 230
    98% energy: rank 244
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 5.1416796446e-01, max: 2.7041444778e+00
SVD-based factorization completed in 0.0367 seconds
SVD reconstruction relative error: 0.411882
TPA reconstruction relative error: 0.411882
Singular values min: 5.1110464334e-01, max: 2.7629158497e+00
SVD-based factorization completed in 0.0374 seconds
SVD reconstruction relative error: 0.406314
TPA reconstruction relative error: 0.406314
Singular values min: 5.6995242834e-01, max: 2.8133573532e+00
SVD-based factorization completed in 0.0383 seconds
SVD reconstruction relative error: 0.444621
TPA reconstruction relative error: 0.444621
Singular values min: 5.2839231491e-01, max: 2.6698014736e+00
SVD-based factorization completed in 0.0378 seconds
SVD reconstruction relative error: 0.422969
TPA reconstruction relative error: 0.422969
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.5333771706e+00, max: 5.6017684937e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.462546
TPA reconstruction relative error: 0.462546
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.6553931236e+00, max: 2.4471662045e+00
SVD-based factorization completed in 0.0368 seconds
SVD reconstruction relative error: 0.705816
TPA reconstruction relative error: 0.705816
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 199,244)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8775, K: 0.4625, V: 0.7058
GQA to TPA conversion complete in 2.57 seconds
  Decomposition completed in 2.57 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.57 seconds
Converting attention layer: model.layers.14.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.14.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.7824167609214783, variation=0.0008143782615661621.
reconstruction error=0.7820449471473694, variation=0.0003718137741088867.
reconstruction error=0.7818421721458435, variation=0.0002027750015258789.
reconstruction error=0.7817118167877197, variation=0.0001303553581237793.
reconstruction error=0.7816271781921387, variation=8.463859558105469e-05.
converged in 6 iterations.
Tucker decomposition completed in 1.53 seconds on cuda:0
Tucker decomposition took 1.53 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 129
    95% energy: rank 160
    98% energy: rank 194
  V singular value analysis:
    90% energy: rank 203
    95% energy: rank 226
    98% energy: rank 241
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 5.7480055094e-01, max: 2.7493972778e+00
SVD-based factorization completed in 0.0364 seconds
SVD reconstruction relative error: 0.412905
TPA reconstruction relative error: 0.412905
Singular values min: 6.1651313305e-01, max: 3.2541141510e+00
SVD-based factorization completed in 0.0379 seconds
SVD reconstruction relative error: 0.401595
TPA reconstruction relative error: 0.401595
Singular values min: 6.2702119350e-01, max: 2.7751152515e+00
SVD-based factorization completed in 0.0352 seconds
SVD reconstruction relative error: 0.398077
TPA reconstruction relative error: 0.398077
Singular values min: 6.0141193867e-01, max: 3.2348067760e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.385113
TPA reconstruction relative error: 0.385113
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.3441314697e+00, max: 5.7975530624e+00
SVD-based factorization completed in 0.0363 seconds
SVD reconstruction relative error: 0.445162
TPA reconstruction relative error: 0.445162
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.3628685474e+00, max: 2.8003640175e+00
SVD-based factorization completed in 0.0369 seconds
SVD reconstruction relative error: 0.664640
TPA reconstruction relative error: 0.664640
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 194,241)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8851, K: 0.4452, V: 0.6646
GQA to TPA conversion complete in 2.05 seconds
  Decomposition completed in 2.05 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.05 seconds
Converting attention layer: model.layers.15.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.15.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.7674961686134338, variation=0.0008870959281921387.
reconstruction error=0.7670801877975464, variation=0.00041598081588745117.
reconstruction error=0.7668394446372986, variation=0.00024074316024780273.
reconstruction error=0.7666831612586975, variation=0.00015628337860107422.
reconstruction error=0.7665747404098511, variation=0.00010842084884643555.
reconstruction error=0.7664929628372192, variation=8.177757263183594e-05.
converged in 7 iterations.
Tucker decomposition completed in 1.81 seconds on cuda:0
Tucker decomposition took 1.81 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 143
    95% energy: rank 175
    98% energy: rank 207
  V singular value analysis:
    90% energy: rank 202
    95% energy: rank 224
    98% energy: rank 241
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 4.9406495690e-01, max: 2.7175579071e+00
SVD-based factorization completed in 0.0367 seconds
SVD reconstruction relative error: 0.417402
TPA reconstruction relative error: 0.417402
Singular values min: 4.4632303715e-01, max: 2.5006599426e+00
SVD-based factorization completed in 0.0383 seconds
SVD reconstruction relative error: 0.373779
TPA reconstruction relative error: 0.373779
Singular values min: 4.4185230136e-01, max: 2.2466056347e+00
SVD-based factorization completed in 0.0368 seconds
SVD reconstruction relative error: 0.451053
TPA reconstruction relative error: 0.451053
Singular values min: 4.5222169161e-01, max: 3.1383328438e+00
SVD-based factorization completed in 0.0368 seconds
SVD reconstruction relative error: 0.356649
TPA reconstruction relative error: 0.356649
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.7473952770e+00, max: 5.8925294876e+00
SVD-based factorization completed in 0.0364 seconds
SVD reconstruction relative error: 0.480946
TPA reconstruction relative error: 0.480946
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.7920020819e+00, max: 3.0888624191e+00
SVD-based factorization completed in 0.0383 seconds
SVD reconstruction relative error: 0.666747
TPA reconstruction relative error: 0.666747
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 207,241)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8807, K: 0.4809, V: 0.6667
GQA to TPA conversion complete in 2.34 seconds
  Decomposition completed in 2.34 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.34 seconds
Converting attention layer: model.layers.16.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.16.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.7825590372085571, variation=0.0008907914161682129.
reconstruction error=0.782123327255249, variation=0.00043570995330810547.
reconstruction error=0.7818605303764343, variation=0.00026279687881469727.
reconstruction error=0.781680703163147, variation=0.00017982721328735352.
reconstruction error=0.7815481424331665, variation=0.00013256072998046875.
reconstruction error=0.7814455628395081, variation=0.00010257959365844727.
reconstruction error=0.7813647985458374, variation=8.07642936706543e-05.
converged in 8 iterations.
Tucker decomposition completed in 2.01 seconds on cuda:0
Tucker decomposition took 2.01 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 157
    95% energy: rank 186
    98% energy: rank 213
  V singular value analysis:
    90% energy: rank 208
    95% energy: rank 229
    98% energy: rank 243
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 6.8208259344e-01, max: 2.7822067738e+00
SVD-based factorization completed in 0.0364 seconds
SVD reconstruction relative error: 0.499905
TPA reconstruction relative error: 0.499905
Singular values min: 5.4436439276e-01, max: 3.2473981380e+00
SVD-based factorization completed in 0.0368 seconds
SVD reconstruction relative error: 0.429232
TPA reconstruction relative error: 0.429232
Singular values min: 6.7993611097e-01, max: 3.0776200294e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.440690
TPA reconstruction relative error: 0.440690
Singular values min: 7.1629792452e-01, max: 3.1300156116e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.465875
TPA reconstruction relative error: 0.465875
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.3523106575e+00, max: 3.9460487366e+00
SVD-based factorization completed in 0.0375 seconds
SVD reconstruction relative error: 0.528349
TPA reconstruction relative error: 0.528349
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.3681069613e+00, max: 2.1941454411e+00
SVD-based factorization completed in 0.0368 seconds
SVD reconstruction relative error: 0.682508
TPA reconstruction relative error: 0.682508
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 213,243)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8956, K: 0.5283, V: 0.6825
GQA to TPA conversion complete in 2.53 seconds
  Decomposition completed in 2.53 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.53 seconds
Converting attention layer: model.layers.17.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.17.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.7814446687698364, variation=0.00047522783279418945.
reconstruction error=0.7812128663063049, variation=0.00023180246353149414.
reconstruction error=0.7810685634613037, variation=0.0001443028450012207.
reconstruction error=0.7809695601463318, variation=9.900331497192383e-05.
converged in 5 iterations.
Tucker decomposition completed in 1.34 seconds on cuda:0
Tucker decomposition took 1.34 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 95
    95% energy: rank 124
    98% energy: rank 159
  V singular value analysis:
    90% energy: rank 205
    95% energy: rank 226
    98% energy: rank 242
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 7.5461494923e-01, max: 4.0903329849e+00
SVD-based factorization completed in 0.0354 seconds
SVD reconstruction relative error: 0.383025
TPA reconstruction relative error: 0.383025
Singular values min: 7.1716219187e-01, max: 4.6074676514e+00
SVD-based factorization completed in 0.0351 seconds
SVD reconstruction relative error: 0.334197
TPA reconstruction relative error: 0.334197
Singular values min: 6.7321974039e-01, max: 4.1957521439e+00
SVD-based factorization completed in 0.0352 seconds
SVD reconstruction relative error: 0.387575
TPA reconstruction relative error: 0.387575
Singular values min: 6.2758028507e-01, max: 4.1971912384e+00
SVD-based factorization completed in 0.0360 seconds
SVD reconstruction relative error: 0.352320
TPA reconstruction relative error: 0.352320
Computing SVD-based factorization for 1 key heads...
Singular values min: 9.1920936108e-01, max: 2.6102962494e+00
SVD-based factorization completed in 0.0357 seconds
SVD reconstruction relative error: 0.308781
TPA reconstruction relative error: 0.308781
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.1811839342e+00, max: 1.7264724970e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.667854
TPA reconstruction relative error: 0.667854
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 159,242)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8635, K: 0.3088, V: 0.6679
GQA to TPA conversion complete in 1.85 seconds
  Decomposition completed in 1.85 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 1.85 seconds
Converting attention layer: model.layers.18.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.18.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.715516209602356, variation=0.0007379651069641113.
reconstruction error=0.7151647210121155, variation=0.0003514885902404785.
reconstruction error=0.7149525880813599, variation=0.00021213293075561523.
reconstruction error=0.7148091793060303, variation=0.00014340877532958984.
reconstruction error=0.7147051692008972, variation=0.00010401010513305664.
reconstruction error=0.7146291732788086, variation=7.599592208862305e-05.
converged in 7 iterations.
Tucker decomposition completed in 1.80 seconds on cuda:0
Tucker decomposition took 1.80 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 117
    95% energy: rank 152
    98% energy: rank 184
  V singular value analysis:
    90% energy: rank 203
    95% energy: rank 226
    98% energy: rank 242
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 4.7070118785e-01, max: 2.4906980991e+00
SVD-based factorization completed in 0.0363 seconds
SVD reconstruction relative error: 0.370371
TPA reconstruction relative error: 0.370371
Singular values min: 5.2360945940e-01, max: 2.6293332577e+00
SVD-based factorization completed in 0.0360 seconds
SVD reconstruction relative error: 0.410427
TPA reconstruction relative error: 0.410427
Singular values min: 4.2696234584e-01, max: 2.3813984394e+00
SVD-based factorization completed in 0.0364 seconds
SVD reconstruction relative error: 0.355535
TPA reconstruction relative error: 0.355535
Singular values min: 4.5167022943e-01, max: 2.5778176785e+00
SVD-based factorization completed in 0.0363 seconds
SVD reconstruction relative error: 0.362722
TPA reconstruction relative error: 0.362722
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.2805856466e+00, max: 7.2117290497e+00
SVD-based factorization completed in 0.0366 seconds
SVD reconstruction relative error: 0.376615
TPA reconstruction relative error: 0.376615
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.6831790209e+00, max: 3.6241626740e+00
SVD-based factorization completed in 0.0369 seconds
SVD reconstruction relative error: 0.653534
TPA reconstruction relative error: 0.653534
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 184,242)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8953, K: 0.3766, V: 0.6535
GQA to TPA conversion complete in 2.32 seconds
  Decomposition completed in 2.32 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.32 seconds
Converting attention layer: model.layers.19.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.19.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.8250051140785217, variation=0.0012257099151611328.
reconstruction error=0.8244110941886902, variation=0.000594019889831543.
reconstruction error=0.8240611553192139, variation=0.00034993886947631836.
reconstruction error=0.8238317370414734, variation=0.00022941827774047852.
reconstruction error=0.8236713409423828, variation=0.00016039609909057617.
reconstruction error=0.8235530257225037, variation=0.00011831521987915039.
reconstruction error=0.8234602808952332, variation=9.274482727050781e-05.
converged in 8 iterations.
Tucker decomposition completed in 1.91 seconds on cuda:0
Tucker decomposition took 1.91 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 181
    95% energy: rank 206
    98% energy: rank 227
  V singular value analysis:
    90% energy: rank 207
    95% energy: rank 227
    98% energy: rank 242
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 9.1512113810e-01, max: 3.1047074795e+00
SVD-based factorization completed in 0.0364 seconds
SVD reconstruction relative error: 0.578849
TPA reconstruction relative error: 0.578849
Singular values min: 8.7807255983e-01, max: 3.0002505779e+00
SVD-based factorization completed in 0.0363 seconds
SVD reconstruction relative error: 0.570965
TPA reconstruction relative error: 0.570965
Singular values min: 8.7533980608e-01, max: 4.2508287430e+00
SVD-based factorization completed in 0.0365 seconds
SVD reconstruction relative error: 0.515840
TPA reconstruction relative error: 0.515840
Singular values min: 8.3643686771e-01, max: 3.7512321472e+00
SVD-based factorization completed in 0.0365 seconds
SVD reconstruction relative error: 0.533814
TPA reconstruction relative error: 0.533814
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.1462988853e+00, max: 3.4641842842e+00
SVD-based factorization completed in 0.0369 seconds
SVD reconstruction relative error: 0.608345
TPA reconstruction relative error: 0.608345
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.1622169018e+00, max: 1.9249645472e+00
SVD-based factorization completed in 0.0365 seconds
SVD reconstruction relative error: 0.687211
TPA reconstruction relative error: 0.687211
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 227,242)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.9092, K: 0.6083, V: 0.6872
GQA to TPA conversion complete in 2.44 seconds
  Decomposition completed in 2.44 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.44 seconds
Converting attention layer: model.layers.20.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.20.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.7921935319900513, variation=0.0009560585021972656.
reconstruction error=0.7917011976242065, variation=0.0004923343658447266.
reconstruction error=0.7913926243782043, variation=0.00030857324600219727.
reconstruction error=0.791185200214386, variation=0.00020742416381835938.
reconstruction error=0.7910314202308655, variation=0.0001537799835205078.
reconstruction error=0.7909160852432251, variation=0.00011533498764038086.
reconstruction error=0.7908245325088501, variation=9.1552734375e-05.
converged in 8 iterations.
Tucker decomposition completed in 1.98 seconds on cuda:0
Tucker decomposition took 1.98 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 152
    95% energy: rank 183
    98% energy: rank 213
  V singular value analysis:
    90% energy: rank 213
    95% energy: rank 232
    98% energy: rank 246
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 5.9924006462e-01, max: 2.8693749905e+00
SVD-based factorization completed in 0.0381 seconds
SVD reconstruction relative error: 0.488997
TPA reconstruction relative error: 0.488997
Singular values min: 6.1133641005e-01, max: 2.6599962711e+00
SVD-based factorization completed in 0.0370 seconds
SVD reconstruction relative error: 0.466203
TPA reconstruction relative error: 0.466203
Singular values min: 5.6525826454e-01, max: 3.2930760384e+00
SVD-based factorization completed in 0.0372 seconds
SVD reconstruction relative error: 0.440742
TPA reconstruction relative error: 0.440742
Singular values min: 5.5066287518e-01, max: 3.1326670647e+00
SVD-based factorization completed in 0.0384 seconds
SVD reconstruction relative error: 0.407791
TPA reconstruction relative error: 0.407791
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.4333126545e+00, max: 4.4011254311e+00
SVD-based factorization completed in 0.0371 seconds
SVD reconstruction relative error: 0.507513
TPA reconstruction relative error: 0.507513
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.5106278658e+00, max: 2.1985292435e+00
SVD-based factorization completed in 0.0370 seconds
SVD reconstruction relative error: 0.708215
TPA reconstruction relative error: 0.708215
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 213,246)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8949, K: 0.5075, V: 0.7082
GQA to TPA conversion complete in 2.52 seconds
  Decomposition completed in 2.52 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.52 seconds
Converting attention layer: model.layers.21.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.21.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.7721774578094482, variation=0.0009355545043945312.
reconstruction error=0.771714985370636, variation=0.00046247243881225586.
reconstruction error=0.7714352011680603, variation=0.0002797842025756836.
reconstruction error=0.7712442874908447, variation=0.00019091367721557617.
reconstruction error=0.7711023092269897, variation=0.00014197826385498047.
reconstruction error=0.7709939479827881, variation=0.00010836124420166016.
reconstruction error=0.7709097266197205, variation=8.422136306762695e-05.
converged in 8 iterations.
Tucker decomposition completed in 2.03 seconds on cuda:0
Tucker decomposition took 2.03 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 161
    95% energy: rank 192
    98% energy: rank 220
  V singular value analysis:
    90% energy: rank 207
    95% energy: rank 228
    98% energy: rank 243
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 4.4496488571e-01, max: 2.6272239685e+00
SVD-based factorization completed in 0.0548 seconds
SVD reconstruction relative error: 0.450995
TPA reconstruction relative error: 0.450995
Singular values min: 4.4682556391e-01, max: 2.3719112873e+00
SVD-based factorization completed in 0.0485 seconds
SVD reconstruction relative error: 0.419160
TPA reconstruction relative error: 0.419160
Singular values min: 4.9711629748e-01, max: 2.0933454037e+00
SVD-based factorization completed in 0.0371 seconds
SVD reconstruction relative error: 0.506199
TPA reconstruction relative error: 0.506199
Singular values min: 5.0329613686e-01, max: 2.3152086735e+00
SVD-based factorization completed in 0.0371 seconds
SVD reconstruction relative error: 0.482409
TPA reconstruction relative error: 0.482409
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.7893068790e+00, max: 6.0033526421e+00
SVD-based factorization completed in 0.0369 seconds
SVD reconstruction relative error: 0.522503
TPA reconstruction relative error: 0.522503
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.9283928871e+00, max: 3.6647491455e+00
SVD-based factorization completed in 0.0370 seconds
SVD reconstruction relative error: 0.687933
TPA reconstruction relative error: 0.687933
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 220,243)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.9014, K: 0.5225, V: 0.6879
GQA to TPA conversion complete in 2.60 seconds
  Decomposition completed in 2.60 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.60 seconds
Converting attention layer: model.layers.22.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.22.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.7929823398590088, variation=0.0010105371475219727.
reconstruction error=0.7924863696098328, variation=0.0004959702491760254.
reconstruction error=0.7921939492225647, variation=0.0002924203872680664.
reconstruction error=0.7920065522193909, variation=0.00018739700317382812.
reconstruction error=0.7918741106987, variation=0.00013244152069091797.
reconstruction error=0.7917730212211609, variation=0.0001010894775390625.
reconstruction error=0.7916967272758484, variation=7.62939453125e-05.
converged in 8 iterations.
Tucker decomposition completed in 2.10 seconds on cuda:0
Tucker decomposition took 2.10 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 176
    95% energy: rank 205
    98% energy: rank 228
  V singular value analysis:
    90% energy: rank 214
    95% energy: rank 233
    98% energy: rank 246
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 3.9456948638e-01, max: 2.6673774719e+00
SVD-based factorization completed in 0.0382 seconds
SVD reconstruction relative error: 0.371015
TPA reconstruction relative error: 0.371015
Singular values min: 5.5218833685e-01, max: 2.6125509739e+00
SVD-based factorization completed in 0.0383 seconds
SVD reconstruction relative error: 0.503824
TPA reconstruction relative error: 0.503824
Singular values min: 5.5933541059e-01, max: 2.6545565128e+00
SVD-based factorization completed in 0.0393 seconds
SVD reconstruction relative error: 0.506512
TPA reconstruction relative error: 0.506512
Singular values min: 4.7291326523e-01, max: 2.8204190731e+00
SVD-based factorization completed in 0.0378 seconds
SVD reconstruction relative error: 0.426119
TPA reconstruction relative error: 0.426119
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.6173461676e+00, max: 4.9895935059e+00
SVD-based factorization completed in 0.0382 seconds
SVD reconstruction relative error: 0.566737
TPA reconstruction relative error: 0.566737
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.7242956161e+00, max: 2.6677563190e+00
SVD-based factorization completed in 0.0379 seconds
SVD reconstruction relative error: 0.707795
TPA reconstruction relative error: 0.707795
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 228,246)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8890, K: 0.5667, V: 0.7078
GQA to TPA conversion complete in 2.65 seconds
  Decomposition completed in 2.65 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.65 seconds
Converting attention layer: model.layers.23.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.23.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.7539736032485962, variation=0.0009381771087646484.
reconstruction error=0.7535080909729004, variation=0.0004655122756958008.
reconstruction error=0.7532177567481995, variation=0.00029033422470092773.
reconstruction error=0.7530139088630676, variation=0.00020384788513183594.
reconstruction error=0.7528678774833679, variation=0.00014603137969970703.
reconstruction error=0.7527515292167664, variation=0.0001163482666015625.
reconstruction error=0.752663254737854, variation=8.827447891235352e-05.
converged in 8 iterations.
Tucker decomposition completed in 2.06 seconds on cuda:0
Tucker decomposition took 2.06 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 137
    95% energy: rank 163
    98% energy: rank 189
  V singular value analysis:
    90% energy: rank 196
    95% energy: rank 222
    98% energy: rank 240
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 4.2960506678e-01, max: 3.4638593197e+00
SVD-based factorization completed in 0.0377 seconds
SVD reconstruction relative error: 0.382045
TPA reconstruction relative error: 0.382045
Singular values min: 3.7581357360e-01, max: 3.3746247292e+00
SVD-based factorization completed in 0.0378 seconds
SVD reconstruction relative error: 0.377205
TPA reconstruction relative error: 0.377205
Singular values min: 4.1886109114e-01, max: 2.3921823502e+00
SVD-based factorization completed in 0.0375 seconds
SVD reconstruction relative error: 0.486332
TPA reconstruction relative error: 0.486332
Singular values min: 4.4303214550e-01, max: 2.7294914722e+00
SVD-based factorization completed in 0.0377 seconds
SVD reconstruction relative error: 0.381337
TPA reconstruction relative error: 0.381337
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.8540202379e+00, max: 6.5380420685e+00
SVD-based factorization completed in 0.0375 seconds
SVD reconstruction relative error: 0.475951
TPA reconstruction relative error: 0.475951
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.9158275127e+00, max: 3.3780653477e+00
SVD-based factorization completed in 0.0379 seconds
SVD reconstruction relative error: 0.629822
TPA reconstruction relative error: 0.629822
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 189,240)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8938, K: 0.4760, V: 0.6298
GQA to TPA conversion complete in 2.59 seconds
  Decomposition completed in 2.59 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.59 seconds
Converting attention layer: model.layers.24.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.24.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.7897436022758484, variation=0.0009978413581848145.
reconstruction error=0.7892463207244873, variation=0.000497281551361084.
reconstruction error=0.7889468669891357, variation=0.0002994537353515625.
reconstruction error=0.7887470722198486, variation=0.00019979476928710938.
reconstruction error=0.7886053323745728, variation=0.0001417398452758789.
reconstruction error=0.7884977459907532, variation=0.00010758638381958008.
reconstruction error=0.78841233253479, variation=8.541345596313477e-05.
converged in 8 iterations.
Tucker decomposition completed in 2.04 seconds on cuda:0
Tucker decomposition took 2.04 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 163
    95% energy: rank 194
    98% energy: rank 221
  V singular value analysis:
    90% energy: rank 209
    95% energy: rank 229
    98% energy: rank 243
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 5.7085698843e-01, max: 3.9711766243e+00
SVD-based factorization completed in 0.0380 seconds
SVD reconstruction relative error: 0.463486
TPA reconstruction relative error: 0.463486
Singular values min: 6.4859533310e-01, max: 3.2473654747e+00
SVD-based factorization completed in 0.0376 seconds
SVD reconstruction relative error: 0.472244
TPA reconstruction relative error: 0.472244
Singular values min: 6.3671994209e-01, max: 2.7106945515e+00
SVD-based factorization completed in 0.0378 seconds
SVD reconstruction relative error: 0.553871
TPA reconstruction relative error: 0.553871
Singular values min: 6.6186261177e-01, max: 3.1175308228e+00
SVD-based factorization completed in 0.0374 seconds
SVD reconstruction relative error: 0.463353
TPA reconstruction relative error: 0.463353
Computing SVD-based factorization for 1 key heads...
Singular values min: 1.4014538527e+00, max: 3.8350954056e+00
SVD-based factorization completed in 0.0382 seconds
SVD reconstruction relative error: 0.534868
TPA reconstruction relative error: 0.534868
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.4845726490e+00, max: 2.1178638935e+00
SVD-based factorization completed in 0.0379 seconds
SVD reconstruction relative error: 0.703079
TPA reconstruction relative error: 0.703079
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 221,243)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8769, K: 0.5349, V: 0.7031
GQA to TPA conversion complete in 2.58 seconds
  Decomposition completed in 2.58 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.58 seconds
Converting attention layer: model.layers.25.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.25.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting GQA to TPA conversion using Tucker decomposition...
  After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using actual weight dimensions to determine head_dim
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Reshaped weight dimensions: Q=torch.Size([1152, 4, 256]), K=torch.Size([1152, 1, 256]), V=torch.Size([1152, 1, 256]), O=torch.Size([1152, 4, 256])
Tensorized weights shape: torch.Size([1152, 256, 4, 4])
Using ranks: R1=96, R2=96, R3=4
Tensor shape: torch.Size([1152, 256, 4, 4])
Applying Tucker decomposition with ranks: (96, 96, 4)
Tensor shape before Tucker decomposition: torch.Size([1152, 256, 4, 4])
Target ranks: [96, 96, 4, 4]
Starting Tucker decomposition...
Running Tucker decomposition on device: cuda:0
Using GPU-accelerated Tucker decomposition...
reconstruction error=0.7967671751976013, variation=0.0006514191627502441.
reconstruction error=0.7964585423469543, variation=0.00030863285064697266.
reconstruction error=0.7962749004364014, variation=0.00018364191055297852.
reconstruction error=0.7961538434028625, variation=0.00012105703353881836.
reconstruction error=0.7960663437843323, variation=8.749961853027344e-05.
converged in 6 iterations.
Tucker decomposition completed in 1.55 seconds on cuda:0
Tucker decomposition took 1.55 seconds
Tucker decomposition complete. Core shape: torch.Size([96, 96, 4, 4])
Factor shapes: [torch.Size([1152, 96]), torch.Size([256, 96]), torch.Size([4, 4]), torch.Size([4, 4])]
Actual ranks from Tucker decomposition: R1=96, R2=96

ANALYZING INTRINSIC RANKS using energy-based approach (cumulative explained variance)
  K singular value analysis:
    90% energy: rank 160
    95% energy: rank 193
    98% energy: rank 221
  V singular value analysis:
    90% energy: rank 205
    95% energy: rank 225
    98% energy: rank 240
  Forcing ranks to match Tucker decomposition rank: K: 96, V: 96
  Selected ranks - K: 96, V: 96 (capped by max_practical_rank=320 and Tucker rank=96)
Intrinsic ranks detected - Q: 96, K: 96, V: 96

USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=96, K=96, V=96
These ranks are determined by energy-based analysis to balance accuracy and efficiency
Using efficient SVD-based factorization for TPA weights with component-specific ranks
Component-specific ranks: Q=96, K=96, V=96
Using cuda:0 for SVD-based TPA projection computation
Using practical computation ranks - Q: 96, K: 96, V: 96
These ranks balance the intrinsic structure with Tucker decomposition constraints
Computing SVD-based factorization for 4 query heads...
Singular values min: 7.3820662498e-01, max: 4.4193816185e+00
SVD-based factorization completed in 0.0378 seconds
SVD reconstruction relative error: 0.430018
TPA reconstruction relative error: 0.430018
Singular values min: 6.7655247450e-01, max: 4.1415624619e+00
SVD-based factorization completed in 0.0377 seconds
SVD reconstruction relative error: 0.403471
TPA reconstruction relative error: 0.403471
Singular values min: 6.6902583838e-01, max: 4.0985350609e+00
SVD-based factorization completed in 0.0378 seconds
SVD reconstruction relative error: 0.354184
TPA reconstruction relative error: 0.354184
Singular values min: 6.7435538769e-01, max: 3.7026479244e+00
SVD-based factorization completed in 0.0375 seconds
SVD reconstruction relative error: 0.382163
TPA reconstruction relative error: 0.382163
Computing SVD-based factorization for 1 key heads...
Singular values min: 9.6316075325e-01, max: 3.8512833118e+00
SVD-based factorization completed in 0.0375 seconds
SVD reconstruction relative error: 0.507108
TPA reconstruction relative error: 0.507108
Computing SVD-based factorization for 1 value heads...
Singular values min: 1.1024863720e+00, max: 1.9375325441e+00
SVD-based factorization completed in 0.0377 seconds
SVD reconstruction relative error: 0.694895
TPA reconstruction relative error: 0.694895
Final TPA projection matrices with practical ranks - Q: 96, K: 96, V: 96
B projection shapes: W_B_q=torch.Size([1152, 24576]), W_B_k=torch.Size([1152, 24576]), W_B_v=torch.Size([1152, 24576])
Note: Initial analysis found high intrinsic ranks for K,V (needed for 95% energy: 221,240)
      but we're using practical ranks capped by Tucker decomposition: 96,96
Verifying reconstruction quality of SVD-based TPA factors...
Debug - q_weights_reshaped shape: torch.Size([1152, 4, 256])
Debug - k_weights_reshaped shape: torch.Size([1152, 1, 256])
Debug - actual_q_rank for head 0: 96, q_rank: 96
Debug - actual_q_rank for head 1: 96, q_rank: 96
Debug - actual_q_rank for head 2: 96, q_rank: 96
Debug - actual_q_rank for head 3: 96, q_rank: 96
Debug - actual_k_rank for head 0: 96, k_rank: 96
Debug - actual_v_rank for head 0: 96, v_rank: 96
Using OPTIMIZED COMPONENT-SPECIFIC ranks for factorized weights - Q: 96, K: 96, V: 96
These ranks were carefully selected based on the intrinsic structure of each component
K & V ranks reflect their true rank structure and are within Tucker decomposition constraints
Debug - After reshape: q_recon shape: torch.Size([1152, 1024]), q_weight shape: torch.Size([1152, 1024])
Reconstruction relative errors - Q: 0.8872, K: 0.5071, V: 0.6949
GQA to TPA conversion complete in 2.08 seconds
  Decomposition completed in 2.08 seconds
  Factorized weights keys: ['Q_core', 'Q_hidden_factor', 'Q_dim_factor', 'Q_head_factor', 'K_core', 'K_hidden_factor', 'K_dim_factor', 'K_head_factor', 'V_core', 'V_hidden_factor', 'V_dim_factor', 'V_head_factor', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 2.08 seconds
GQA to TPA conversion complete: 26/26 layers converted in 62.64 seconds
Copying factorized TPA weights...
  Found factorized module: model.layers.0.self_attn
  Layer index: 0
  Recorded ranks for layer 0: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.1.self_attn
  Layer index: 1
  Recorded ranks for layer 1: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.2.self_attn
  Layer index: 2
  Recorded ranks for layer 2: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.3.self_attn
  Layer index: 3
  Recorded ranks for layer 3: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.4.self_attn
  Layer index: 4
  Recorded ranks for layer 4: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.5.self_attn
  Layer index: 5
  Recorded ranks for layer 5: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.6.self_attn
  Layer index: 6
  Recorded ranks for layer 6: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.7.self_attn
  Layer index: 7
  Recorded ranks for layer 7: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.8.self_attn
  Layer index: 8
  Recorded ranks for layer 8: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.9.self_attn
  Layer index: 9
  Recorded ranks for layer 9: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.10.self_attn
  Layer index: 10
  Recorded ranks for layer 10: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.11.self_attn
  Layer index: 11
  Recorded ranks for layer 11: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.12.self_attn
  Layer index: 12
  Recorded ranks for layer 12: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.13.self_attn
  Layer index: 13
  Recorded ranks for layer 13: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.14.self_attn
  Layer index: 14
  Recorded ranks for layer 14: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.15.self_attn
  Layer index: 15
  Recorded ranks for layer 15: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.16.self_attn
  Layer index: 16
  Recorded ranks for layer 16: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.17.self_attn
  Layer index: 17
  Recorded ranks for layer 17: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.18.self_attn
  Layer index: 18
  Recorded ranks for layer 18: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.19.self_attn
  Layer index: 19
  Recorded ranks for layer 19: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.20.self_attn
  Layer index: 20
  Recorded ranks for layer 20: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.21.self_attn
  Layer index: 21
  Recorded ranks for layer 21: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.22.self_attn
  Layer index: 22
  Recorded ranks for layer 22: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.23.self_attn
  Layer index: 23
  Recorded ranks for layer 23: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.24.self_attn
  Layer index: 24
  Recorded ranks for layer 24: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
  Found factorized module: model.layers.25.self_attn
  Layer index: 25
  Recorded ranks for layer 25: {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}
  Source W_A_q shape: torch.Size([1152, 384])
  Source W_A_q shape: torch.Size([1152, 384])
  Unknown B matrix with dimensions [out=384, in=1152]
  Creating W_A_q with in_features=1152, out_features=384
  W_A_q transposing weight from torch.Size([1152, 384]) to torch.Size([384, 1152])
  Created W_A_q with shape torch.Size([384, 1152])
  Source W_A_k shape: torch.Size([1152, 96])
  Source W_A_k shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_k with in_features=1152, out_features=96
  W_A_k transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_k with shape torch.Size([96, 1152])
  Source W_A_v shape: torch.Size([1152, 96])
  Source W_A_v shape: torch.Size([1152, 96])
  Unknown B matrix with dimensions [out=96, in=1152]
  Creating W_A_v with in_features=1152, out_features=96
  W_A_v transposing weight from torch.Size([1152, 96]) to torch.Size([96, 1152])
  Created W_A_v with shape torch.Size([96, 1152])
  Source W_B_q shape: torch.Size([1152, 24576])
  Source W_B_q shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using actual tensor dimensions: 24576
  Creating W_B_q with in_features=1152, out_features=24576
  W_B_q transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_q with shape torch.Size([24576, 1152])
  Source W_B_k shape: torch.Size([1152, 24576])
  Source W_B_k shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using actual tensor dimensions: 24576
  Creating W_B_k with in_features=1152, out_features=24576
  W_B_k transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_k with shape torch.Size([24576, 1152])
  Source W_B_v shape: torch.Size([1152, 24576])
  Source W_B_v shape: torch.Size([1152, 24576])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using actual tensor dimensions: 24576
  Creating W_B_v with in_features=1152, out_features=24576
  W_B_v transposing weight from torch.Size([1152, 24576]) to torch.Size([24576, 1152])
  Created W_B_v with shape torch.Size([24576, 1152])
Storing layer-specific ranks in model config: [{'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}, {'q_rank': 96, 'k_rank': 96, 'v_rank': 96}]
  Found factorized module: model.layers.0.self_attn
  W_A_k weight distribution: 0.2930 mean, 0.2373 std
  W_A_q weight distribution: 0.2148 mean, 0.1787 std
  W_A_v weight distribution: 0.2930 mean, 0.2363 std
  W_B_k weight distribution: 0.5781 mean, 0.5508 std
  W_B_q weight distribution: 0.2168 mean, 0.2012 std
  W_B_v weight distribution: 0.6211 mean, 0.5000 std
  Found factorized module: model.layers.1.self_attn
  W_A_k weight distribution: 0.3066 mean, 0.2402 std
  W_A_q weight distribution: 0.2080 mean, 0.1729 std
  W_A_v weight distribution: 0.3008 mean, 0.2344 std
  W_B_k weight distribution: 0.6211 mean, 0.5469 std
  W_B_q weight distribution: 0.2129 mean, 0.1875 std
  W_B_v weight distribution: 0.6406 mean, 0.4922 std
  Found factorized module: model.layers.2.self_attn
  W_A_k weight distribution: 0.3379 mean, 0.2656 std
  W_A_q weight distribution: 0.1914 mean, 0.1553 std
  W_A_v weight distribution: 0.3281 mean, 0.2539 std
  W_B_k weight distribution: 0.6758 mean, 0.6133 std
  W_B_q weight distribution: 0.1953 mean, 0.1719 std
  W_B_v weight distribution: 0.6953 mean, 0.5391 std
  Found factorized module: model.layers.3.self_attn
  W_A_k weight distribution: 0.2910 mean, 0.2402 std
  W_A_q weight distribution: 0.2256 mean, 0.1875 std
  W_A_v weight distribution: 0.2734 mean, 0.2207 std
  W_B_k weight distribution: 0.5664 mean, 0.5664 std
  W_B_q weight distribution: 0.2314 mean, 0.2031 std
  W_B_v weight distribution: 0.5586 mean, 0.4922 std
  Found factorized module: model.layers.4.self_attn
  W_A_k weight distribution: 0.2812 mean, 0.2217 std
  W_A_q weight distribution: 0.2432 mean, 0.2021 std
  W_A_v weight distribution: 0.2578 mean, 0.2090 std
  W_B_k weight distribution: 0.5547 mean, 0.5156 std
  W_B_q weight distribution: 0.2500 mean, 0.2246 std
  W_B_v weight distribution: 0.5586 mean, 0.4297 std
  Found factorized module: model.layers.5.self_attn
  W_A_k weight distribution: 0.2930 mean, 0.2988 std
  W_A_q weight distribution: 0.2109 mean, 0.1797 std
  W_A_v weight distribution: 0.2930 mean, 0.2598 std
  W_B_k weight distribution: 0.5938 mean, 0.6602 std
  W_B_q weight distribution: 0.2090 mean, 0.2139 std
  W_B_v weight distribution: 0.6602 mean, 0.5039 std
  Found factorized module: model.layers.6.self_attn
  W_A_k weight distribution: 0.3398 mean, 0.2695 std
  W_A_q weight distribution: 0.2031 mean, 0.1611 std
  W_A_v weight distribution: 0.3105 mean, 0.2480 std
  W_B_k weight distribution: 0.6680 mean, 0.6289 std
  W_B_q weight distribution: 0.2051 mean, 0.1846 std
  W_B_v weight distribution: 0.6523 mean, 0.5312 std
  Found factorized module: model.layers.7.self_attn
  W_A_k weight distribution: 0.3203 mean, 0.2559 std
  W_A_q weight distribution: 0.2148 mean, 0.1719 std
  W_A_v weight distribution: 0.3008 mean, 0.2344 std
  W_B_k weight distribution: 0.6016 mean, 0.6250 std
  W_B_q weight distribution: 0.2207 mean, 0.2021 std
  W_B_v weight distribution: 0.6289 mean, 0.5078 std
  Found factorized module: model.layers.8.self_attn
  W_A_k weight distribution: 0.3145 mean, 0.2695 std
  W_A_q weight distribution: 0.2178 mean, 0.1816 std
  W_A_v weight distribution: 0.2852 mean, 0.2393 std
  W_B_k weight distribution: 0.5742 mean, 0.6641 std
  W_B_q weight distribution: 0.2168 mean, 0.1992 std
  W_B_v weight distribution: 0.6094 mean, 0.5000 std
  Found factorized module: model.layers.9.self_attn
  W_A_k weight distribution: 0.3496 mean, 0.2793 std
  W_A_q weight distribution: 0.1982 mean, 0.1592 std
  W_A_v weight distribution: 0.3223 mean, 0.2578 std
  W_B_k weight distribution: 0.6484 mean, 0.6914 std
  W_B_q weight distribution: 0.1924 mean, 0.1963 std
  W_B_v weight distribution: 0.6641 mean, 0.5703 std
  Found factorized module: model.layers.10.self_attn
  W_A_k weight distribution: 0.2793 mean, 0.2217 std
  W_A_q weight distribution: 0.2471 mean, 0.2021 std
  W_A_v weight distribution: 0.2520 mean, 0.2168 std
  W_B_k weight distribution: 0.5547 mean, 0.5156 std
  W_B_q weight distribution: 0.2559 mean, 0.2236 std
  W_B_v weight distribution: 0.5508 mean, 0.4453 std
  Found factorized module: model.layers.11.self_attn
  W_A_k weight distribution: 0.2754 mean, 0.2451 std
  W_A_q weight distribution: 0.2402 mean, 0.2080 std
  W_A_v weight distribution: 0.2637 mean, 0.2158 std
  W_B_k weight distribution: 0.4844 mean, 0.6133 std
  W_B_q weight distribution: 0.2324 mean, 0.2520 std
  W_B_v weight distribution: 0.5664 mean, 0.4492 std
  Found factorized module: model.layers.12.self_attn
  W_A_k weight distribution: 0.3516 mean, 0.2793 std
  W_A_q weight distribution: 0.1973 mean, 0.1602 std
  W_A_v weight distribution: 0.3223 mean, 0.2598 std
  W_B_k weight distribution: 0.6836 mean, 0.6641 std
  W_B_q weight distribution: 0.1914 mean, 0.1865 std
  W_B_v weight distribution: 0.6914 mean, 0.5391 std
  Found factorized module: model.layers.13.self_attn
  W_A_k weight distribution: 0.3340 mean, 0.2637 std
  W_A_q weight distribution: 0.2080 mean, 0.1689 std
  W_A_v weight distribution: 0.3047 mean, 0.2432 std
  W_B_k weight distribution: 0.6367 mean, 0.6445 std
  W_B_q weight distribution: 0.1924 mean, 0.2109 std
  W_B_v weight distribution: 0.6562 mean, 0.5039 std
  Found factorized module: model.layers.14.self_attn
  W_A_k weight distribution: 0.3105 mean, 0.2451 std
  W_A_q weight distribution: 0.2246 mean, 0.1875 std
  W_A_v weight distribution: 0.2715 mean, 0.2471 std
  W_B_k weight distribution: 0.5625 mean, 0.6211 std
  W_B_q weight distribution: 0.2061 mean, 0.2334 std
  W_B_v weight distribution: 0.6172 mean, 0.4746 std
  Found factorized module: model.layers.15.self_attn
  W_A_k weight distribution: 0.3555 mean, 0.2773 std
  W_A_q weight distribution: 0.1973 mean, 0.1621 std
  W_A_v weight distribution: 0.3242 mean, 0.2676 std
  W_B_k weight distribution: 0.6914 mean, 0.6602 std
  W_B_q weight distribution: 0.1953 mean, 0.1934 std
  W_B_v weight distribution: 0.7031 mean, 0.5469 std
  Found factorized module: model.layers.16.self_attn
  W_A_k weight distribution: 0.3047 mean, 0.2393 std
  W_A_q weight distribution: 0.2246 mean, 0.1816 std
  W_A_v weight distribution: 0.2832 mean, 0.2295 std
  W_B_k weight distribution: 0.6016 mean, 0.5625 std
  W_B_q weight distribution: 0.2217 mean, 0.2100 std
  W_B_v weight distribution: 0.6055 mean, 0.4785 std
  Found factorized module: model.layers.17.self_attn
  W_A_k weight distribution: 0.2852 mean, 0.2520 std
  W_A_q weight distribution: 0.2393 mean, 0.2061 std
  W_A_v weight distribution: 0.2578 mean, 0.2246 std
  W_B_k weight distribution: 0.4766 mean, 0.6523 std
  W_B_q weight distribution: 0.2373 mean, 0.2520 std
  W_B_v weight distribution: 0.5703 mean, 0.4492 std
  Found factorized module: model.layers.18.self_attn
  W_A_k weight distribution: 0.3398 mean, 0.2812 std
  W_A_q weight distribution: 0.1992 mean, 0.1729 std
  W_A_v weight distribution: 0.3145 mean, 0.2676 std
  W_B_k weight distribution: 0.6680 mean, 0.6523 std
  W_B_q weight distribution: 0.1846 mean, 0.2002 std
  W_B_v weight distribution: 0.6914 mean, 0.5352 std
  Found factorized module: model.layers.19.self_attn
  W_A_k weight distribution: 0.2676 mean, 0.2158 std
  W_A_q weight distribution: 0.2393 mean, 0.1982 std
  W_A_v weight distribution: 0.2578 mean, 0.2100 std
  W_B_k weight distribution: 0.5430 mean, 0.4863 std
  W_B_q weight distribution: 0.2480 mean, 0.2178 std
  W_B_v weight distribution: 0.5352 mean, 0.4609 std
  Found factorized module: model.layers.20.self_attn
  W_A_k weight distribution: 0.3164 mean, 0.2520 std
  W_A_q weight distribution: 0.2139 mean, 0.1777 std
  W_A_v weight distribution: 0.2891 mean, 0.2344 std
  W_B_k weight distribution: 0.6211 mean, 0.5898 std
  W_B_q weight distribution: 0.2070 mean, 0.2119 std
  W_B_v weight distribution: 0.6211 mean, 0.4883 std
  Found factorized module: model.layers.21.self_attn
  W_A_k weight distribution: 0.3574 mean, 0.2812 std
  W_A_q weight distribution: 0.1895 mean, 0.1543 std
  W_A_v weight distribution: 0.3340 mean, 0.2676 std
  W_B_k weight distribution: 0.7109 mean, 0.6562 std
  W_B_q weight distribution: 0.1885 mean, 0.1748 std
  W_B_v weight distribution: 0.7070 mean, 0.5664 std
  Found factorized module: model.layers.22.self_attn
  W_A_k weight distribution: 0.3340 mean, 0.2617 std
  W_A_q weight distribution: 0.1973 mean, 0.1611 std
  W_A_v weight distribution: 0.3105 mean, 0.2520 std
  W_B_k weight distribution: 0.6641 mean, 0.6094 std
  W_B_q weight distribution: 0.1914 mean, 0.1992 std
  W_B_v weight distribution: 0.6445 mean, 0.5508 std
  Found factorized module: model.layers.23.self_attn
  W_A_k weight distribution: 0.3477 mean, 0.3086 std
  W_A_q weight distribution: 0.1836 mean, 0.1602 std
  W_A_v weight distribution: 0.3301 mean, 0.2930 std
  W_B_k weight distribution: 0.6914 mean, 0.7031 std
  W_B_q weight distribution: 0.1787 mean, 0.1826 std
  W_B_v weight distribution: 0.7422 mean, 0.5703 std
  Found factorized module: model.layers.24.self_attn
  W_A_k weight distribution: 0.3145 mean, 0.2441 std
  W_A_q weight distribution: 0.2168 mean, 0.1748 std
  W_A_v weight distribution: 0.2891 mean, 0.2305 std
  W_B_k weight distribution: 0.6211 mean, 0.5742 std
  W_B_q weight distribution: 0.2178 mean, 0.2178 std
  W_B_v weight distribution: 0.6172 mean, 0.4844 std
  Found factorized module: model.layers.25.self_attn
  W_A_k weight distribution: 0.2695 mean, 0.2109 std
  W_A_q weight distribution: 0.2480 mean, 0.2070 std
  W_A_v weight distribution: 0.2490 mean, 0.2002 std
  W_B_k weight distribution: 0.5312 mean, 0.4941 std
  W_B_q weight distribution: 0.2266 mean, 0.2559 std
  W_B_v weight distribution: 0.5234 mean, 0.4297 std
TPA model creation complete in 205.21 seconds
Successfully created TPA model with factorized weights
Model converted to TPA in 207.32 seconds
Saving TPA model to tpa_model.pt...
TPA model saved successfully
Model moved to cuda:0 in 234.08 seconds
Generating response with temperature=0.9, top_p=0.95, top_k=64...
Using simple prompt format for debugging
Using Gemma3ForMultimodalLMwithTPA generate() interface
DEBUG GENERATE: Prompts: ['hey']
DEBUG GENERATE: Device: cuda:0, max_tokens: 100
DEBUG GENERATE: Temperature: 0.9, top_p: 0.95, top_k: 64
DEBUG GENERATE: Tokenized first prompt: [2, 20835]
DEBUG GENERATE: Decoded first tokens: ['<bos>', 'hey']
DEBUG GENERATE: Token tensor shape: torch.Size([1, 102])
DEBUG GENERATE: Prompt mask sum (non-pad tokens): 2
DEBUG GENERATE: Processing result: min_prompt_len=2, max_prompt_len=2, max_seq_len=102
DEBUG: Input token IDs shape: torch.Size([1, 2])
DEBUG: First few input tokens: [2, 20835]
DEBUG: Input positions: [0, 1]
DEBUG: Token embeddings shape: torch.Size([1, 2, 1152])
DEBUG: Token embeddings mean: 0.031250, std: 0.996094
DEBUG: Token embeddings min: -2.984375, max: 25.250000
DEBUG: Hidden states before model: mean=0.031250, std=0.996094
DEBUG: Hidden states after model: shape=torch.Size([1, 2, 1152])
DEBUG: Hidden states after model: mean=-0.083984, std=3.796875
DEBUG: Hidden states after model: min=-23.125000, max=52.750000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.500000, std=3.984375
DEBUG SAMPLER: Top 10 logits: [17.0, 14.8125, 14.8125, 14.625, 14.25, 14.25, 14.125, 14.0, 13.875, 13.75]
DEBUG SAMPLER: Top 10 indices: [950, 102315, 19843, 53441, 209498, 238021, 41085, 94922, 6639, 2409]
DEBUG SAMPLER: Top 10 probs: [0.282718300819397, 0.0317201130092144, 0.0317201130092144, 0.02629689686000347, 0.01807357557117939, 0.01807357557117939, 0.015949875116348267, 0.01407571416348219, 0.012421774677932262, 0.010962177999317646]
DEBUG SAMPLER: Top 10 token indices: [950, 19843, 102315, 53441, 209498, 238021, 41085, 94922, 6639, 2409]
DEBUG SAMPLER: Probability sum: 1.0000004768371582
DEBUG SAMPLER: Special token probabilities: {0: 8.217751479300262e-10, 1: 5.5403845067303337e-08, 2: 3.425668637646595e-10, 3: 3.5900696304658197e-10, 4: 1.4422603111796661e-09}
DEBUG SAMPLER: Final top 5 probs: [0.412925660610199, 0.04632897302508354, 0.04632897302508354, 0.0384080670773983, 0.026397453621029854]
DEBUG SAMPLER: Final top 5 indices: [950, 19843, 102315, 53441, 209498]
DEBUG SAMPLER: Sampled token: 11687
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.500000, std=3.984375
DEBUG: Logits min/max: min=-20.500000, max=17.000000
DEBUG: Top 10 token IDs: [950, 102315, 19843, 53441, 209498, 238021, 41085, 94922, 6639, 2409]
DEBUG: Top 10 probabilities: [0.283203125, 0.03173828125, 0.03173828125, 0.0262451171875, 0.01806640625, 0.01806640625, 0.0159912109375, 0.01409912109375, 0.012451171875, 0.010986328125]
DEBUG: Selected next token: 11687
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [11687]
DEBUG: Input positions: [2]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.002975, std: 0.988281
DEBUG: Token embeddings min: -3.968750, max: 3.656250
DEBUG: Hidden states before model: mean=-0.002975, std=0.988281
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.245117, std=5.406250
DEBUG: Hidden states after model: min=-21.625000, max=17.750000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.109375, std=5.250000
DEBUG SAMPLER: Top 10 logits: [20.25, 20.0, 19.875, 19.875, 18.75, 18.75, 18.625, 18.5, 18.375, 18.375]
DEBUG SAMPLER: Top 10 indices: [120395, 203201, 121338, 225149, 236789, 50494, 214846, 177237, 45693, 213508]
DEBUG SAMPLER: Top 10 probs: [0.14225804805755615, 0.1107906773686409, 0.09777243435382843, 0.09777243435382843, 0.03174206241965294, 0.03174206241965294, 0.028012268245220184, 0.02472073957324028, 0.02181597799062729, 0.02181597799062729]
DEBUG SAMPLER: Top 10 token indices: [120395, 203201, 121338, 225149, 50494, 236789, 214846, 177237, 45693, 213508]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 4.18249679681737e-12, 1: 1.891294898470619e-10, 2: 4.5674193593914225e-11, 3: 7.498804095035227e-13, 4: 1.401379429166738e-10}
DEBUG SAMPLER: Final top 5 probs: [0.1639491319656372, 0.12768371403217316, 0.11268048733472824, 0.11268048733472824, 0.03658200055360794]
DEBUG SAMPLER: Final top 5 indices: [120395, 203201, 121338, 225149, 50494]
DEBUG SAMPLER: Sampled token: 203201
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.109375, std=5.250000
DEBUG: Logits min/max: min=-27.625000, max=20.250000
DEBUG: Top 10 token IDs: [120395, 203201, 121338, 225149, 236789, 50494, 214846, 177237, 45693, 213508]
DEBUG: Top 10 probabilities: [0.142578125, 0.11083984375, 0.09765625, 0.09765625, 0.03173828125, 0.03173828125, 0.0279541015625, 0.0247802734375, 0.0218505859375, 0.0218505859375]
DEBUG: Selected next token: 203201
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [203201]
DEBUG: Input positions: [3]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.026245, std: 0.953125
DEBUG: Token embeddings min: -2.968750, max: 3.218750
DEBUG: Hidden states before model: mean=0.026245, std=0.953125
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.049561, std=4.625000
DEBUG: Hidden states after model: min=-13.312500, max=18.375000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.718750, std=4.625000
DEBUG SAMPLER: Top 10 logits: [18.75, 18.625, 18.0, 18.0, 17.625, 16.5, 16.5, 16.25, 16.0, 15.625]
DEBUG SAMPLER: Top 10 indices: [13203, 231049, 132286, 245520, 142638, 1882, 219018, 98880, 19251, 188965]
DEBUG SAMPLER: Top 10 probs: [0.1859912872314453, 0.16413673758506775, 0.08785606920719147, 0.08785606920719147, 0.060382530093193054, 0.019603336229920387, 0.019603336229920387, 0.01526709459722042, 0.01189002487808466, 0.008171887136995792]
DEBUG SAMPLER: Top 10 token indices: [13203, 231049, 132286, 245520, 142638, 1882, 219018, 98880, 19251, 188965]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 1.2201401998132577e-12, 1: 9.656425659088086e-10, 2: 1.008490926324157e-06, 3: 1.4264881593659129e-12, 4: 1.737818569602645e-11}
DEBUG SAMPLER: Final top 5 probs: [0.22542640566825867, 0.19893810153007507, 0.10648389905691147, 0.10648389905691147, 0.07318523526191711]
DEBUG SAMPLER: Final top 5 indices: [13203, 231049, 132286, 245520, 142638]
DEBUG SAMPLER: Sampled token: 13203
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.718750, std=4.625000
DEBUG: Logits min/max: min=-24.000000, max=18.750000
DEBUG: Top 10 token IDs: [13203, 231049, 132286, 245520, 142638, 1882, 219018, 98880, 19251, 188965]
DEBUG: Top 10 probabilities: [0.185546875, 0.1640625, 0.087890625, 0.087890625, 0.060302734375, 0.0196533203125, 0.0196533203125, 0.0152587890625, 0.01190185546875, 0.0081787109375]
DEBUG: Selected next token: 13203
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [13203]
DEBUG: Input positions: [4]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.047607, std: 0.972656
DEBUG: Token embeddings min: -3.062500, max: 3.343750
DEBUG: Hidden states before model: mean=0.047607, std=0.972656
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.157227, std=5.843750
DEBUG: Hidden states after model: min=-39.750000, max=20.000000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.796875, std=5.500000
DEBUG SAMPLER: Top 10 logits: [20.875, 20.375, 20.375, 20.125, 19.875, 19.875, 19.5, 19.125, 19.0, 19.0]
DEBUG SAMPLER: Top 10 indices: [1311, 59049, 10558, 2234, 4800, 236905, 28374, 72525, 139537, 161380]
DEBUG SAMPLER: Top 10 probs: [0.12883765995502472, 0.07814399152994156, 0.07814399152994156, 0.06085860729217529, 0.04739673435688019, 0.04739673435688019, 0.03257526457309723, 0.022388627752661705, 0.019757896661758423, 0.019757896661758423]
DEBUG SAMPLER: Top 10 token indices: [1311, 10558, 59049, 2234, 4800, 236905, 28374, 72525, 139537, 161380]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 5.630251581796586e-13, 1: 2.441991275858868e-09, 2: 3.4489470741166928e-12, 3: 1.6642991238630112e-13, 4: 2.2523841041528943e-14}
DEBUG SAMPLER: Final top 5 probs: [0.15370845794677734, 0.09322889894247055, 0.09322889894247055, 0.07260674238204956, 0.05654619261622429]
DEBUG SAMPLER: Final top 5 indices: [1311, 10558, 59049, 2234, 4800]
DEBUG SAMPLER: Sampled token: 1311
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.796875, std=5.500000
DEBUG: Logits min/max: min=-27.625000, max=20.875000
DEBUG: Top 10 token IDs: [1311, 59049, 10558, 2234, 4800, 236905, 28374, 72525, 139537, 161380]
DEBUG: Top 10 probabilities: [0.12890625, 0.078125, 0.078125, 0.060791015625, 0.04736328125, 0.04736328125, 0.032470703125, 0.0223388671875, 0.019775390625, 0.019775390625]
DEBUG: Selected next token: 1311
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [1311]
DEBUG: Input positions: [5]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.052979, std: 1.101562
DEBUG: Token embeddings min: -5.562500, max: 4.125000
DEBUG: Hidden states before model: mean=0.052979, std=1.101562
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.207031, std=4.468750
DEBUG: Hidden states after model: min=-19.375000, max=26.625000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.390625, std=4.312500
DEBUG SAMPLER: Top 10 logits: [17.25, 17.125, 16.0, 16.0, 15.8125, 15.625, 15.0, 14.625, 14.1875, 14.125]
DEBUG SAMPLER: Top 10 indices: [17707, 817, 1072, 5899, 502, 66800, 17230, 8207, 1389, 13471]
DEBUG SAMPLER: Top 10 probs: [0.21880914270877838, 0.19309839606285095, 0.062689870595932, 0.062689870595932, 0.05197172984480858, 0.04308607429265976, 0.023062312975525856, 0.015850482508540154, 0.01023384090512991, 0.009613803587853909]
DEBUG SAMPLER: Top 10 token indices: [17707, 817, 1072, 5899, 502, 66800, 17230, 8207, 1389, 13471]
DEBUG SAMPLER: Probability sum: 1.0000007152557373
DEBUG SAMPLER: Special token probabilities: {0: 1.0280159536135702e-11, 1: 1.228523238694379e-08, 2: 6.287888254519203e-08, 3: 3.5527277734287432e-12, 4: 2.2596001514330055e-08}
DEBUG SAMPLER: Final top 5 probs: [0.2681168019771576, 0.2366122454404831, 0.07681675255298615, 0.07681675255298615, 0.06368332356214523]
DEBUG SAMPLER: Final top 5 indices: [17707, 817, 1072, 5899, 502]
DEBUG SAMPLER: Sampled token: 502
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.390625, std=4.312500
DEBUG: Logits min/max: min=-23.500000, max=17.250000
DEBUG: Top 10 token IDs: [17707, 817, 1072, 5899, 502, 66800, 17230, 8207, 1389, 13471]
DEBUG: Top 10 probabilities: [0.21875, 0.193359375, 0.0625, 0.0625, 0.052001953125, 0.04296875, 0.0230712890625, 0.015869140625, 0.01025390625, 0.0096435546875]
DEBUG: Selected next token: 502
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [502]
DEBUG: Input positions: [6]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.004333, std: 0.980469
DEBUG: Token embeddings min: -3.796875, max: 4.250000
DEBUG: Hidden states before model: mean=-0.004333, std=0.980469
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.166016, std=4.843750
DEBUG: Hidden states after model: min=-21.750000, max=20.250000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.031250, std=4.718750
DEBUG SAMPLER: Top 10 logits: [19.125, 19.0, 19.0, 18.875, 18.75, 18.375, 17.875, 17.75, 17.625, 17.625]
DEBUG SAMPLER: Top 10 indices: [15850, 124557, 103391, 130693, 71950, 211067, 163901, 91931, 208525, 237268]
DEBUG SAMPLER: Top 10 probs: [0.0843135267496109, 0.07440643012523651, 0.07440643012523651, 0.06566344201564789, 0.057947784662246704, 0.0398268923163414, 0.024156231433153152, 0.021317798644304276, 0.018812891095876694, 0.018812891095876694]
DEBUG SAMPLER: Top 10 token indices: [15850, 103391, 124557, 130693, 71950, 211067, 163901, 91931, 208525, 237268]
DEBUG SAMPLER: Probability sum: 1.0000004768371582
DEBUG SAMPLER: Special token probabilities: {0: 6.3300501992880065e-12, 1: 8.243524085571607e-09, 2: 1.7037099812530765e-12, 3: 7.17288641677305e-12, 4: 3.3306846169978144e-09}
DEBUG SAMPLER: Final top 5 probs: [0.1188615933060646, 0.10489499568939209, 0.10489499568939209, 0.092569500207901, 0.08169230073690414]
DEBUG SAMPLER: Final top 5 indices: [15850, 103391, 124557, 130693, 71950]
DEBUG SAMPLER: Sampled token: 103391
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.031250, std=4.718750
DEBUG: Logits min/max: min=-23.625000, max=19.125000
DEBUG: Top 10 token IDs: [15850, 124557, 103391, 130693, 71950, 211067, 163901, 91931, 208525, 237268]
DEBUG: Top 10 probabilities: [0.08447265625, 0.07421875, 0.07421875, 0.0654296875, 0.057861328125, 0.039794921875, 0.024169921875, 0.0213623046875, 0.018798828125, 0.018798828125]
DEBUG: Selected next token: 103391
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [103391]
DEBUG: Input positions: [7]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.013794, std: 0.937500
DEBUG: Token embeddings min: -2.937500, max: 3.562500
DEBUG: Hidden states before model: mean=0.013794, std=0.937500
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.031494, std=3.843750
DEBUG: Hidden states after model: min=-14.375000, max=21.125000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-0.804688, std=3.765625
DEBUG SAMPLER: Top 10 logits: [16.125, 15.625, 15.1875, 14.9375, 14.8125, 14.625, 14.5625, 14.3125, 14.25, 14.25]
DEBUG SAMPLER: Top 10 indices: [49668, 85329, 236930, 239290, 121633, 237900, 11309, 23447, 18440, 59122]
DEBUG SAMPLER: Top 10 probs: [0.07626307755708694, 0.04625589773058891, 0.029865050688385963, 0.02325892448425293, 0.02052593044936657, 0.017016593366861343, 0.015985609963536263, 0.012449605390429497, 0.011695322580635548, 0.011695322580635548]
DEBUG SAMPLER: Top 10 token indices: [49668, 85329, 236930, 239290, 121633, 237900, 11309, 23447, 18440, 59122]
DEBUG SAMPLER: Probability sum: 1.0000001192092896
DEBUG SAMPLER: Special token probabilities: {0: 2.715998259628094e-10, 1: 3.5423294852954257e-10, 2: 1.4433173873840133e-06, 3: 1.431230689519225e-10, 4: 6.760653842308173e-11}
DEBUG SAMPLER: Final top 5 probs: [0.14187824726104736, 0.0860535129904747, 0.05556032061576843, 0.04327042028307915, 0.03818601369857788]
DEBUG SAMPLER: Final top 5 indices: [49668, 85329, 236930, 239290, 121633]
DEBUG SAMPLER: Sampled token: 1248
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-0.804688, std=3.765625
DEBUG: Logits min/max: min=-17.875000, max=16.125000
DEBUG: Top 10 token IDs: [49668, 85329, 236930, 239290, 121633, 237900, 11309, 23447, 18440, 59122]
DEBUG: Top 10 probabilities: [0.076171875, 0.046142578125, 0.0299072265625, 0.0233154296875, 0.0205078125, 0.0169677734375, 0.0159912109375, 0.012451171875, 0.01171875, 0.01171875]
DEBUG: Selected next token: 1248
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [1248]
DEBUG: Input positions: [8]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.036377, std: 1.023438
DEBUG: Token embeddings min: -10.562500, max: 3.875000
DEBUG: Hidden states before model: mean=-0.036377, std=1.023438
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.093262, std=5.718750
DEBUG: Hidden states after model: min=-18.625000, max=18.875000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-0.478516, std=5.718750
DEBUG SAMPLER: Top 10 logits: [22.875, 22.375, 22.375, 22.25, 22.125, 22.0, 21.75, 21.75, 21.75, 21.75]
DEBUG SAMPLER: Top 10 indices: [24305, 102717, 45284, 20374, 184023, 9821, 148403, 181019, 57426, 14520]
DEBUG SAMPLER: Top 10 probs: [0.07430066913366318, 0.045065633952617645, 0.045065633952617645, 0.03977028280496597, 0.0350971519947052, 0.030973127111792564, 0.02412189543247223, 0.02412189543247223, 0.02412189543247223, 0.02412189543247223]
DEBUG SAMPLER: Top 10 token indices: [24305, 45284, 102717, 20374, 184023, 9821, 14520, 57426, 148403, 181019]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 5.6423802126870023e-14, 1: 1.0525497834423803e-10, 2: 1.2202502327812378e-14, 3: 1.3401828136291004e-14, 4: 8.291417140544877e-13}
DEBUG SAMPLER: Final top 5 probs: [0.09549938142299652, 0.05792330205440521, 0.05792330205440521, 0.05111713707447052, 0.04511071369051933]
DEBUG SAMPLER: Final top 5 indices: [24305, 45284, 102717, 20374, 184023]
DEBUG SAMPLER: Sampled token: 53157
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-0.478516, std=5.718750
DEBUG: Logits min/max: min=-27.125000, max=22.875000
DEBUG: Top 10 token IDs: [24305, 102717, 45284, 20374, 184023, 9821, 148403, 181019, 57426, 14520]
DEBUG: Top 10 probabilities: [0.07421875, 0.045166015625, 0.045166015625, 0.039794921875, 0.03515625, 0.031005859375, 0.024169921875, 0.024169921875, 0.024169921875, 0.024169921875]
DEBUG: Selected next token: 53157
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [53157]
DEBUG: Input positions: [9]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.032959, std: 0.960938
DEBUG: Token embeddings min: -3.703125, max: 3.453125
DEBUG: Hidden states before model: mean=0.032959, std=0.960938
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.304688, std=5.687500
DEBUG: Hidden states after model: min=-44.500000, max=19.125000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.562500, std=5.500000
DEBUG SAMPLER: Top 10 logits: [24.0, 22.25, 21.375, 21.375, 21.125, 20.875, 20.375, 20.375, 20.25, 20.25]
DEBUG SAMPLER: Top 10 indices: [180046, 141382, 29940, 239044, 236843, 3086, 23684, 150042, 157975, 184872]
DEBUG SAMPLER: Top 10 probs: [0.5490567684173584, 0.0954117476940155, 0.03977353870868683, 0.03977353870868683, 0.030975664034485817, 0.024123869836330414, 0.014631866477429867, 0.014631866477429867, 0.012912577018141747, 0.012912577018141747]
DEBUG SAMPLER: Top 10 token indices: [180046, 141382, 29940, 239044, 236843, 3086, 23684, 150042, 157975, 184872]
DEBUG SAMPLER: Probability sum: 1.0
DEBUG SAMPLER: Special token probabilities: {0: 3.961896660894214e-15, 1: 1.598343202582897e-12, 2: 3.7963978748861704e-13, 3: 3.4963616356108564e-15, 4: 5.44953448686214e-12}
DEBUG SAMPLER: Final top 5 probs: [0.577800989151001, 0.10040674358606339, 0.041855763643980026, 0.041855763643980026, 0.03259730339050293]
DEBUG SAMPLER: Final top 5 indices: [180046, 141382, 29940, 239044, 236843]
DEBUG SAMPLER: Sampled token: 180046
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.562500, std=5.500000
DEBUG: Logits min/max: min=-29.625000, max=24.000000
DEBUG: Top 10 token IDs: [180046, 141382, 29940, 239044, 236843, 3086, 23684, 150042, 157975, 184872]
DEBUG: Top 10 probabilities: [0.55078125, 0.09521484375, 0.039794921875, 0.039794921875, 0.031005859375, 0.024169921875, 0.0146484375, 0.0146484375, 0.012939453125, 0.012939453125]
DEBUG: Selected next token: 180046
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [180046]
DEBUG: Input positions: [10]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.045410, std: 0.937500
DEBUG: Token embeddings min: -2.937500, max: 3.171875
DEBUG: Hidden states before model: mean=-0.045410, std=0.937500
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.179688, std=4.718750
DEBUG: Hidden states after model: min=-15.875000, max=14.937500
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.453125, std=4.656250
DEBUG SAMPLER: Top 10 logits: [22.25, 18.5, 18.5, 18.0, 17.75, 17.75, 17.75, 17.625, 17.375, 17.25]
DEBUG SAMPLER: Top 10 indices: [236829, 104827, 9558, 236789, 126836, 22876, 237611, 131463, 172851, 239212]
DEBUG SAMPLER: Top 10 probs: [0.7619963884353638, 0.017920438200235367, 0.017920438200235367, 0.01086929440498352, 0.008465016260743141, 0.008465016260743141, 0.008465016260743141, 0.007470350246876478, 0.005817913915961981, 0.005134291481226683]
DEBUG SAMPLER: Top 10 token indices: [236829, 9558, 104827, 236789, 22876, 126836, 237611, 131463, 172851, 239212]
DEBUG SAMPLER: Probability sum: 1.0000001192092896
DEBUG SAMPLER: Special token probabilities: {0: 2.649293002858488e-13, 1: 1.2745807076441906e-08, 2: 1.6769271357329756e-15, 3: 1.1043896103613698e-13, 4: 2.4223728925520938e-11}
DEBUG SAMPLER: Final top 5 probs: [0.8186508417129517, 0.019252821803092957, 0.019252821803092957, 0.011677426286041737, 0.009094390086829662]
DEBUG SAMPLER: Final top 5 indices: [236829, 9558, 104827, 236789, 22876]
DEBUG SAMPLER: Sampled token: 236829
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.453125, std=4.656250
DEBUG: Logits min/max: min=-21.125000, max=22.250000
DEBUG: Top 10 token IDs: [236829, 104827, 9558, 236789, 126836, 22876, 237611, 131463, 172851, 239212]
DEBUG: Top 10 probabilities: [0.76171875, 0.0179443359375, 0.0179443359375, 0.0108642578125, 0.00848388671875, 0.00848388671875, 0.00848388671875, 0.007476806640625, 0.005828857421875, 0.005126953125]
DEBUG: Selected next token: 236829
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [236829]
DEBUG: Input positions: [11]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.003387, std: 1.023438
DEBUG: Token embeddings min: -3.312500, max: 5.375000
DEBUG: Hidden states before model: mean=0.003387, std=1.023438
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.044189, std=4.875000
DEBUG: Hidden states after model: min=-52.500000, max=17.125000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.312500, std=4.500000
DEBUG SAMPLER: Top 10 logits: [20.375, 19.0, 18.375, 17.75, 17.5, 17.125, 17.0, 17.0, 17.0, 16.75]
DEBUG SAMPLER: Top 10 indices: [49457, 140887, 113045, 117982, 76676, 213513, 48787, 18625, 685, 49196]
DEBUG SAMPLER: Top 10 probs: [0.3858044147491455, 0.09754662960767746, 0.052212949842214584, 0.027947580441832542, 0.02176559716463089, 0.014959260821342468, 0.013201501220464706, 0.013201501220464706, 0.013201501220464706, 0.010281339287757874]
DEBUG SAMPLER: Top 10 token indices: [49457, 140887, 113045, 117982, 76676, 213513, 685, 18625, 48787, 49196]
DEBUG SAMPLER: Probability sum: 1.0000004768371582
DEBUG SAMPLER: Special token probabilities: {0: 2.1615835787969218e-09, 1: 1.1438741154279342e-07, 2: 2.0508535314434084e-08, 3: 1.1302101476928783e-09, 4: 4.87120432879351e-09}
DEBUG SAMPLER: Final top 5 probs: [0.4603722095489502, 0.11640031635761261, 0.062304601073265076, 0.03334925323724747, 0.02597242407500744]
DEBUG SAMPLER: Final top 5 indices: [49457, 140887, 113045, 117982, 76676]
DEBUG SAMPLER: Sampled token: 66459
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.312500, std=4.500000
DEBUG: Logits min/max: min=-22.000000, max=20.375000
DEBUG: Top 10 token IDs: [49457, 140887, 113045, 117982, 76676, 213513, 48787, 18625, 685, 49196]
DEBUG: Top 10 probabilities: [0.38671875, 0.09765625, 0.05224609375, 0.0279541015625, 0.021728515625, 0.01495361328125, 0.01318359375, 0.01318359375, 0.01318359375, 0.01025390625]
DEBUG: Selected next token: 66459
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [66459]
DEBUG: Input positions: [12]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.036865, std: 0.937500
DEBUG: Token embeddings min: -3.234375, max: 3.343750
DEBUG: Hidden states before model: mean=-0.036865, std=0.937500
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.068359, std=4.656250
DEBUG: Hidden states after model: min=-14.437500, max=17.125000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.296875, std=4.500000
DEBUG SAMPLER: Top 10 logits: [21.75, 20.375, 20.0, 18.625, 18.625, 18.5, 17.75, 17.75, 17.75, 17.625]
DEBUG SAMPLER: Top 10 indices: [236808, 1943, 239498, 253826, 223722, 139038, 135102, 105726, 235902, 37001]
DEBUG SAMPLER: Top 10 probs: [0.5145537853240967, 0.1300995647907257, 0.0894160270690918, 0.022607915103435516, 0.022607915103435516, 0.019951414316892624, 0.009424380958080292, 0.009424380958080292, 0.009424380958080292, 0.008316987194120884]
DEBUG SAMPLER: Top 10 token indices: [236808, 1943, 239498, 223722, 253826, 139038, 105726, 135102, 235902, 37001]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 2.7984507072753573e-12, 1: 1.2920439829144925e-08, 2: 7.239060542845981e-17, 3: 2.319997059047063e-12, 4: 6.052073447428141e-13}
DEBUG SAMPLER: Final top 5 probs: [0.5502113103866577, 0.13911519944667816, 0.09561237692832947, 0.02417459897696972, 0.02417459897696972]
DEBUG SAMPLER: Final top 5 indices: [236808, 1943, 239498, 223722, 253826]
DEBUG SAMPLER: Sampled token: 2175
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.296875, std=4.500000
DEBUG: Logits min/max: min=-21.625000, max=21.750000
DEBUG: Top 10 token IDs: [236808, 1943, 239498, 253826, 223722, 139038, 135102, 105726, 235902, 37001]
DEBUG: Top 10 probabilities: [0.515625, 0.1298828125, 0.08935546875, 0.0225830078125, 0.0225830078125, 0.0198974609375, 0.0093994140625, 0.0093994140625, 0.0093994140625, 0.00830078125]
DEBUG: Selected next token: 2175
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [2175]
DEBUG: Input positions: [13]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.035889, std: 0.968750
DEBUG: Token embeddings min: -2.968750, max: 2.906250
DEBUG: Hidden states before model: mean=-0.035889, std=0.968750
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.089355, std=5.312500
DEBUG: Hidden states after model: min=-25.625000, max=19.125000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.914062, std=5.250000
DEBUG SAMPLER: Top 10 logits: [21.25, 21.125, 21.125, 21.125, 20.5, 20.125, 19.875, 19.75, 18.875, 18.75]
DEBUG SAMPLER: Top 10 indices: [13829, 23290, 68984, 71049, 45951, 141530, 2292, 244602, 19540, 114507]
DEBUG SAMPLER: Top 10 probs: [0.13698184490203857, 0.12088605016469955, 0.12088605016469955, 0.12088605016469955, 0.06470564007759094, 0.04447149112820625, 0.034634433686733246, 0.03056478127837181, 0.012741295620799065, 0.011244154535233974]
DEBUG SAMPLER: Top 10 token indices: [13829, 23290, 68984, 71049, 45951, 141530, 2292, 244602, 19540, 114507]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 3.909343772231242e-15, 1: 1.7295460619237701e-09, 2: 8.986283939194095e-13, 3: 2.2274768462289974e-15, 4: 4.622980756405353e-17}
DEBUG SAMPLER: Final top 5 probs: [0.15288768708705902, 0.13492290675640106, 0.13492290675640106, 0.13492290675640106, 0.07221902161836624]
DEBUG SAMPLER: Final top 5 indices: [13829, 23290, 68984, 71049, 45951]
DEBUG SAMPLER: Sampled token: 23290
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.914062, std=5.250000
DEBUG: Logits min/max: min=-24.500000, max=21.250000
DEBUG: Top 10 token IDs: [13829, 23290, 68984, 71049, 45951, 141530, 2292, 244602, 19540, 114507]
DEBUG: Top 10 probabilities: [0.13671875, 0.12109375, 0.12109375, 0.12109375, 0.06494140625, 0.04443359375, 0.03466796875, 0.030517578125, 0.01275634765625, 0.01123046875]
DEBUG: Selected next token: 23290
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [23290]
DEBUG: Input positions: [14]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.038086, std: 0.953125
DEBUG: Token embeddings min: -3.171875, max: 3.406250
DEBUG: Hidden states before model: mean=0.038086, std=0.953125
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.148438, std=5.000000
DEBUG: Hidden states after model: min=-16.000000, max=19.500000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.437500, std=5.031250
DEBUG SAMPLER: Top 10 logits: [23.25, 19.0, 19.0, 18.625, 18.625, 18.5, 18.25, 17.875, 17.75, 17.75]
DEBUG SAMPLER: Top 10 indices: [8674, 81627, 65566, 200994, 507, 101236, 54093, 88392, 691, 5581]
DEBUG SAMPLER: Top 10 probs: [0.836133599281311, 0.011926804669201374, 0.011926804669201374, 0.00819716602563858, 0.00819716602563858, 0.007233973126858473, 0.005633823573589325, 0.003872066969051957, 0.003417087020352483, 0.003417087020352483]
DEBUG SAMPLER: Top 10 token indices: [8674, 65566, 81627, 507, 200994, 101236, 54093, 88392, 691, 5581]
DEBUG SAMPLER: Probability sum: 1.0000001192092896
DEBUG SAMPLER: Special token probabilities: {0: 3.22944270744795e-15, 1: 5.234018080058078e-12, 2: 3.10757947152851e-11, 3: 4.414124586817969e-15, 4: 2.6773017396813925e-15}
DEBUG SAMPLER: Final top 5 probs: [0.879837691783905, 0.012550209648907185, 0.012550209648907185, 0.008625625632703304, 0.008625625632703304]
DEBUG SAMPLER: Final top 5 indices: [8674, 65566, 81627, 507, 200994]
DEBUG SAMPLER: Sampled token: 8674
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.437500, std=5.031250
DEBUG: Logits min/max: min=-25.000000, max=23.250000
DEBUG: Top 10 token IDs: [8674, 81627, 65566, 200994, 507, 101236, 54093, 88392, 691, 5581]
DEBUG: Top 10 probabilities: [0.8359375, 0.01190185546875, 0.01190185546875, 0.0081787109375, 0.0081787109375, 0.007232666015625, 0.005645751953125, 0.003875732421875, 0.00341796875, 0.00341796875]
DEBUG: Selected next token: 8674
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [8674]
DEBUG: Input positions: [15]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.040527, std: 0.960938
DEBUG: Token embeddings min: -3.343750, max: 2.843750
DEBUG: Hidden states before model: mean=-0.040527, std=0.960938
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.102051, std=4.875000
DEBUG: Hidden states after model: min=-19.625000, max=17.250000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.203125, std=4.843750
DEBUG SAMPLER: Top 10 logits: [22.25, 18.25, 17.875, 17.5, 17.375, 17.125, 17.125, 17.0, 16.875, 16.75]
DEBUG SAMPLER: Top 10 indices: [236743, 787, 496, 150180, 4305, 237534, 72745, 8026, 99163, 534]
DEBUG SAMPLER: Top 10 probs: [0.8062371611595154, 0.014766748994588852, 0.010149028152227402, 0.006975317839533091, 0.006155696231871843, 0.004794061183929443, 0.004794061183929443, 0.004230744205415249, 0.0037336188834160566, 0.003294907044619322]
DEBUG SAMPLER: Top 10 token indices: [236743, 787, 496, 150180, 4305, 72745, 237534, 8026, 99163, 534]
DEBUG SAMPLER: Probability sum: 1.0
DEBUG SAMPLER: Special token probabilities: {0: 1.5814054303260952e-14, 1: 2.3668343729355e-08, 2: 2.8369969568009523e-11, 3: 1.3110310736065122e-14, 4: 8.634181152163134e-13}
DEBUG SAMPLER: Final top 5 probs: [0.8658615946769714, 0.015858808532357216, 0.010899588465690613, 0.007491169963032007, 0.006610934156924486]
DEBUG SAMPLER: Final top 5 indices: [236743, 787, 496, 150180, 4305]
DEBUG SAMPLER: Sampled token: 236743
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.203125, std=4.843750
DEBUG: Logits min/max: min=-23.250000, max=22.250000
DEBUG: Top 10 token IDs: [236743, 787, 496, 150180, 4305, 237534, 72745, 8026, 99163, 534]
DEBUG: Top 10 probabilities: [0.8046875, 0.0147705078125, 0.0101318359375, 0.006988525390625, 0.00616455078125, 0.004791259765625, 0.004791259765625, 0.004241943359375, 0.0037384033203125, 0.0032958984375]
DEBUG: Selected next token: 236743
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [236743]
DEBUG: Input positions: [16]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.003265, std: 1.000000
DEBUG: Token embeddings min: -11.312500, max: 18.750000
DEBUG: Hidden states before model: mean=0.003265, std=1.000000
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.201172, std=4.187500
DEBUG: Hidden states after model: min=-30.500000, max=15.375000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-4.187500, std=3.890625
DEBUG SAMPLER: Top 10 logits: [15.0, 13.0, 12.625, 12.5, 12.4375, 11.875, 11.75, 11.6875, 11.5, 11.4375]
DEBUG SAMPLER: Top 10 indices: [872, 162166, 30968, 1364, 236832, 4817, 174962, 73810, 236879, 4875]
DEBUG SAMPLER: Top 10 probs: [0.3342660367488861, 0.045237988233566284, 0.031091583892703056, 0.027438227087259293, 0.025775829330086708, 0.014686624519526958, 0.01296090055257082, 0.01217564009130001, 0.010093959979712963, 0.009482397697865963]
DEBUG SAMPLER: Top 10 token indices: [872, 162166, 30968, 1364, 236832, 4817, 174962, 73810, 236879, 4875]
DEBUG SAMPLER: Probability sum: 1.0000004768371582
DEBUG SAMPLER: Special token probabilities: {0: 2.6714406728411966e-11, 1: 1.0669007366459482e-07, 2: 6.131518603069708e-06, 3: 1.1136221285401593e-11, 4: 4.196444436388447e-08}
DEBUG SAMPLER: Final top 5 probs: [0.47669848799705505, 0.06451412290334702, 0.0443398654460907, 0.039129793643951416, 0.036759041249752045]
DEBUG SAMPLER: Final top 5 indices: [872, 162166, 30968, 1364, 236832]
DEBUG SAMPLER: Sampled token: 872
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-4.187500, std=3.890625
DEBUG: Logits min/max: min=-22.125000, max=15.000000
DEBUG: Top 10 token IDs: [872, 162166, 30968, 1364, 236832, 4817, 174962, 73810, 236879, 4875]
DEBUG: Top 10 probabilities: [0.333984375, 0.045166015625, 0.0311279296875, 0.0274658203125, 0.0257568359375, 0.01470947265625, 0.012939453125, 0.01214599609375, 0.01007080078125, 0.00946044921875]
DEBUG: Selected next token: 872
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [872]
DEBUG: Input positions: [17]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.002548, std: 1.000000
DEBUG: Token embeddings min: -3.984375, max: 3.062500
DEBUG: Hidden states before model: mean=-0.002548, std=1.000000
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.029175, std=4.812500
DEBUG: Hidden states after model: min=-32.250000, max=14.062500
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-0.316406, std=4.656250
DEBUG SAMPLER: Top 10 logits: [21.0, 20.0, 19.25, 18.875, 18.625, 18.625, 18.375, 18.375, 17.875, 17.875]
DEBUG SAMPLER: Top 10 indices: [88179, 36758, 130960, 93656, 76219, 14084, 99995, 114845, 81041, 140834]
DEBUG SAMPLER: Top 10 probs: [0.29090943932533264, 0.10701961070299149, 0.050552476197481155, 0.034744177013635635, 0.027058791369199753, 0.027058791369199753, 0.021073410287499428, 0.021073410287499428, 0.012781668454408646, 0.012781668454408646]
DEBUG SAMPLER: Top 10 token indices: [88179, 36758, 130960, 93656, 14084, 76219, 99995, 114845, 81041, 140834]
DEBUG SAMPLER: Probability sum: 1.0000004768371582
DEBUG SAMPLER: Special token probabilities: {0: 2.7493453477397956e-13, 1: 3.0450659771474875e-09, 2: 8.987658475234639e-07, 3: 1.6162569058727344e-13, 4: 4.080390539340506e-11}
DEBUG SAMPLER: Final top 5 probs: [0.3651845157146454, 0.13434389233589172, 0.06345954537391663, 0.04361506924033165, 0.03396745026111603]
DEBUG SAMPLER: Final top 5 indices: [88179, 36758, 130960, 93656, 14084]
DEBUG SAMPLER: Sampled token: 81041
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-0.316406, std=4.656250
DEBUG: Logits min/max: min=-24.875000, max=21.000000
DEBUG: Top 10 token IDs: [88179, 36758, 130960, 93656, 76219, 14084, 99995, 114845, 81041, 140834]
DEBUG: Top 10 probabilities: [0.291015625, 0.10693359375, 0.050537109375, 0.03466796875, 0.027099609375, 0.027099609375, 0.0211181640625, 0.0211181640625, 0.01275634765625, 0.01275634765625]
DEBUG: Selected next token: 81041
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [81041]
DEBUG: Input positions: [18]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.010925, std: 0.945312
DEBUG: Token embeddings min: -3.187500, max: 2.937500
DEBUG: Hidden states before model: mean=-0.010925, std=0.945312
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.129883, std=5.718750
DEBUG: Hidden states after model: min=-18.750000, max=20.875000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.609375, std=5.625000
DEBUG SAMPLER: Top 10 logits: [22.125, 22.0, 22.0, 21.375, 21.25, 21.25, 20.875, 20.75, 20.25, 20.25]
DEBUG SAMPLER: Top 10 indices: [10610, 243389, 241972, 102573, 249625, 130979, 132045, 190185, 14180, 161379]
DEBUG SAMPLER: Top 10 probs: [0.1395992785692215, 0.12319592386484146, 0.12319592386484146, 0.06594202667474747, 0.0581936351954937, 0.0581936351954937, 0.039995864033699036, 0.03529622405767441, 0.021408243104815483, 0.021408243104815483]
DEBUG SAMPLER: Top 10 token indices: [10610, 241972, 243389, 102573, 130979, 249625, 132045, 190185, 14180, 161379]
DEBUG SAMPLER: Probability sum: 1.0
DEBUG SAMPLER: Special token probabilities: {0: 1.1528209891689331e-14, 1: 3.260471059807202e-10, 2: 9.327354888455375e-08, 3: 2.1537532160823512e-14, 4: 4.805673189982795e-15}
DEBUG SAMPLER: Final top 5 probs: [0.15204603970050812, 0.13418015837669373, 0.13418015837669373, 0.07182145863771439, 0.06338221579790115]
DEBUG SAMPLER: Final top 5 indices: [10610, 241972, 243389, 102573, 130979]
DEBUG SAMPLER: Sampled token: 40222
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.609375, std=5.625000
DEBUG: Logits min/max: min=-26.375000, max=22.125000
DEBUG: Top 10 token IDs: [10610, 243389, 241972, 102573, 249625, 130979, 132045, 190185, 14180, 161379]
DEBUG: Top 10 probabilities: [0.1396484375, 0.123046875, 0.123046875, 0.06591796875, 0.05810546875, 0.05810546875, 0.0400390625, 0.035400390625, 0.0213623046875, 0.0213623046875]
DEBUG: Selected next token: 40222
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [40222]
DEBUG: Input positions: [19]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.021240, std: 0.976562
DEBUG: Token embeddings min: -3.312500, max: 2.828125
DEBUG: Hidden states before model: mean=-0.021240, std=0.976562
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.083496, std=4.968750
DEBUG: Hidden states after model: min=-21.375000, max=18.375000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-0.306641, std=4.781250
DEBUG SAMPLER: Top 10 logits: [19.875, 19.75, 19.625, 19.25, 19.25, 19.125, 19.125, 19.0, 18.75, 18.5]
DEBUG SAMPLER: Top 10 indices: [130093, 214302, 49318, 240902, 89581, 126851, 50451, 46985, 160961, 19729]
DEBUG SAMPLER: Top 10 probs: [0.07324139773845673, 0.06463530659675598, 0.05704045668244362, 0.03920329734683037, 0.03920329734683037, 0.03459678962826729, 0.03459678962826729, 0.03053155727684498, 0.023778000846505165, 0.018518324941396713]
DEBUG SAMPLER: Top 10 token indices: [130093, 214302, 49318, 89581, 240902, 50451, 126851, 46985, 160961, 19729]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 2.9851392469026905e-09, 1: 9.670973744047018e-11, 2: 7.171764050684093e-12, 3: 1.897469958933584e-09, 4: 1.5754276138854606e-12}
DEBUG SAMPLER: Final top 5 probs: [0.10729379951953888, 0.09468644857406616, 0.08356049656867981, 0.05743023753166199, 0.05743023753166199]
DEBUG SAMPLER: Final top 5 indices: [130093, 214302, 49318, 89581, 240902]
DEBUG SAMPLER: Sampled token: 46985
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-0.306641, std=4.781250
DEBUG: Logits min/max: min=-22.875000, max=19.875000
DEBUG: Top 10 token IDs: [130093, 214302, 49318, 240902, 89581, 126851, 50451, 46985, 160961, 19729]
DEBUG: Top 10 probabilities: [0.0732421875, 0.064453125, 0.05712890625, 0.039306640625, 0.039306640625, 0.03466796875, 0.03466796875, 0.030517578125, 0.0238037109375, 0.0185546875]
DEBUG: Selected next token: 46985
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [46985]
DEBUG: Input positions: [20]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.024902, std: 0.937500
DEBUG: Token embeddings min: -4.187500, max: 2.828125
DEBUG: Hidden states before model: mean=0.024902, std=0.937500
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.213867, std=5.375000
DEBUG: Hidden states after model: min=-19.750000, max=19.375000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.304688, std=5.281250
DEBUG SAMPLER: Top 10 logits: [24.875, 22.5, 21.375, 21.25, 21.25, 21.125, 20.875, 20.75, 20.5, 20.5]
DEBUG SAMPLER: Top 10 indices: [3048, 56727, 237779, 238158, 23606, 236821, 245808, 34814, 1599, 237437]
DEBUG SAMPLER: Top 10 probs: [0.6675708889961243, 0.062093764543533325, 0.02015889436006546, 0.017790162935853004, 0.017790162935853004, 0.015699762850999832, 0.012226988561451435, 0.010790279135107994, 0.008403478190302849, 0.008403478190302849]
DEBUG SAMPLER: Top 10 token indices: [3048, 56727, 237779, 23606, 238158, 236821, 245808, 34814, 1599, 237437]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 1.0626295308090172e-13, 1: 4.262544206490304e-10, 2: 2.1132910959370532e-13, 3: 3.672349289897056e-14, 4: 4.78244249757509e-11}
DEBUG SAMPLER: Final top 5 probs: [0.7025454044342041, 0.06534690409898758, 0.021215032786130905, 0.018722202628850937, 0.018722202628850937]
DEBUG SAMPLER: Final top 5 indices: [3048, 56727, 237779, 23606, 238158]
DEBUG SAMPLER: Sampled token: 3048
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.304688, std=5.281250
DEBUG: Logits min/max: min=-23.250000, max=24.875000
DEBUG: Top 10 token IDs: [3048, 56727, 237779, 238158, 23606, 236821, 245808, 34814, 1599, 237437]
DEBUG: Top 10 probabilities: [0.66796875, 0.06201171875, 0.0201416015625, 0.017822265625, 0.017822265625, 0.0157470703125, 0.01220703125, 0.01080322265625, 0.0084228515625, 0.0084228515625]
DEBUG: Selected next token: 3048
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [3048]
DEBUG: Input positions: [21]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.026123, std: 1.054688
DEBUG: Token embeddings min: -12.625000, max: 3.062500
DEBUG: Hidden states before model: mean=0.026123, std=1.054688
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.003525, std=4.718750
DEBUG: Hidden states after model: min=-34.000000, max=25.750000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-4.250000, std=4.656250
DEBUG SAMPLER: Top 10 logits: [17.875, 17.25, 16.5, 16.0, 15.5625, 15.375, 15.25, 15.0625, 14.875, 14.5]
DEBUG SAMPLER: Top 10 indices: [1730, 236743, 205861, 1555, 236810, 45939, 696, 37180, 89298, 614]
DEBUG SAMPLER: Top 10 probs: [0.3119111955165863, 0.16695404052734375, 0.07886349409818649, 0.04783312976360321, 0.030883390456438065, 0.02560322917997837, 0.022594770416617393, 0.018731722608208656, 0.015529142692685127, 0.010673013515770435]
DEBUG SAMPLER: Top 10 token indices: [1730, 236743, 205861, 1555, 236810, 45939, 696, 37180, 89298, 614]
DEBUG SAMPLER: Probability sum: 1.0000004768371582
DEBUG SAMPLER: Special token probabilities: {0: 1.6630521219580634e-14, 1: 4.040098033897266e-08, 2: 6.530209706312462e-10, 3: 6.5126055149601885e-15, 4: 1.0498137470449365e-08}
DEBUG SAMPLER: Final top 5 probs: [0.35230952501296997, 0.18857769668102264, 0.08907778561115265, 0.05402841046452522, 0.03488336503505707]
DEBUG SAMPLER: Final top 5 indices: [1730, 236743, 205861, 1555, 236810]
DEBUG SAMPLER: Sampled token: 236743
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-4.250000, std=4.656250
DEBUG: Logits min/max: min=-23.375000, max=17.875000
DEBUG: Top 10 token IDs: [1730, 236743, 205861, 1555, 236810, 45939, 696, 37180, 89298, 614]
DEBUG: Top 10 probabilities: [0.3125, 0.1669921875, 0.0791015625, 0.0478515625, 0.0308837890625, 0.025634765625, 0.0225830078125, 0.0186767578125, 0.0155029296875, 0.01068115234375]
DEBUG: Selected next token: 236743
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [236743]
DEBUG: Input positions: [22]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.003265, std: 1.000000
DEBUG: Token embeddings min: -11.312500, max: 18.750000
DEBUG: Hidden states before model: mean=0.003265, std=1.000000
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.103027, std=5.875000
DEBUG: Hidden states after model: min=-17.875000, max=23.375000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.085938, std=5.812500
DEBUG SAMPLER: Top 10 logits: [27.0, 24.875, 24.875, 24.625, 22.375, 22.125, 21.625, 21.625, 21.5, 21.5]
DEBUG SAMPLER: Top 10 indices: [14952, 97566, 86413, 97666, 201438, 121927, 85524, 2292, 2003, 107590]
DEBUG SAMPLER: Top 10 probs: [0.6918389201164246, 0.08262837678194046, 0.08262837678194046, 0.06435104459524155, 0.00678255083039403, 0.005282255355268717, 0.0032038502395153046, 0.0032038502395153046, 0.0028273877687752247, 0.0028273877687752247]
DEBUG SAMPLER: Top 10 token indices: [14952, 86413, 97566, 97666, 201438, 121927, 2292, 85524, 2003, 107590]
DEBUG SAMPLER: Probability sum: 0.9999999403953552
DEBUG SAMPLER: Special token probabilities: {0: 6.346854931467915e-16, 1: 2.199343658582098e-11, 2: 5.616220336013278e-11, 3: 1.9356815133172978e-16, 4: 1.6792193720596704e-13}
DEBUG SAMPLER: Final top 5 probs: [0.727328360080719, 0.08686698973178864, 0.08686698973178864, 0.06765207648277283, 0.007130477111786604]
DEBUG SAMPLER: Final top 5 indices: [14952, 86413, 97566, 97666, 201438]
DEBUG SAMPLER: Sampled token: 97566
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.085938, std=5.812500
DEBUG: Logits min/max: min=-26.625000, max=27.000000
DEBUG: Top 10 token IDs: [14952, 97566, 86413, 97666, 201438, 121927, 85524, 2292, 2003, 107590]
DEBUG: Top 10 probabilities: [0.69140625, 0.08251953125, 0.08251953125, 0.064453125, 0.00677490234375, 0.005279541015625, 0.003204345703125, 0.003204345703125, 0.0028228759765625, 0.0028228759765625]
DEBUG: Selected next token: 97566
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [97566]
DEBUG: Input positions: [23]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.053467, std: 0.996094
DEBUG: Token embeddings min: -3.078125, max: 3.250000
DEBUG: Hidden states before model: mean=-0.053467, std=0.996094
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.175781, std=4.218750
DEBUG: Hidden states after model: min=-14.625000, max=13.187500
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-0.570312, std=4.125000
DEBUG SAMPLER: Top 10 logits: [17.875, 17.75, 17.625, 17.125, 16.75, 16.75, 16.625, 16.375, 16.25, 16.125]
DEBUG SAMPLER: Top 10 indices: [12253, 216231, 115847, 237346, 21981, 64826, 87137, 2096, 28385, 32019]
DEBUG SAMPLER: Top 10 probs: [0.08265788108110428, 0.07294532656669617, 0.06437402218580246, 0.0390448197722435, 0.026835085824131966, 0.026835085824131966, 0.02368188090622425, 0.01844346709549427, 0.016276301816105843, 0.014363785274326801]
DEBUG SAMPLER: Top 10 token indices: [12253, 216231, 115847, 237346, 21981, 64826, 87137, 2096, 28385, 32019]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 4.4190773262897665e-10, 1: 1.2323041254092004e-08, 2: 5.239890583652596e-07, 3: 1.6772924715802162e-10, 4: 7.833001802737272e-09}
DEBUG SAMPLER: Final top 5 probs: [0.1293420046567917, 0.11414392292499542, 0.10073165595531464, 0.06109683960676193, 0.04199120029807091]
DEBUG SAMPLER: Final top 5 indices: [12253, 216231, 115847, 237346, 21981]
DEBUG SAMPLER: Sampled token: 44125
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-0.570312, std=4.125000
DEBUG: Logits min/max: min=-18.000000, max=17.875000
DEBUG: Top 10 token IDs: [12253, 216231, 115847, 237346, 21981, 64826, 87137, 2096, 28385, 32019]
DEBUG: Top 10 probabilities: [0.08251953125, 0.07275390625, 0.064453125, 0.0390625, 0.02685546875, 0.02685546875, 0.023681640625, 0.0184326171875, 0.0162353515625, 0.01434326171875]
DEBUG: Selected next token: 44125
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [44125]
DEBUG: Input positions: [24]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.022461, std: 0.968750
DEBUG: Token embeddings min: -3.000000, max: 3.500000
DEBUG: Hidden states before model: mean=-0.022461, std=0.968750
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.025635, std=5.093750
DEBUG: Hidden states after model: min=-18.375000, max=19.625000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-4.468750, std=5.343750
DEBUG SAMPLER: Top 10 logits: [17.25, 16.875, 16.75, 16.625, 16.5, 16.5, 16.375, 16.0, 16.0, 15.8125]
DEBUG SAMPLER: Top 10 indices: [156710, 69966, 568, 37251, 20424, 236769, 29241, 8366, 34509, 66032]
DEBUG SAMPLER: Top 10 probs: [0.08301252126693726, 0.057053618133068085, 0.0503496415913105, 0.04443340376019478, 0.039212342351675034, 0.039212342351675034, 0.034604765474796295, 0.02378348633646965, 0.02378348633646965, 0.019717203453183174]
DEBUG SAMPLER: Top 10 token indices: [156710, 69966, 568, 37251, 20424, 236769, 29241, 8366, 34509, 66032]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 2.225569116273544e-15, 1: 1.947004335534075e-08, 2: 1.5369723271874136e-09, 3: 1.5296097891179684e-15, 4: 5.445788134675333e-13}
DEBUG SAMPLER: Final top 5 probs: [0.11350280046463013, 0.07800926268100739, 0.06884293258190155, 0.060753677040338516, 0.05361493304371834]
DEBUG SAMPLER: Final top 5 indices: [156710, 69966, 568, 37251, 20424]
DEBUG SAMPLER: Sampled token: 236769
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-4.468750, std=5.343750
DEBUG: Logits min/max: min=-26.125000, max=17.250000
DEBUG: Top 10 token IDs: [156710, 69966, 568, 37251, 20424, 236769, 29241, 8366, 34509, 66032]
DEBUG: Top 10 probabilities: [0.0830078125, 0.05712890625, 0.05029296875, 0.04443359375, 0.039306640625, 0.039306640625, 0.03466796875, 0.0238037109375, 0.0238037109375, 0.019775390625]
DEBUG: Selected next token: 236769
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [236769]
DEBUG: Input positions: [25]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.000813, std: 1.015625
DEBUG: Token embeddings min: -5.531250, max: 8.625000
DEBUG: Hidden states before model: mean=-0.000813, std=1.015625
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.081543, std=5.312500
DEBUG: Hidden states after model: min=-40.000000, max=19.875000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.625000, std=5.250000
DEBUG SAMPLER: Top 10 logits: [20.25, 20.125, 20.0, 20.0, 19.875, 19.875, 19.625, 19.5, 19.5, 18.875]
DEBUG SAMPLER: Top 10 indices: [117288, 1553, 10423, 635, 24009, 96179, 100272, 214780, 3269, 103960]
DEBUG SAMPLER: Top 10 probs: [0.08114564418792725, 0.07161077857017517, 0.06319628655910492, 0.06319628655910492, 0.05577053129673004, 0.05577053129673004, 0.04343413561582565, 0.03833048790693283, 0.03833048790693283, 0.020516831427812576]
DEBUG SAMPLER: Top 10 token indices: [117288, 1553, 635, 10423, 24009, 96179, 100272, 3269, 214780, 103960]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 1.8215389320595105e-14, 1: 4.8193372036153814e-12, 2: 8.058731459925639e-09, 3: 9.159272190521761e-15, 4: 4.985760710951581e-07}
DEBUG SAMPLER: Final top 5 probs: [0.10385122150182724, 0.09164837747812271, 0.08087940514087677, 0.08087940514087677, 0.07137583196163177]
DEBUG SAMPLER: Final top 5 indices: [117288, 1553, 635, 10423, 24009]
DEBUG SAMPLER: Sampled token: 229797
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.625000, std=5.250000
DEBUG: Logits min/max: min=-24.500000, max=20.250000
DEBUG: Top 10 token IDs: [117288, 1553, 10423, 635, 24009, 96179, 100272, 214780, 3269, 103960]
DEBUG: Top 10 probabilities: [0.0810546875, 0.07177734375, 0.06298828125, 0.06298828125, 0.0556640625, 0.0556640625, 0.04345703125, 0.038330078125, 0.038330078125, 0.0205078125]
DEBUG: Selected next token: 229797
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [229797]
DEBUG: Input positions: [26]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.008911, std: 0.941406
DEBUG: Token embeddings min: -3.593750, max: 3.234375
DEBUG: Hidden states before model: mean=-0.008911, std=0.941406
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.113281, std=4.531250
DEBUG: Hidden states after model: min=-14.187500, max=12.875000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=0.019531, std=4.375000
DEBUG SAMPLER: Top 10 logits: [20.375, 20.25, 20.25, 19.625, 18.875, 18.75, 18.5, 18.25, 18.25, 18.25]
DEBUG SAMPLER: Top 10 indices: [27075, 243416, 131654, 236843, 241155, 156622, 103850, 238438, 11095, 5667]
DEBUG SAMPLER: Top 10 probs: [0.14875663816928864, 0.13127726316452026, 0.13127726316452026, 0.07026766240596771, 0.033192094415426254, 0.029291916638612747, 0.02281256951391697, 0.017766447737812996, 0.017766447737812996, 0.017766447737812996]
DEBUG SAMPLER: Top 10 token indices: [27075, 131654, 243416, 236843, 241155, 156622, 103850, 5667, 11095, 238438]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 4.173280644670374e-12, 1: 9.99612215224488e-09, 2: 2.152742047123457e-11, 3: 2.9134197224861547e-12, 4: 1.198896910636904e-07}
DEBUG SAMPLER: Final top 5 probs: [0.18214936554431915, 0.160746231675148, 0.160746231675148, 0.08604127168655396, 0.04064301773905754]
DEBUG SAMPLER: Final top 5 indices: [27075, 131654, 243416, 236843, 241155]
DEBUG SAMPLER: Sampled token: 243416
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=0.019531, std=4.375000
DEBUG: Logits min/max: min=-20.250000, max=20.375000
DEBUG: Top 10 token IDs: [27075, 243416, 131654, 236843, 241155, 156622, 103850, 238438, 11095, 5667]
DEBUG: Top 10 probabilities: [0.1484375, 0.130859375, 0.130859375, 0.0703125, 0.033203125, 0.029296875, 0.0228271484375, 0.017822265625, 0.017822265625, 0.017822265625]
DEBUG: Selected next token: 243416
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [243416]
DEBUG: Input positions: [27]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.013855, std: 0.957031
DEBUG: Token embeddings min: -3.640625, max: 3.921875
DEBUG: Hidden states before model: mean=0.013855, std=0.957031
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.172852, std=4.812500
DEBUG: Hidden states after model: min=-20.375000, max=23.000000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=0.233398, std=4.750000
DEBUG SAMPLER: Top 10 logits: [21.125, 21.0, 20.875, 20.25, 20.125, 19.75, 19.75, 19.75, 19.5, 19.5]
DEBUG SAMPLER: Top 10 indices: [109984, 191289, 7596, 92421, 27238, 34509, 153544, 61602, 53860, 87413]
DEBUG SAMPLER: Top 10 probs: [0.1042618453502655, 0.09201075881719589, 0.08119920641183853, 0.04346280172467232, 0.03835579380393028, 0.02636152319610119, 0.02636152319610119, 0.02636152319610119, 0.020530374720692635, 0.020530374720692635]
DEBUG SAMPLER: Top 10 token indices: [109984, 191289, 7596, 92421, 27238, 34509, 61602, 153544, 53860, 87413]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 8.33256467297483e-12, 1: 1.0683132600775025e-08, 2: 1.4738805662051213e-11, 3: 3.2124901697128116e-12, 4: 8.633294634741162e-10}
DEBUG SAMPLER: Final top 5 probs: [0.13574224710464478, 0.11979211866855621, 0.10571616888046265, 0.05658578500151634, 0.04993679001927376]
DEBUG SAMPLER: Final top 5 indices: [109984, 191289, 7596, 92421, 27238]
DEBUG SAMPLER: Sampled token: 6567
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=0.233398, std=4.750000
DEBUG: Logits min/max: min=-19.500000, max=21.125000
DEBUG: Top 10 token IDs: [109984, 191289, 7596, 92421, 27238, 34509, 153544, 61602, 53860, 87413]
DEBUG: Top 10 probabilities: [0.1044921875, 0.091796875, 0.0810546875, 0.04345703125, 0.038330078125, 0.0263671875, 0.0263671875, 0.0263671875, 0.0205078125, 0.0205078125]
DEBUG: Selected next token: 6567
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [6567]
DEBUG: Input positions: [28]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.010071, std: 0.960938
DEBUG: Token embeddings min: -2.968750, max: 3.156250
DEBUG: Hidden states before model: mean=0.010071, std=0.960938
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.048584, std=4.437500
DEBUG: Hidden states after model: min=-16.875000, max=17.625000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.656250, std=4.187500
DEBUG SAMPLER: Top 10 logits: [18.75, 17.0, 16.5, 15.75, 15.375, 15.0625, 15.0, 14.625, 14.5625, 14.5625]
DEBUG SAMPLER: Top 10 indices: [11348, 7452, 232359, 193977, 233498, 254582, 121161, 7435, 2576, 236777]
DEBUG SAMPLER: Top 10 probs: [0.4106409549713135, 0.07135869562625885, 0.04328123852610588, 0.020444609224796295, 0.014051361009478569, 0.010280195623636246, 0.009657350368797779, 0.006637393031269312, 0.006235253531485796, 0.006235253531485796]
DEBUG SAMPLER: Top 10 token indices: [11348, 7452, 232359, 193977, 233498, 254582, 121161, 7435, 2576, 236777]
DEBUG SAMPLER: Probability sum: 1.0000005960464478
DEBUG SAMPLER: Special token probabilities: {0: 3.583858210198798e-10, 1: 5.032843065910786e-12, 2: 1.0230948532807815e-08, 3: 2.0103621545253247e-10, 4: 1.4681617033218686e-09}
DEBUG SAMPLER: Final top 5 probs: [0.54584801197052, 0.09485416114330292, 0.05753195658326149, 0.02717617154121399, 0.01867789216339588]
DEBUG SAMPLER: Final top 5 indices: [11348, 7452, 232359, 193977, 233498]
DEBUG SAMPLER: Sampled token: 123105
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.656250, std=4.187500
DEBUG: Logits min/max: min=-20.125000, max=18.750000
DEBUG: Top 10 token IDs: [11348, 7452, 232359, 193977, 233498, 254582, 121161, 7435, 2576, 236777]
DEBUG: Top 10 probabilities: [0.41015625, 0.0712890625, 0.043212890625, 0.0203857421875, 0.0140380859375, 0.01025390625, 0.0096435546875, 0.006622314453125, 0.0062255859375, 0.0062255859375]
DEBUG: Selected next token: 123105
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [123105]
DEBUG: Input positions: [29]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.015015, std: 0.984375
DEBUG: Token embeddings min: -3.593750, max: 4.812500
DEBUG: Hidden states before model: mean=0.015015, std=0.984375
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.183594, std=5.468750
DEBUG: Hidden states after model: min=-29.500000, max=22.875000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.890625, std=5.406250
DEBUG SAMPLER: Top 10 logits: [20.75, 20.5, 20.25, 19.875, 19.25, 19.125, 19.0, 18.875, 18.75, 18.75]
DEBUG SAMPLER: Top 10 indices: [99493, 75402, 190229, 108852, 66501, 185201, 10210, 12910, 50361, 68343]
DEBUG SAMPLER: Top 10 probs: [0.14901356399059296, 0.11605188250541687, 0.09038130193948746, 0.06211809441447258, 0.033249422907829285, 0.02934250980615616, 0.02589467354118824, 0.0228519719094038, 0.02016679383814335, 0.02016679383814335]
DEBUG SAMPLER: Top 10 token indices: [99493, 75402, 190229, 108852, 66501, 185201, 10210, 12910, 50361, 68343]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 6.311594584647306e-13, 1: 9.81677933387104e-11, 2: 3.2274987461278215e-05, 3: 9.092714374405592e-14, 4: 2.503810366929623e-18}
DEBUG SAMPLER: Final top 5 probs: [0.1766081005334854, 0.13754253089427948, 0.10711824148893356, 0.07362121343612671, 0.03940659761428833]
DEBUG SAMPLER: Final top 5 indices: [99493, 75402, 190229, 108852, 66501]
DEBUG SAMPLER: Sampled token: 104486
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.890625, std=5.406250
DEBUG: Logits min/max: min=-27.125000, max=20.750000
DEBUG: Top 10 token IDs: [99493, 75402, 190229, 108852, 66501, 185201, 10210, 12910, 50361, 68343]
DEBUG: Top 10 probabilities: [0.1494140625, 0.1162109375, 0.09033203125, 0.06201171875, 0.033203125, 0.029296875, 0.02587890625, 0.0228271484375, 0.0201416015625, 0.0201416015625]
DEBUG: Selected next token: 104486
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [104486]
DEBUG: Input positions: [30]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.007996, std: 0.953125
DEBUG: Token embeddings min: -2.984375, max: 3.156250
DEBUG: Hidden states before model: mean=-0.007996, std=0.953125
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.044922, std=4.781250
DEBUG: Hidden states after model: min=-27.250000, max=13.312500
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.515625, std=4.625000
DEBUG SAMPLER: Top 10 logits: [20.0, 18.75, 18.25, 17.875, 17.625, 17.375, 17.25, 17.25, 16.875, 16.625]
DEBUG SAMPLER: Top 10 indices: [8015, 125190, 226412, 105273, 235150, 181014, 207545, 151129, 146978, 242698]
DEBUG SAMPLER: Top 10 probs: [0.38578376173973083, 0.11052890121936798, 0.06703916192054749, 0.046075303107500076, 0.035883478820323944, 0.02794608287513256, 0.024662330746650696, 0.024662330746650696, 0.016950156539678574, 0.013200794346630573]
DEBUG SAMPLER: Top 10 token indices: [8015, 125190, 226412, 105273, 235150, 181014, 151129, 207545, 146978, 242698]
DEBUG SAMPLER: Probability sum: 1.0000004768371582
DEBUG SAMPLER: Special token probabilities: {0: 4.2625945091342987e-13, 1: 2.84119616722478e-09, 2: 5.751452647473343e-08, 3: 2.354028159467364e-13, 4: 1.0180183362962225e-09}
DEBUG SAMPLER: Final top 5 probs: [0.4317825734615326, 0.12370777875185013, 0.0750325545668602, 0.051569078117609024, 0.04016203433275223]
DEBUG SAMPLER: Final top 5 indices: [8015, 125190, 226412, 105273, 235150]
DEBUG SAMPLER: Sampled token: 8015
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.515625, std=4.625000
DEBUG: Logits min/max: min=-25.500000, max=20.000000
DEBUG: Top 10 token IDs: [8015, 125190, 226412, 105273, 235150, 181014, 207545, 151129, 146978, 242698]
DEBUG: Top 10 probabilities: [0.38671875, 0.1103515625, 0.06689453125, 0.046142578125, 0.035888671875, 0.0279541015625, 0.024658203125, 0.024658203125, 0.0169677734375, 0.01318359375]
DEBUG: Selected next token: 8015
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [8015]
DEBUG: Input positions: [31]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.012512, std: 1.031250
DEBUG: Token embeddings min: -5.937500, max: 3.734375
DEBUG: Hidden states before model: mean=0.012512, std=1.031250
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.238281, std=4.812500
DEBUG: Hidden states after model: min=-21.375000, max=26.250000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.265625, std=4.625000
DEBUG SAMPLER: Top 10 logits: [18.375, 16.5, 15.875, 15.4375, 15.125, 15.0625, 15.0625, 15.0, 14.9375, 14.875]
DEBUG SAMPLER: Top 10 indices: [71945, 187046, 38997, 191666, 201571, 133429, 5918, 73813, 172860, 13102]
DEBUG SAMPLER: Top 10 probs: [0.3567805290222168, 0.05471406877040863, 0.029286328703165054, 0.018908675760030746, 0.013833882287144661, 0.01299573015421629, 0.01299573015421629, 0.012208358384668827, 0.011468691751360893, 0.010773838497698307]
DEBUG SAMPLER: Top 10 token indices: [71945, 187046, 38997, 191666, 201571, 5918, 133429, 73813, 172860, 13102]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 1.0822601495541484e-11, 1: 5.850643580718184e-11, 2: 2.802946852398236e-08, 3: 8.972251760996919e-12, 4: 1.97924704403718e-10}
DEBUG SAMPLER: Final top 5 probs: [0.4824865162372589, 0.07399170845746994, 0.039604902267456055, 0.02557084895670414, 0.018708031624555588]
DEBUG SAMPLER: Final top 5 indices: [71945, 187046, 38997, 191666, 201571]
DEBUG SAMPLER: Sampled token: 119715
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.265625, std=4.625000
DEBUG: Logits min/max: min=-24.000000, max=18.375000
DEBUG: Top 10 token IDs: [71945, 187046, 38997, 191666, 201571, 133429, 5918, 73813, 172860, 13102]
DEBUG: Top 10 probabilities: [0.357421875, 0.0546875, 0.029296875, 0.0189208984375, 0.01385498046875, 0.01300048828125, 0.01300048828125, 0.01220703125, 0.011474609375, 0.01080322265625]
DEBUG: Selected next token: 119715
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [119715]
DEBUG: Input positions: [32]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.026855, std: 0.914062
DEBUG: Token embeddings min: -2.968750, max: 3.234375
DEBUG: Hidden states before model: mean=0.026855, std=0.914062
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.312500, std=5.468750
DEBUG: Hidden states after model: min=-27.375000, max=17.750000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.890625, std=5.781250
DEBUG SAMPLER: Top 10 logits: [24.5, 24.125, 21.625, 21.5, 20.75, 20.375, 20.25, 20.0, 19.625, 19.625]
DEBUG SAMPLER: Top 10 indices: [16465, 236769, 139043, 244470, 212850, 148965, 562, 217984, 237221, 238931]
DEBUG SAMPLER: Top 10 probs: [0.505692183971405, 0.34755685925483704, 0.028529204428195953, 0.02517693117260933, 0.011892741546034813, 0.008173753507435322, 0.007213311735540628, 0.005617733579128981, 0.0038610075134783983, 0.0038610075134783983]
DEBUG SAMPLER: Top 10 token indices: [16465, 236769, 139043, 244470, 212850, 148965, 562, 217984, 237221, 238931]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 8.95569527454651e-19, 1: 1.3392593457839386e-11, 2: 3.5462788261497735e-10, 3: 8.95569527454651e-19, 4: 7.287453518238507e-13}
DEBUG SAMPLER: Final top 5 probs: [0.532179594039917, 0.365761399269104, 0.030023522675037384, 0.026495663449168205, 0.012515665963292122]
DEBUG SAMPLER: Final top 5 indices: [16465, 236769, 139043, 244470, 212850]
DEBUG SAMPLER: Sampled token: 16465
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.890625, std=5.781250
DEBUG: Logits min/max: min=-27.500000, max=24.500000
DEBUG: Top 10 token IDs: [16465, 236769, 139043, 244470, 212850, 148965, 562, 217984, 237221, 238931]
DEBUG: Top 10 probabilities: [0.50390625, 0.34765625, 0.028564453125, 0.025146484375, 0.01190185546875, 0.0081787109375, 0.0072021484375, 0.005615234375, 0.0038604736328125, 0.0038604736328125]
DEBUG: Selected next token: 16465
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [16465]
DEBUG: Input positions: [33]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.005951, std: 0.941406
DEBUG: Token embeddings min: -3.734375, max: 3.109375
DEBUG: Hidden states before model: mean=0.005951, std=0.941406
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.171875, std=4.625000
DEBUG: Hidden states after model: min=-20.375000, max=15.000000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.890625, std=4.531250
DEBUG SAMPLER: Top 10 logits: [18.25, 17.875, 17.5, 17.375, 17.125, 17.0, 17.0, 16.875, 16.75, 16.5]
DEBUG SAMPLER: Top 10 indices: [53059, 26594, 46497, 6528, 88548, 12663, 146325, 5639, 170367, 23854]
DEBUG SAMPLER: Top 10 probs: [0.1065424382686615, 0.0732254758477211, 0.05032708868384361, 0.0444134958088398, 0.034589264541864395, 0.03052492067217827, 0.03052492067217827, 0.026938147842884064, 0.02377283200621605, 0.018514297902584076]
DEBUG SAMPLER: Top 10 token indices: [53059, 26594, 46497, 6528, 88548, 12663, 146325, 5639, 170367, 23854]
DEBUG SAMPLER: Probability sum: 1.0000004768371582
DEBUG SAMPLER: Special token probabilities: {0: 9.444892090648693e-11, 1: 1.5154230093017418e-09, 2: 9.050355131989818e-09, 3: 7.355690362365053e-11, 4: 3.264067835462292e-11}
DEBUG SAMPLER: Final top 5 probs: [0.14600495994091034, 0.10034763813018799, 0.06896786391735077, 0.0608639195561409, 0.04740086570382118]
DEBUG SAMPLER: Final top 5 indices: [53059, 26594, 46497, 6528, 88548]
DEBUG SAMPLER: Sampled token: 6528
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.890625, std=4.531250
DEBUG: Logits min/max: min=-21.625000, max=18.250000
DEBUG: Top 10 token IDs: [53059, 26594, 46497, 6528, 88548, 12663, 146325, 5639, 170367, 23854]
DEBUG: Top 10 probabilities: [0.1064453125, 0.0732421875, 0.05029296875, 0.04443359375, 0.03466796875, 0.030517578125, 0.030517578125, 0.0269775390625, 0.0238037109375, 0.0185546875]
DEBUG: Selected next token: 6528
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [6528]
DEBUG: Input positions: [34]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.066406, std: 0.957031
DEBUG: Token embeddings min: -4.031250, max: 3.000000
DEBUG: Hidden states before model: mean=-0.066406, std=0.957031
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.021851, std=3.796875
DEBUG: Hidden states after model: min=-31.500000, max=19.875000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.843750, std=3.718750
DEBUG SAMPLER: Top 10 logits: [13.0625, 12.625, 12.5, 12.4375, 12.125, 12.0, 12.0, 11.9375, 11.9375, 11.6875]
DEBUG SAMPLER: Top 10 indices: [238855, 2368, 63506, 63356, 1390, 242619, 2032, 12165, 236836, 5626]
DEBUG SAMPLER: Top 10 probs: [0.0762500986456871, 0.04923076555132866, 0.043445996940135956, 0.040813740342855453, 0.029859967529773712, 0.026351330801844597, 0.026351330801844597, 0.024754783138632774, 0.024754783138632774, 0.01927904412150383]
DEBUG SAMPLER: Top 10 token indices: [238855, 2368, 63506, 63356, 1390, 2032, 242619, 12165, 236836, 5626]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 2.26415015208703e-11, 1: 1.9641689164018317e-08, 2: 1.1999557163883123e-12, 3: 1.2119123413145694e-11, 4: 1.248688192845293e-07}
DEBUG SAMPLER: Final top 5 probs: [0.13071757555007935, 0.08439761400222778, 0.07448063045740128, 0.069968082010746, 0.051189735531806946]
DEBUG SAMPLER: Final top 5 indices: [238855, 2368, 63506, 63356, 1390]
DEBUG SAMPLER: Sampled token: 4318
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.843750, std=3.718750
DEBUG: Logits min/max: min=-20.500000, max=13.062500
DEBUG: Top 10 token IDs: [238855, 2368, 63506, 63356, 1390, 242619, 2032, 12165, 236836, 5626]
DEBUG: Top 10 probabilities: [0.076171875, 0.04931640625, 0.04345703125, 0.040771484375, 0.0299072265625, 0.0263671875, 0.0263671875, 0.0247802734375, 0.0247802734375, 0.019287109375]
DEBUG: Selected next token: 4318
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [4318]
DEBUG: Input positions: [35]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.013062, std: 0.980469
DEBUG: Token embeddings min: -3.140625, max: 3.281250
DEBUG: Hidden states before model: mean=-0.013062, std=0.980469
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.316406, std=5.187500
DEBUG: Hidden states after model: min=-40.500000, max=29.625000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.921875, std=5.187500
DEBUG SAMPLER: Top 10 logits: [19.75, 18.875, 18.375, 18.0, 17.125, 16.875, 16.875, 16.75, 16.75, 16.375]
DEBUG SAMPLER: Top 10 indices: [502, 197958, 236761, 48987, 20211, 243685, 52718, 44057, 239025, 206]
DEBUG SAMPLER: Top 10 probs: [0.34687647223472595, 0.14459961652755737, 0.08770410716533661, 0.060278087854385376, 0.025127647444605827, 0.019569432362914085, 0.019569432362914085, 0.017269961535930634, 0.017269961535930634, 0.011869459412992]
DEBUG SAMPLER: Top 10 token indices: [502, 197958, 236761, 48987, 20211, 52718, 243685, 44057, 239025, 206]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 1.6160578658356239e-15, 1: 7.942551130679654e-12, 2: 1.3620766736721635e-09, 3: 7.633717677517891e-16, 4: 4.287243982048494e-08}
DEBUG SAMPLER: Final top 5 probs: [0.38949063420295715, 0.16236384212970734, 0.09847865253686905, 0.06768331676721573, 0.028214607387781143]
DEBUG SAMPLER: Final top 5 indices: [502, 197958, 236761, 48987, 20211]
DEBUG SAMPLER: Sampled token: 236761
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.921875, std=5.187500
DEBUG: Logits min/max: min=-27.125000, max=19.750000
DEBUG: Top 10 token IDs: [502, 197958, 236761, 48987, 20211, 243685, 52718, 44057, 239025, 206]
DEBUG: Top 10 probabilities: [0.34765625, 0.14453125, 0.087890625, 0.060302734375, 0.025146484375, 0.01953125, 0.01953125, 0.0172119140625, 0.0172119140625, 0.0118408203125]
DEBUG: Selected next token: 236761
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [236761]
DEBUG: Input positions: [36]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.021240, std: 1.007812
DEBUG: Token embeddings min: -19.125000, max: 15.312500
DEBUG: Hidden states before model: mean=-0.021240, std=1.007812
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.158203, std=5.781250
DEBUG: Hidden states after model: min=-28.625000, max=27.125000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.906250, std=6.218750
DEBUG SAMPLER: Top 10 logits: [25.5, 25.375, 22.875, 22.25, 22.125, 21.375, 21.25, 20.375, 20.375, 20.25]
DEBUG SAMPLER: Top 10 indices: [611, 1599, 3048, 236778, 48410, 2542, 1701, 237408, 157783, 25211]
DEBUG SAMPLER: Top 10 probs: [0.46644851565361023, 0.41163936257362366, 0.03378941863775253, 0.01808617077767849, 0.015960989519953728, 0.007539437618106604, 0.006653530523180962, 0.0027736041229218245, 0.0027736041229218245, 0.002447697101160884]
DEBUG SAMPLER: Top 10 token indices: [611, 1599, 3048, 236778, 48410, 2542, 1701, 157783, 237408, 25211]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 2.4945148407855275e-20, 1: 2.4870162963004823e-14, 2: 7.970468174356427e-10, 3: 9.176806339148645e-21, 4: 2.462479572939362e-15}
DEBUG SAMPLER: Final top 5 probs: [0.4892146587371826, 0.4317304193973541, 0.03543859347701073, 0.018968909978866577, 0.01674000360071659]
DEBUG SAMPLER: Final top 5 indices: [611, 1599, 3048, 236778, 48410]
DEBUG SAMPLER: Sampled token: 611
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.906250, std=6.218750
DEBUG: Logits min/max: min=-29.250000, max=25.500000
DEBUG: Top 10 token IDs: [611, 1599, 3048, 236778, 48410, 2542, 1701, 237408, 157783, 25211]
DEBUG: Top 10 probabilities: [0.466796875, 0.412109375, 0.03369140625, 0.01806640625, 0.0159912109375, 0.007537841796875, 0.00665283203125, 0.002777099609375, 0.002777099609375, 0.00244140625]
DEBUG: Selected next token: 611
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [611]
DEBUG: Input positions: [37]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.007599, std: 1.015625
DEBUG: Token embeddings min: -15.312500, max: 3.015625
DEBUG: Hidden states before model: mean=0.007599, std=1.015625
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.016235, std=4.343750
DEBUG: Hidden states after model: min=-14.812500, max=15.562500
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=1.250000, std=4.218750
DEBUG SAMPLER: Top 10 logits: [20.375, 20.125, 20.125, 20.0, 19.625, 19.125, 18.875, 18.625, 18.625, 17.875]
DEBUG SAMPLER: Top 10 indices: [203623, 213130, 4546, 134876, 150994, 247113, 220001, 91610, 223718, 15636]
DEBUG SAMPLER: Top 10 probs: [0.12069536745548248, 0.09399764984846115, 0.09399764984846115, 0.08295263350009918, 0.05701245740056038, 0.0345798023045063, 0.026930777356028557, 0.020973708480596542, 0.020973708480596542, 0.009907279163599014]
DEBUG SAMPLER: Top 10 token indices: [203623, 4546, 213130, 134876, 150994, 247113, 220001, 91610, 223718, 15636]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 1.108238362190761e-10, 1: 7.086846109949363e-10, 2: 1.811187172462425e-10, 3: 1.1659694043597568e-10, 4: 7.43813918877706e-13}
DEBUG SAMPLER: Final top 5 probs: [0.1648438572883606, 0.12838053703308105, 0.12838053703308105, 0.11329542100429535, 0.07786673307418823]
DEBUG SAMPLER: Final top 5 indices: [203623, 4546, 213130, 134876, 150994]
DEBUG SAMPLER: Sampled token: 4546
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=1.250000, std=4.218750
DEBUG: Logits min/max: min=-17.125000, max=20.375000
DEBUG: Top 10 token IDs: [203623, 213130, 4546, 134876, 150994, 247113, 220001, 91610, 223718, 15636]
DEBUG: Top 10 probabilities: [0.12060546875, 0.09423828125, 0.09423828125, 0.0830078125, 0.05712890625, 0.03466796875, 0.0269775390625, 0.02099609375, 0.02099609375, 0.0098876953125]
DEBUG: Selected next token: 4546
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [4546]
DEBUG: Input positions: [38]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.019043, std: 0.968750
DEBUG: Token embeddings min: -3.000000, max: 3.062500
DEBUG: Hidden states before model: mean=-0.019043, std=0.968750
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.039551, std=5.406250
DEBUG: Hidden states after model: min=-20.000000, max=17.875000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.906250, std=5.531250
DEBUG SAMPLER: Top 10 logits: [22.125, 21.125, 20.75, 20.75, 20.5, 20.5, 20.375, 20.25, 19.875, 19.875]
DEBUG SAMPLER: Top 10 indices: [110668, 102877, 1967, 31959, 187415, 36746, 49041, 516, 3564, 129046]
DEBUG SAMPLER: Top 10 probs: [0.21952560544013977, 0.0807589665055275, 0.05550476536154747, 0.05550476536154747, 0.04322715103626251, 0.04322715103626251, 0.03814782574772835, 0.03366534411907196, 0.023137828335165977, 0.023137828335165977]
DEBUG SAMPLER: Top 10 token indices: [110668, 102877, 1967, 31959, 36746, 187415, 49041, 516, 3564, 129046]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 5.1939268349828014e-15, 1: 5.457901908201279e-10, 2: 1.052038123283694e-09, 3: 2.0339708988793316e-15, 4: 1.5092453111917314e-12}
DEBUG SAMPLER: Final top 5 probs: [0.2603798806667328, 0.09578841179609299, 0.06583434343338013, 0.06583434343338013, 0.05127183347940445]
DEBUG SAMPLER: Final top 5 indices: [110668, 102877, 1967, 31959, 36746]
DEBUG SAMPLER: Sampled token: 7297
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.906250, std=5.531250
DEBUG: Logits min/max: min=-27.000000, max=22.125000
DEBUG: Top 10 token IDs: [110668, 102877, 1967, 31959, 187415, 36746, 49041, 516, 3564, 129046]
DEBUG: Top 10 probabilities: [0.2197265625, 0.08056640625, 0.055419921875, 0.055419921875, 0.043212890625, 0.043212890625, 0.0380859375, 0.03369140625, 0.023193359375, 0.023193359375]
DEBUG: Selected next token: 7297
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [7297]
DEBUG: Input positions: [39]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.017578, std: 0.968750
DEBUG: Token embeddings min: -3.375000, max: 2.921875
DEBUG: Hidden states before model: mean=-0.017578, std=0.968750
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.053223, std=4.343750
DEBUG: Hidden states after model: min=-14.125000, max=19.625000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.484375, std=4.781250
DEBUG SAMPLER: Top 10 logits: [18.5, 17.375, 17.25, 16.75, 16.625, 16.25, 15.875, 15.875, 15.875, 15.8125]
DEBUG SAMPLER: Top 10 indices: [237268, 988, 1233, 54366, 217463, 669, 77517, 35209, 236743, 125546]
DEBUG SAMPLER: Top 10 probs: [0.20391196012496948, 0.06620052456855774, 0.05842175707221031, 0.03543458506464958, 0.031270913779735565, 0.021492162719368935, 0.014771333895623684, 0.014771333895623684, 0.014771333895623684, 0.013876383192837238]
DEBUG SAMPLER: Top 10 token indices: [237268, 988, 1233, 54366, 217463, 669, 35209, 77517, 236743, 125546]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 1.0112725298477908e-15, 1: 1.7321507783663037e-08, 2: 1.925942960667726e-09, 3: 1.5662895979867502e-15, 4: 1.8466426304764028e-11}
DEBUG SAMPLER: Final top 5 probs: [0.2835262715816498, 0.09204750508069992, 0.08123163878917694, 0.049269478768110275, 0.04348016530275345]
DEBUG SAMPLER: Final top 5 indices: [237268, 988, 1233, 54366, 217463]
DEBUG SAMPLER: Sampled token: 93079
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.484375, std=4.781250
DEBUG: Logits min/max: min=-22.250000, max=18.500000
DEBUG: Top 10 token IDs: [237268, 988, 1233, 54366, 217463, 669, 77517, 35209, 236743, 125546]
DEBUG: Top 10 probabilities: [0.2041015625, 0.06640625, 0.058349609375, 0.035400390625, 0.03125, 0.021484375, 0.0147705078125, 0.0147705078125, 0.0147705078125, 0.01385498046875]
DEBUG: Selected next token: 93079
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [93079]
DEBUG: Input positions: [40]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.016724, std: 0.945312
DEBUG: Token embeddings min: -3.093750, max: 2.781250
DEBUG: Hidden states before model: mean=-0.016724, std=0.945312
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.111328, std=5.031250
DEBUG: Hidden states after model: min=-15.937500, max=17.000000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-0.468750, std=4.906250
DEBUG SAMPLER: Top 10 logits: [21.5, 21.5, 21.5, 21.25, 20.75, 20.5, 20.25, 20.125, 20.0, 19.5]
DEBUG SAMPLER: Top 10 indices: [46618, 222988, 180623, 47453, 238276, 129915, 1558, 6817, 24839, 77801]
DEBUG SAMPLER: Top 10 probs: [0.11314841359853745, 0.11314841359853745, 0.11314841359853745, 0.08812007308006287, 0.053447529673576355, 0.04162497818470001, 0.03241756558418274, 0.028608398512005806, 0.02524682506918907, 0.015312972478568554]
DEBUG SAMPLER: Top 10 token indices: [46618, 180623, 222988, 47453, 238276, 129915, 1558, 6817, 24839, 77801]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 3.6977831907591963e-10, 1: 6.023787371689515e-12, 2: 7.439494853489492e-11, 3: 1.3817605393207089e-10, 4: 1.9780992504257223e-14}
DEBUG SAMPLER: Final top 5 probs: [0.13243044912815094, 0.13243044912815094, 0.13243044912815094, 0.10313693434000015, 0.0625557154417038]
DEBUG SAMPLER: Final top 5 indices: [46618, 180623, 222988, 47453, 238276]
DEBUG SAMPLER: Sampled token: 47453
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-0.468750, std=4.906250
DEBUG: Logits min/max: min=-23.375000, max=21.500000
DEBUG: Top 10 token IDs: [46618, 222988, 180623, 47453, 238276, 129915, 1558, 6817, 24839, 77801]
DEBUG: Top 10 probabilities: [0.11328125, 0.11328125, 0.11328125, 0.087890625, 0.053466796875, 0.04150390625, 0.032470703125, 0.028564453125, 0.0252685546875, 0.01531982421875]
DEBUG: Selected next token: 47453
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [47453]
DEBUG: Input positions: [41]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.075684, std: 0.972656
DEBUG: Token embeddings min: -2.828125, max: 2.625000
DEBUG: Hidden states before model: mean=-0.075684, std=0.972656
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.033447, std=5.625000
DEBUG: Hidden states after model: min=-17.000000, max=30.125000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.703125, std=5.687500
DEBUG SAMPLER: Top 10 logits: [23.75, 22.875, 22.5, 21.375, 21.0, 20.75, 20.5, 20.375, 20.375, 20.375]
DEBUG SAMPLER: Top 10 indices: [29783, 51311, 66994, 44603, 19959, 88142, 54236, 215943, 148160, 1297]
DEBUG SAMPLER: Top 10 probs: [0.35280027985572815, 0.1470690369606018, 0.10107897967100143, 0.032815538346767426, 0.022553768008947372, 0.017564890906214714, 0.013679551891982555, 0.01207216177135706, 0.01207216177135706, 0.01207216177135706]
DEBUG SAMPLER: Top 10 token indices: [29783, 51311, 66994, 44603, 19959, 88142, 54236, 1297, 148160, 215943]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 1.4340507676745967e-13, 1: 3.066135670012904e-12, 2: 7.545549601806201e-11, 3: 1.7847047895525742e-13, 4: 1.2032430607433753e-11}
DEBUG SAMPLER: Final top 5 probs: [0.4004723131656647, 0.16694170236587524, 0.11473724991083145, 0.03724972903728485, 0.02560134045779705]
DEBUG SAMPLER: Final top 5 indices: [29783, 51311, 66994, 44603, 19959]
DEBUG SAMPLER: Sampled token: 66994
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.703125, std=5.687500
DEBUG: Logits min/max: min=-27.125000, max=23.750000
DEBUG: Top 10 token IDs: [29783, 51311, 66994, 44603, 19959, 88142, 54236, 215943, 148160, 1297]
DEBUG: Top 10 probabilities: [0.353515625, 0.1474609375, 0.10107421875, 0.03271484375, 0.0225830078125, 0.017578125, 0.013671875, 0.0120849609375, 0.0120849609375, 0.0120849609375]
DEBUG: Selected next token: 66994
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [66994]
DEBUG: Input positions: [42]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.000324, std: 0.960938
DEBUG: Token embeddings min: -3.187500, max: 4.125000
DEBUG: Hidden states before model: mean=0.000324, std=0.960938
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.076172, std=5.031250
DEBUG: Hidden states after model: min=-18.875000, max=28.625000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.968750, std=5.125000
DEBUG SAMPLER: Top 10 logits: [24.125, 19.5, 17.75, 17.25, 17.125, 17.0, 17.0, 17.0, 16.75, 16.625]
DEBUG SAMPLER: Top 10 indices: [153066, 190929, 239937, 237634, 55623, 40016, 82069, 10514, 233085, 151858]
DEBUG SAMPLER: Top 10 probs: [0.9689809679985046, 0.009499555453658104, 0.0016507753171026707, 0.0010012458078563213, 0.0008835963089950383, 0.0007797710131853819, 0.0007797710131853819, 0.0007797710131853819, 0.000607286230660975, 0.0005359282367862761]
DEBUG SAMPLER: Top 10 token indices: [153066, 190929, 239937, 237634, 55623, 10514, 40016, 82069, 233085, 151858]
DEBUG SAMPLER: Probability sum: 1.0000001192092896
DEBUG SAMPLER: Special token probabilities: {0: 1.9834765385062038e-16, 1: 7.592006992691858e-13, 2: 4.500801953355449e-09, 3: 5.682755954529922e-17, 4: 2.358251371958886e-08}
DEBUG SAMPLER: Final top 5 probs: [1.0, 0.0, 0.0, 0.0, 0.0]
DEBUG SAMPLER: Final top 5 indices: [153066, 190929, 239937, 237634, 55623]
DEBUG SAMPLER: Sampled token: 153066
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.968750, std=5.125000
DEBUG: Logits min/max: min=-26.250000, max=24.125000
DEBUG: Top 10 token IDs: [153066, 190929, 239937, 237634, 55623, 40016, 82069, 10514, 233085, 151858]
DEBUG: Top 10 probabilities: [0.96875, 0.009521484375, 0.00164794921875, 0.00099945068359375, 0.000885009765625, 0.0007781982421875, 0.0007781982421875, 0.0007781982421875, 0.000606536865234375, 0.0005340576171875]
DEBUG: Selected next token: 153066
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [153066]
DEBUG: Input positions: [43]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.022461, std: 1.007812
DEBUG: Token embeddings min: -2.890625, max: 3.187500
DEBUG: Hidden states before model: mean=0.022461, std=1.007812
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.022583, std=4.062500
DEBUG: Hidden states after model: min=-16.375000, max=14.500000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.195312, std=4.156250
DEBUG SAMPLER: Top 10 logits: [19.75, 18.25, 18.25, 17.625, 17.375, 15.8125, 15.6875, 15.625, 15.4375, 15.375]
DEBUG SAMPLER: Top 10 indices: [236760, 36131, 4058, 15322, 2296, 36965, 6180, 1094, 13238, 243844]
DEBUG SAMPLER: Top 10 probs: [0.4269845187664032, 0.09527312964200974, 0.09527312964200974, 0.05099603161215782, 0.03971574455499649, 0.008324873633682728, 0.007346674334257841, 0.006901562213897705, 0.005721596069633961, 0.005374941974878311]
DEBUG SAMPLER: Top 10 token indices: [236760, 4058, 36131, 15322, 2296, 36965, 6180, 1094, 13238, 243844]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 2.98175958750313e-12, 1: 5.989023860175635e-11, 2: 1.0936748173762112e-09, 3: 1.5469164577758265e-12, 4: 1.1894938367618124e-09}
DEBUG SAMPLER: Final top 5 probs: [0.5072632431983948, 0.11318574100732803, 0.11318574100732803, 0.06058396026492119, 0.0471828319132328]
DEBUG SAMPLER: Final top 5 indices: [236760, 4058, 36131, 15322, 2296]
DEBUG SAMPLER: Sampled token: 236760
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.195312, std=4.156250
DEBUG: Logits min/max: min=-20.125000, max=19.750000
DEBUG: Top 10 token IDs: [236760, 36131, 4058, 15322, 2296, 36965, 6180, 1094, 13238, 243844]
DEBUG: Top 10 probabilities: [0.427734375, 0.09521484375, 0.09521484375, 0.051025390625, 0.039794921875, 0.00830078125, 0.007354736328125, 0.00689697265625, 0.005706787109375, 0.00537109375]
DEBUG: Selected next token: 236760
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [236760]
DEBUG: Input positions: [44]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.069824, std: 1.015625
DEBUG: Token embeddings min: -4.750000, max: 3.734375
DEBUG: Hidden states before model: mean=0.069824, std=1.015625
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.045898, std=5.000000
DEBUG: Hidden states after model: min=-36.250000, max=17.000000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=0.527344, std=4.718750
DEBUG SAMPLER: Top 10 logits: [21.75, 21.25, 21.125, 20.75, 20.5, 20.25, 19.75, 19.625, 19.25, 19.125]
DEBUG SAMPLER: Top 10 indices: [191105, 237530, 6119, 227546, 139084, 171347, 8693, 236841, 131879, 57082]
DEBUG SAMPLER: Top 10 probs: [0.1759265661239624, 0.10670486092567444, 0.09416671097278595, 0.0647197738289833, 0.05040380731225014, 0.03925452381372452, 0.023809071630239487, 0.021011432632803917, 0.014440932311117649, 0.012744078412652016]
DEBUG SAMPLER: Top 10 token indices: [191105, 237530, 6119, 227546, 139084, 171347, 8693, 236841, 131879, 57082]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 2.964893608403685e-11, 1: 5.381634338164076e-08, 2: 5.72872025372817e-08, 3: 7.409088967347266e-12, 4: 2.401761012293946e-09}
DEBUG SAMPLER: Final top 5 probs: [0.2238379269838333, 0.13576456904411316, 0.11981181800365448, 0.0823453813791275, 0.06413064152002335]
DEBUG SAMPLER: Final top 5 indices: [191105, 237530, 6119, 227546, 139084]
DEBUG SAMPLER: Sampled token: 171347
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=0.527344, std=4.718750
DEBUG: Logits min/max: min=-20.375000, max=21.750000
DEBUG: Top 10 token IDs: [191105, 237530, 6119, 227546, 139084, 171347, 8693, 236841, 131879, 57082]
DEBUG: Top 10 probabilities: [0.17578125, 0.10693359375, 0.09423828125, 0.06494140625, 0.05029296875, 0.039306640625, 0.0238037109375, 0.02099609375, 0.01446533203125, 0.01275634765625]
DEBUG: Selected next token: 171347
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [171347]
DEBUG: Input positions: [45]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.000977, std: 0.953125
DEBUG: Token embeddings min: -3.421875, max: 3.781250
DEBUG: Hidden states before model: mean=0.000977, std=0.953125
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.019165, std=4.750000
DEBUG: Hidden states after model: min=-16.000000, max=26.625000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-0.859375, std=4.781250
DEBUG SAMPLER: Top 10 logits: [20.375, 20.0, 19.5, 19.125, 18.75, 18.75, 18.625, 18.375, 18.375, 18.25]
DEBUG SAMPLER: Top 10 indices: [133898, 195404, 908, 100221, 98036, 90452, 14110, 45219, 7282, 86198]
DEBUG SAMPLER: Top 10 probs: [0.16290099918842316, 0.11196011304855347, 0.06790723651647568, 0.04667191952466965, 0.032077107578516006, 0.032077107578516006, 0.028307948261499405, 0.022046253085136414, 0.022046253085136414, 0.01945575140416622]
DEBUG SAMPLER: Top 10 token indices: [133898, 195404, 908, 100221, 90452, 98036, 14110, 7282, 45219, 86198]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 9.33787552490073e-14, 1: 8.245568117182245e-10, 2: 1.452372510452049e-11, 3: 4.410900017091067e-14, 4: 1.3721907724667526e-12}
DEBUG SAMPLER: Final top 5 probs: [0.2003052532672882, 0.13766765594482422, 0.08349964767694473, 0.05738842114806175, 0.03944244235754013]
DEBUG SAMPLER: Final top 5 indices: [133898, 195404, 908, 100221, 90452]
DEBUG SAMPLER: Sampled token: 100221
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-0.859375, std=4.781250
DEBUG: Logits min/max: min=-23.750000, max=20.375000
DEBUG: Top 10 token IDs: [133898, 195404, 908, 100221, 98036, 90452, 14110, 45219, 7282, 86198]
DEBUG: Top 10 probabilities: [0.1630859375, 0.11181640625, 0.06787109375, 0.046630859375, 0.031982421875, 0.031982421875, 0.0283203125, 0.0220947265625, 0.0220947265625, 0.0194091796875]
DEBUG: Selected next token: 100221
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [100221]
DEBUG: Input positions: [46]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.019043, std: 0.980469
DEBUG: Token embeddings min: -3.453125, max: 3.703125
DEBUG: Hidden states before model: mean=0.019043, std=0.980469
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.145508, std=5.031250
DEBUG: Hidden states after model: min=-17.625000, max=20.750000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.585938, std=4.937500
DEBUG SAMPLER: Top 10 logits: [20.375, 20.25, 20.0, 19.875, 19.875, 19.75, 19.5, 19.125, 19.0, 19.0]
DEBUG SAMPLER: Top 10 indices: [84453, 125824, 243491, 78419, 38047, 149555, 30078, 5129, 115408, 130109]
DEBUG SAMPLER: Top 10 probs: [0.0962924063205719, 0.08497775346040726, 0.0661807432770729, 0.05840430036187172, 0.05840430036187172, 0.05154161527752876, 0.04014064744114876, 0.027588237076997757, 0.02434653416275978, 0.02434653416275978]
DEBUG SAMPLER: Top 10 token indices: [84453, 125824, 243491, 38047, 78419, 149555, 30078, 5129, 115408, 130109]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 1.885900290099496e-12, 1: 2.4946540388981475e-09, 2: 0.014766919426620007, 3: 1.945765380415021e-12, 4: 1.2176287539009123e-12}
DEBUG SAMPLER: High probability tokens in first 100: {2: 0.014766919426620007}
DEBUG SAMPLER: Final top 5 probs: [0.11430509388446808, 0.10087389498949051, 0.07856067270040512, 0.06932955235242844, 0.06932955235242844]
DEBUG SAMPLER: Final top 5 indices: [84453, 125824, 243491, 38047, 78419]
DEBUG SAMPLER: Sampled token: 238326
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.585938, std=4.937500
DEBUG: Logits min/max: min=-25.375000, max=20.375000
DEBUG: Top 10 token IDs: [84453, 125824, 243491, 78419, 38047, 149555, 30078, 5129, 115408, 130109]
DEBUG: Top 10 probabilities: [0.09619140625, 0.0849609375, 0.06640625, 0.058349609375, 0.058349609375, 0.051513671875, 0.0400390625, 0.027587890625, 0.0242919921875, 0.0242919921875]
DEBUG: Selected next token: 238326
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [238326]
DEBUG: Input positions: [47]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.021118, std: 0.972656
DEBUG: Token embeddings min: -4.187500, max: 3.296875
DEBUG: Hidden states before model: mean=-0.021118, std=0.972656
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.175781, std=5.468750
DEBUG: Hidden states after model: min=-18.250000, max=18.750000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.062500, std=5.500000
DEBUG SAMPLER: Top 10 logits: [22.0, 20.125, 20.125, 20.0, 19.75, 19.75, 19.625, 19.25, 19.125, 19.125]
DEBUG SAMPLER: Top 10 indices: [3111, 79405, 13399, 236763, 241068, 243339, 80476, 46019, 55930, 124199]
DEBUG SAMPLER: Top 10 probs: [0.3153655230998993, 0.04836286976933479, 0.04836286976933479, 0.042680080980062485, 0.03323927894234657, 0.03323927894234657, 0.029333561658859253, 0.02016064152121544, 0.017791705206036568, 0.017791705206036568]
DEBUG SAMPLER: Top 10 token indices: [3111, 13399, 79405, 236763, 241068, 243339, 80476, 46019, 55930, 124199]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 4.525614874983843e-15, 1: 7.222112491378496e-11, 2: 3.439781082192894e-08, 3: 1.4692519153625214e-15, 4: 9.258044109117236e-09}
DEBUG SAMPLER: Final top 5 probs: [0.3565638065338135, 0.05468083173036575, 0.05468083173036575, 0.048255663365125656, 0.03758154436945915]
DEBUG SAMPLER: Final top 5 indices: [3111, 13399, 79405, 236763, 241068]
DEBUG SAMPLER: Sampled token: 149259
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.062500, std=5.500000
DEBUG: Logits min/max: min=-26.250000, max=22.000000
DEBUG: Top 10 token IDs: [3111, 79405, 13399, 236763, 241068, 243339, 80476, 46019, 55930, 124199]
DEBUG: Top 10 probabilities: [0.314453125, 0.04833984375, 0.04833984375, 0.042724609375, 0.033203125, 0.033203125, 0.029296875, 0.0201416015625, 0.017822265625, 0.017822265625]
DEBUG: Selected next token: 149259
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [149259]
DEBUG: Input positions: [48]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.014954, std: 0.949219
DEBUG: Token embeddings min: -3.156250, max: 3.078125
DEBUG: Hidden states before model: mean=-0.014954, std=0.949219
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.015015, std=4.687500
DEBUG: Hidden states after model: min=-16.250000, max=17.875000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.000000, std=4.968750
DEBUG SAMPLER: Top 10 logits: [19.0, 19.0, 18.25, 16.625, 16.625, 16.625, 16.0, 15.875, 15.875, 15.625]
DEBUG SAMPLER: Top 10 indices: [179349, 27373, 5267, 211222, 60812, 132906, 112280, 65886, 91165, 236786]
DEBUG SAMPLER: Top 10 probs: [0.23129726946353912, 0.23129726946353912, 0.10925710201263428, 0.02151399664580822, 0.02151399664580822, 0.02151399664580822, 0.011515612713992596, 0.010162493214011192, 0.010162493214011192, 0.007914557121694088]
DEBUG SAMPLER: Top 10 token indices: [27373, 179349, 5267, 60812, 132906, 211222, 112280, 65886, 91165, 236786]
DEBUG SAMPLER: Probability sum: 1.0000001192092896
DEBUG SAMPLER: Special token probabilities: {0: 4.536813497579492e-15, 1: 5.6376626457466106e-12, 2: 7.705770917187937e-12, 3: 2.2812530622260416e-15, 4: 1.6830953297741758e-11}
DEBUG SAMPLER: Final top 5 probs: [0.2758706510066986, 0.2758706510066986, 0.13031207025051117, 0.025659967213869095, 0.025659967213869095]
DEBUG SAMPLER: Final top 5 indices: [27373, 179349, 5267, 60812, 132906]
DEBUG SAMPLER: Sampled token: 179349
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.000000, std=4.968750
DEBUG: Logits min/max: min=-24.125000, max=19.000000
DEBUG: Top 10 token IDs: [179349, 27373, 5267, 211222, 60812, 132906, 112280, 65886, 91165, 236786]
DEBUG: Top 10 probabilities: [0.2314453125, 0.2314453125, 0.109375, 0.021484375, 0.021484375, 0.021484375, 0.01153564453125, 0.01019287109375, 0.01019287109375, 0.0079345703125]
DEBUG: Selected next token: 179349
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [179349]
DEBUG: Input positions: [49]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.013062, std: 0.941406
DEBUG: Token embeddings min: -3.390625, max: 3.234375
DEBUG: Hidden states before model: mean=0.013062, std=0.941406
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.023804, std=4.968750
DEBUG: Hidden states after model: min=-15.625000, max=22.375000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=1.664062, std=4.781250
DEBUG SAMPLER: Top 10 logits: [21.75, 21.75, 21.625, 21.625, 21.375, 21.125, 21.125, 21.125, 21.0, 21.0]
DEBUG SAMPLER: Top 10 indices: [26347, 37262, 167811, 38975, 594, 88421, 139534, 162630, 4493, 135045]
DEBUG SAMPLER: Top 10 probs: [0.06016658991575241, 0.06016658991575241, 0.05309683084487915, 0.05309683084487915, 0.04135185480117798, 0.03220485523343086, 0.03220485523343086, 0.03220485523343086, 0.028420686721801758, 0.028420686721801758]
DEBUG SAMPLER: Top 10 token indices: [26347, 37262, 38975, 167811, 594, 88421, 139534, 162630, 4493, 135045]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 7.332071581822586e-12, 1: 1.09605589036256e-13, 2: 6.916866523987153e-10, 3: 5.6657859827391466e-12, 4: 9.434379666384274e-11}
DEBUG SAMPLER: Final top 5 probs: [0.08603570610284805, 0.08603570610284805, 0.07592624425888062, 0.07592624425888062, 0.059131424874067307]
DEBUG SAMPLER: Final top 5 indices: [26347, 37262, 38975, 167811, 594]
DEBUG SAMPLER: Sampled token: 26347
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=1.664062, std=4.781250
DEBUG: Logits min/max: min=-20.500000, max=21.750000
DEBUG: Top 10 token IDs: [26347, 37262, 167811, 38975, 594, 88421, 139534, 162630, 4493, 135045]
DEBUG: Top 10 probabilities: [0.06005859375, 0.06005859375, 0.052978515625, 0.052978515625, 0.041259765625, 0.0322265625, 0.0322265625, 0.0322265625, 0.0284423828125, 0.0284423828125]
DEBUG: Selected next token: 26347
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [26347]
DEBUG: Input positions: [50]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.033447, std: 0.906250
DEBUG: Token embeddings min: -3.078125, max: 2.828125
DEBUG: Hidden states before model: mean=-0.033447, std=0.906250
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.182617, std=5.125000
DEBUG: Hidden states after model: min=-16.250000, max=19.625000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=0.636719, std=5.031250
DEBUG SAMPLER: Top 10 logits: [24.0, 23.25, 21.125, 20.75, 20.5, 20.375, 20.25, 20.25, 20.125, 19.875]
DEBUG SAMPLER: Top 10 indices: [62015, 76966, 105895, 119996, 4582, 211534, 39865, 216988, 242825, 126605]
DEBUG SAMPLER: Top 10 probs: [0.4349820911884308, 0.205471009016037, 0.024540012702345848, 0.016866086050868034, 0.01313532143831253, 0.011591879650950432, 0.010229798965156078, 0.010229798965156078, 0.00902776513248682, 0.007030830718576908]
DEBUG SAMPLER: Top 10 token indices: [62015, 76966, 105895, 119996, 4582, 211534, 39865, 216988, 242825, 126605]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 1.9777905535534046e-08, 1: 4.193290159548724e-11, 2: 7.475348118290981e-11, 3: 1.2769574730953082e-08, 4: 7.052028738741001e-09}
DEBUG SAMPLER: Final top 5 probs: [0.497751921415329, 0.23512138426303864, 0.028081245720386505, 0.019299937412142754, 0.015030806884169579]
DEBUG SAMPLER: Final top 5 indices: [62015, 76966, 105895, 119996, 4582]
DEBUG SAMPLER: Sampled token: 62015
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=0.636719, std=5.031250
DEBUG: Logits min/max: min=-22.750000, max=24.000000
DEBUG: Top 10 token IDs: [62015, 76966, 105895, 119996, 4582, 211534, 39865, 216988, 242825, 126605]
DEBUG: Top 10 probabilities: [0.435546875, 0.205078125, 0.0245361328125, 0.016845703125, 0.01312255859375, 0.0115966796875, 0.01025390625, 0.01025390625, 0.009033203125, 0.00701904296875]
DEBUG: Selected next token: 62015
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [62015]
DEBUG: Input positions: [51]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.019653, std: 0.921875
DEBUG: Token embeddings min: -3.031250, max: 2.953125
DEBUG: Hidden states before model: mean=0.019653, std=0.921875
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.018799, std=5.000000
DEBUG: Hidden states after model: min=-24.375000, max=17.375000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.453125, std=4.812500
DEBUG SAMPLER: Top 10 logits: [22.625, 18.25, 16.0, 15.8125, 15.75, 15.625, 15.4375, 15.25, 15.25, 15.125]
DEBUG SAMPLER: Top 10 indices: [6922, 9592, 33186, 149846, 7961, 17382, 36294, 232891, 26314, 3133]
DEBUG SAMPLER: Top 10 probs: [0.9567867517471313, 0.012044168077409267, 0.001269445987418294, 0.0010524076642468572, 0.0009886454790830612, 0.000872476608492434, 0.0007233085343614221, 0.0005996437976136804, 0.0005996437976136804, 0.0005291838315315545]
DEBUG SAMPLER: Top 10 token indices: [6922, 9592, 33186, 149846, 7961, 17382, 36294, 26314, 232891, 3133]
DEBUG SAMPLER: Probability sum: 1.0000001192092896
DEBUG SAMPLER: Special token probabilities: {0: 5.838252515981412e-13, 1: 3.785342525541857e-11, 2: 1.5294165239820856e-11, 3: 3.0288453494418943e-13, 4: 3.2612210542883264e-15}
DEBUG SAMPLER: Final top 5 probs: [1.0, 0.0, 0.0, 0.0, 0.0]
DEBUG SAMPLER: Final top 5 indices: [6922, 9592, 33186, 149846, 7961]
DEBUG SAMPLER: Sampled token: 6922
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.453125, std=4.812500
DEBUG: Logits min/max: min=-27.750000, max=22.625000
DEBUG: Top 10 token IDs: [6922, 9592, 33186, 149846, 7961, 17382, 36294, 232891, 26314, 3133]
DEBUG: Top 10 probabilities: [0.95703125, 0.01202392578125, 0.0012664794921875, 0.0010528564453125, 0.0009918212890625, 0.000873565673828125, 0.00072479248046875, 0.000598907470703125, 0.000598907470703125, 0.000530242919921875]
DEBUG: Selected next token: 6922
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [6922]
DEBUG: Input positions: [52]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.036133, std: 0.945312
DEBUG: Token embeddings min: -2.968750, max: 2.656250
DEBUG: Hidden states before model: mean=0.036133, std=0.945312
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.046631, std=5.625000
DEBUG: Hidden states after model: min=-28.250000, max=30.625000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.703125, std=5.906250
DEBUG SAMPLER: Top 10 logits: [26.0, 23.5, 22.625, 22.625, 21.375, 20.375, 20.25, 20.25, 20.125, 19.875]
DEBUG SAMPLER: Top 10 indices: [8382, 22841, 28446, 33775, 154302, 236764, 42239, 3756, 8245, 62366]
DEBUG SAMPLER: Top 10 probs: [0.8237659335136414, 0.06761882454156876, 0.028187720105051994, 0.028187720105051994, 0.008075918070971966, 0.0029709639493376017, 0.0026218665298074484, 0.0026218665298074484, 0.002313789213076234, 0.0018019807757809758]
DEBUG SAMPLER: Top 10 token indices: [8382, 22841, 28446, 33775, 154302, 236764, 3756, 42239, 8245, 62366]
DEBUG SAMPLER: Probability sum: 1.0000001192092896
DEBUG SAMPLER: Special token probabilities: {0: 2.560398413019835e-18, 1: 1.0432309347169298e-14, 2: 5.818673857625398e-14, 3: 8.848499904989997e-19, 4: 1.0329384679682533e-15}
DEBUG SAMPLER: Final top 5 probs: [0.8618276119232178, 0.0707431212067604, 0.029490120708942413, 0.029490120708942413, 0.008449061773717403]
DEBUG SAMPLER: Final top 5 indices: [8382, 22841, 28446, 33775, 154302]
DEBUG SAMPLER: Sampled token: 8382
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.703125, std=5.906250
DEBUG: Logits min/max: min=-27.750000, max=26.000000
DEBUG: Top 10 token IDs: [8382, 22841, 28446, 33775, 154302, 236764, 42239, 3756, 8245, 62366]
DEBUG: Top 10 probabilities: [0.82421875, 0.0673828125, 0.0281982421875, 0.0281982421875, 0.008056640625, 0.0029754638671875, 0.00262451171875, 0.00262451171875, 0.0023193359375, 0.001800537109375]
DEBUG: Selected next token: 8382
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [8382]
DEBUG: Input positions: [53]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.039307, std: 1.007812
DEBUG: Token embeddings min: -3.265625, max: 3.765625
DEBUG: Hidden states before model: mean=0.039307, std=1.007812
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.008545, std=4.593750
DEBUG: Hidden states after model: min=-16.625000, max=18.750000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.937500, std=4.937500
DEBUG SAMPLER: Top 10 logits: [18.875, 18.875, 17.375, 16.625, 16.375, 16.0, 15.75, 15.375, 15.375, 15.375]
DEBUG SAMPLER: Top 10 indices: [236743, 549, 4919, 568, 3460, 5457, 236769, 72896, 57258, 5197]
DEBUG SAMPLER: Top 10 probs: [0.3264848291873932, 0.3264848291873932, 0.0728486180305481, 0.034411247819662094, 0.026799507439136505, 0.018419014289975166, 0.014344742521643639, 0.009858988225460052, 0.009858988225460052, 0.009858988225460052]
DEBUG SAMPLER: Top 10 token indices: [549, 236743, 4919, 568, 3460, 5457, 236769, 5197, 57258, 72896]
DEBUG SAMPLER: Probability sum: 1.0000004768371582
DEBUG SAMPLER: Special token probabilities: {0: 2.995136294759788e-16, 1: 2.114300201583319e-07, 2: 1.157849337241057e-11, 3: 8.581208732316551e-17, 4: 4.4384902003757887e-11}
DEBUG SAMPLER: Final top 5 probs: [0.3474525511264801, 0.3474525511264801, 0.07752715051174164, 0.03662123158574104, 0.028520643711090088]
DEBUG SAMPLER: Final top 5 indices: [549, 236743, 4919, 568, 3460]
DEBUG SAMPLER: Sampled token: 549
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.937500, std=4.937500
DEBUG: Logits min/max: min=-25.125000, max=18.875000
DEBUG: Top 10 token IDs: [236743, 549, 4919, 568, 3460, 5457, 236769, 72896, 57258, 5197]
DEBUG: Top 10 probabilities: [0.326171875, 0.326171875, 0.07275390625, 0.034423828125, 0.02685546875, 0.0184326171875, 0.01434326171875, 0.0098876953125, 0.0098876953125, 0.0098876953125]
DEBUG: Selected next token: 549
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [549]
DEBUG: Input positions: [54]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.024658, std: 0.992188
DEBUG: Token embeddings min: -2.875000, max: 4.468750
DEBUG: Hidden states before model: mean=0.024658, std=0.992188
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.123535, std=5.031250
DEBUG: Hidden states after model: min=-24.250000, max=26.250000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.007812, std=4.687500
DEBUG SAMPLER: Top 10 logits: [20.25, 19.125, 19.0, 19.0, 18.625, 18.375, 17.875, 17.875, 17.75, 17.75]
DEBUG SAMPLER: Top 10 indices: [14610, 5426, 217022, 4118, 180095, 36742, 83766, 104787, 141165, 237776]
DEBUG SAMPLER: Top 10 probs: [0.1890108436346054, 0.06136283278465271, 0.05415251478552818, 0.05415251478552818, 0.03721844032406807, 0.028985751792788506, 0.017580745741724968, 0.017580745741724968, 0.01551495399326086, 0.01551495399326086]
DEBUG SAMPLER: Top 10 token indices: [14610, 5426, 4118, 217022, 180095, 36742, 83766, 104787, 141165, 237776]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 7.014222092038835e-09, 1: 1.5873398524490767e-06, 2: 1.1306871288725836e-15, 3: 4.7460604335469725e-09, 4: 1.213774740438378e-11}
DEBUG SAMPLER: Final top 5 probs: [0.2506764233112335, 0.08138271421194077, 0.07181999832391739, 0.07181999832391739, 0.04936110973358154]
DEBUG SAMPLER: Final top 5 indices: [14610, 5426, 4118, 217022, 180095]
DEBUG SAMPLER: Sampled token: 38719
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.007812, std=4.687500
DEBUG: Logits min/max: min=-22.125000, max=20.250000
DEBUG: Top 10 token IDs: [14610, 5426, 217022, 4118, 180095, 36742, 83766, 104787, 141165, 237776]
DEBUG: Top 10 probabilities: [0.189453125, 0.061279296875, 0.05419921875, 0.05419921875, 0.037109375, 0.0289306640625, 0.017578125, 0.017578125, 0.0155029296875, 0.0155029296875]
DEBUG: Selected next token: 38719
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [38719]
DEBUG: Input positions: [55]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.037598, std: 0.960938
DEBUG: Token embeddings min: -5.125000, max: 3.921875
DEBUG: Hidden states before model: mean=-0.037598, std=0.960938
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.037109, std=5.531250
DEBUG: Hidden states after model: min=-20.125000, max=21.875000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.437500, std=5.531250
DEBUG SAMPLER: Top 10 logits: [23.625, 22.5, 22.125, 22.0, 21.75, 20.875, 20.125, 19.875, 19.75, 19.625]
DEBUG SAMPLER: Top 10 indices: [45370, 5976, 898, 22141, 71859, 96051, 233100, 42282, 41641, 1562]
DEBUG SAMPLER: Top 10 probs: [0.41050198674201965, 0.1332704722881317, 0.0915953740477562, 0.08083263039588928, 0.06295251846313477, 0.026242513209581375, 0.012396086007356644, 0.009654081426560879, 0.00851969700306654, 0.007518606260418892]
DEBUG SAMPLER: Top 10 token indices: [45370, 5976, 898, 22141, 71859, 96051, 233100, 42282, 41641, 1562]
DEBUG SAMPLER: Probability sum: 1.0000001192092896
DEBUG SAMPLER: Special token probabilities: {0: 1.5855033757249115e-15, 1: 1.8799998450363953e-12, 2: 5.557268512555724e-13, 3: 4.542542404918247e-16, 4: 2.3530954745484833e-13}
DEBUG SAMPLER: Final top 5 probs: [0.4318809509277344, 0.14021119475364685, 0.09636566042900085, 0.08504239469766617, 0.06623108685016632]
DEBUG SAMPLER: Final top 5 indices: [45370, 5976, 898, 22141, 71859]
DEBUG SAMPLER: Sampled token: 45370
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.437500, std=5.531250
DEBUG: Logits min/max: min=-28.375000, max=23.625000
DEBUG: Top 10 token IDs: [45370, 5976, 898, 22141, 71859, 96051, 233100, 42282, 41641, 1562]
DEBUG: Top 10 probabilities: [0.41015625, 0.1328125, 0.091796875, 0.0810546875, 0.06298828125, 0.0262451171875, 0.01239013671875, 0.0096435546875, 0.008544921875, 0.00750732421875]
DEBUG: Selected next token: 45370
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [45370]
DEBUG: Input positions: [56]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.016235, std: 0.976562
DEBUG: Token embeddings min: -3.437500, max: 3.656250
DEBUG: Hidden states before model: mean=0.016235, std=0.976562
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.167969, std=4.406250
DEBUG: Hidden states after model: min=-13.375000, max=15.687500
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=0.246094, std=4.187500
DEBUG SAMPLER: Top 10 logits: [19.625, 19.25, 18.75, 17.625, 17.375, 17.375, 17.375, 17.375, 17.125, 17.0]
DEBUG SAMPLER: Top 10 indices: [237534, 33080, 244629, 78878, 14709, 2875, 30883, 243527, 7604, 12865]
DEBUG SAMPLER: Top 10 probs: [0.17042909562587738, 0.11713409423828125, 0.0710454136133194, 0.023065069690346718, 0.017963094636797905, 0.017963094636797905, 0.017963094636797905, 0.017963094636797905, 0.013989672064781189, 0.012345843017101288]
DEBUG SAMPLER: Top 10 token indices: [237534, 33080, 244629, 78878, 2875, 14709, 30883, 243527, 7604, 12865]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 6.226596571679011e-09, 1: 1.2722538300646846e-10, 2: 1.6218616565311095e-06, 3: 3.438652917964191e-09, 4: 5.621902682967361e-13}
DEBUG SAMPLER: Final top 5 probs: [0.23926807940006256, 0.1644463837146759, 0.09974176436662674, 0.032381411641836166, 0.025218669325113297]
DEBUG SAMPLER: Final top 5 indices: [237534, 33080, 244629, 78878, 2875]
DEBUG SAMPLER: Sampled token: 199023
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=0.246094, std=4.187500
DEBUG: Logits min/max: min=-18.750000, max=19.625000
DEBUG: Top 10 token IDs: [237534, 33080, 244629, 78878, 14709, 2875, 30883, 243527, 7604, 12865]
DEBUG: Top 10 probabilities: [0.1708984375, 0.1171875, 0.0712890625, 0.0230712890625, 0.0179443359375, 0.0179443359375, 0.0179443359375, 0.0179443359375, 0.01397705078125, 0.0123291015625]
DEBUG: Selected next token: 199023
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [199023]
DEBUG: Input positions: [57]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.023682, std: 0.949219
DEBUG: Token embeddings min: -3.218750, max: 3.078125
DEBUG: Hidden states before model: mean=0.023682, std=0.949219
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.002655, std=5.718750
DEBUG: Hidden states after model: min=-21.875000, max=41.750000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-4.812500, std=5.750000
DEBUG SAMPLER: Top 10 logits: [20.125, 19.875, 19.75, 19.625, 19.125, 18.75, 18.75, 18.375, 18.0, 17.875]
DEBUG SAMPLER: Top 10 indices: [20732, 1621, 144738, 86922, 181599, 52544, 94891, 244694, 179, 61314]
DEBUG SAMPLER: Top 10 probs: [0.15866337716579437, 0.12356717139482498, 0.10904764384031296, 0.09623420983552933, 0.05836900323629379, 0.040116384625434875, 0.040116384625434875, 0.027571558952331543, 0.018949639052152634, 0.016722997650504112]
DEBUG SAMPLER: Top 10 token indices: [20732, 1621, 144738, 86922, 181599, 52544, 94891, 244694, 179, 61314]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 6.944090980434226e-16, 1: 1.6189138918321078e-10, 2: 1.4656450497696483e-09, 3: 1.1448870009585915e-15, 4: 1.3102538361741117e-14}
DEBUG SAMPLER: Final top 5 probs: [0.1736825555562973, 0.13526412844657898, 0.11937016993761063, 0.10534380376338959, 0.06389424949884415]
DEBUG SAMPLER: Final top 5 indices: [20732, 1621, 144738, 86922, 181599]
DEBUG SAMPLER: Sampled token: 52544
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-4.812500, std=5.750000
DEBUG: Logits min/max: min=-31.125000, max=20.125000
DEBUG: Top 10 token IDs: [20732, 1621, 144738, 86922, 181599, 52544, 94891, 244694, 179, 61314]
DEBUG: Top 10 probabilities: [0.158203125, 0.12353515625, 0.10888671875, 0.09619140625, 0.058349609375, 0.0400390625, 0.0400390625, 0.027587890625, 0.0189208984375, 0.0167236328125]
DEBUG: Selected next token: 52544
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [52544]
DEBUG: Input positions: [58]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.015747, std: 0.992188
DEBUG: Token embeddings min: -3.734375, max: 2.812500
DEBUG: Hidden states before model: mean=0.015747, std=0.992188
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.063477, std=4.843750
DEBUG: Hidden states after model: min=-17.000000, max=17.750000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.453125, std=4.843750
DEBUG SAMPLER: Top 10 logits: [20.875, 20.875, 19.75, 19.625, 19.5, 19.125, 18.625, 18.5, 18.375, 18.375]
DEBUG SAMPLER: Top 10 indices: [134850, 59572, 77931, 135153, 6670, 135872, 97699, 1754, 159043, 194917]
DEBUG SAMPLER: Top 10 probs: [0.21898789703845978, 0.21898789703845978, 0.07109495997428894, 0.06274108588695526, 0.0553688108921051, 0.038054388016462326, 0.023081153631210327, 0.020369045436382294, 0.017975620925426483, 0.017975620925426483]
DEBUG SAMPLER: Top 10 token indices: [59572, 134850, 77931, 135153, 6670, 135872, 97699, 1754, 159043, 194917]
DEBUG SAMPLER: Probability sum: 1.0000004768371582
DEBUG SAMPLER: Special token probabilities: {0: 1.422430966416255e-13, 1: 2.392161295439177e-11, 2: 1.319089815154939e-08, 3: 9.47543299790847e-14, 4: 6.932374460718801e-14}
DEBUG SAMPLER: Final top 5 probs: [0.24594153463840485, 0.24594153463840485, 0.07984552532434464, 0.07046343386173248, 0.06218376010656357]
DEBUG SAMPLER: Final top 5 indices: [59572, 134850, 77931, 135153, 6670]
DEBUG SAMPLER: Sampled token: 59572
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.453125, std=4.843750
DEBUG: Logits min/max: min=-22.750000, max=20.875000
DEBUG: Top 10 token IDs: [134850, 59572, 77931, 135153, 6670, 135872, 97699, 1754, 159043, 194917]
DEBUG: Top 10 probabilities: [0.21875, 0.21875, 0.0712890625, 0.0625, 0.055419921875, 0.0380859375, 0.0230712890625, 0.0203857421875, 0.0179443359375, 0.0179443359375]
DEBUG: Selected next token: 59572
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [59572]
DEBUG: Input positions: [59]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.029663, std: 0.964844
DEBUG: Token embeddings min: -3.750000, max: 2.640625
DEBUG: Hidden states before model: mean=-0.029663, std=0.964844
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.058105, std=4.875000
DEBUG: Hidden states after model: min=-29.500000, max=19.500000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-6.000000, std=4.812500
DEBUG SAMPLER: Top 10 logits: [17.125, 16.0, 15.6875, 15.5625, 15.5625, 14.75, 14.625, 14.5, 14.25, 14.125]
DEBUG SAMPLER: Top 10 indices: [175662, 151314, 32299, 91182, 52668, 496, 19848, 239897, 155074, 1911]
DEBUG SAMPLER: Top 10 probs: [0.3110414743423462, 0.10098037868738174, 0.07387882471084595, 0.06519783288240433, 0.06519783288240433, 0.02893136255443096, 0.025531837716698647, 0.022531770169734955, 0.017547760158777237, 0.01548584271222353]
DEBUG SAMPLER: Top 10 token indices: [175662, 151314, 32299, 52668, 91182, 496, 19848, 239897, 155074, 1911]
DEBUG SAMPLER: Probability sum: 1.0000001192092896
DEBUG SAMPLER: Special token probabilities: {0: 2.620046757675909e-12, 1: 1.4223519029243903e-09, 2: 4.1330537214889773e-07, 3: 6.223156213437664e-13, 4: 2.7631581467346678e-09}
DEBUG SAMPLER: Final top 5 probs: [0.3429509401321411, 0.11133985966444016, 0.08145798742771149, 0.07188642024993896, 0.07188642024993896]
DEBUG SAMPLER: Final top 5 indices: [175662, 151314, 32299, 52668, 91182]
DEBUG SAMPLER: Sampled token: 52668
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-6.000000, std=4.812500
DEBUG: Logits min/max: min=-29.000000, max=17.125000
DEBUG: Top 10 token IDs: [175662, 151314, 32299, 91182, 52668, 496, 19848, 239897, 155074, 1911]
DEBUG: Top 10 probabilities: [0.310546875, 0.10107421875, 0.07373046875, 0.0654296875, 0.0654296875, 0.0289306640625, 0.0255126953125, 0.0225830078125, 0.017578125, 0.0155029296875]
DEBUG: Selected next token: 52668
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [52668]
DEBUG: Input positions: [60]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.017578, std: 0.949219
DEBUG: Token embeddings min: -2.968750, max: 2.828125
DEBUG: Hidden states before model: mean=-0.017578, std=0.949219
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.212891, std=4.312500
DEBUG: Hidden states after model: min=-16.250000, max=13.125000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-0.142578, std=4.093750
DEBUG SAMPLER: Top 10 logits: [19.625, 18.75, 18.5, 18.0, 17.375, 16.75, 16.625, 16.625, 16.5, 16.375]
DEBUG SAMPLER: Top 10 indices: [245279, 69682, 2107, 183461, 245909, 191176, 239600, 81410, 97614, 32623]
DEBUG SAMPLER: Top 10 probs: [0.2620958089828491, 0.10925779491662979, 0.08509005606174469, 0.05160972476005554, 0.027624696493148804, 0.014786435291171074, 0.013048982247710228, 0.013048982247710228, 0.011515687219798565, 0.01016255747526884]
DEBUG SAMPLER: Top 10 token indices: [245279, 69682, 2107, 183461, 245909, 191176, 81410, 239600, 97614, 32623]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 2.4349611216223366e-10, 1: 3.468061171574277e-09, 2: 3.157705874556882e-09, 3: 2.653470221325449e-10, 4: 4.8221795480307694e-12}
DEBUG SAMPLER: Final top 5 probs: [0.3421080410480499, 0.14261186122894287, 0.11106622964143753, 0.06736507266759872, 0.03605792671442032]
DEBUG SAMPLER: Final top 5 indices: [245279, 69682, 2107, 183461, 245909]
DEBUG SAMPLER: Sampled token: 245279
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-0.142578, std=4.093750
DEBUG: Logits min/max: min=-18.375000, max=19.625000
DEBUG: Top 10 token IDs: [245279, 69682, 2107, 183461, 245909, 191176, 239600, 81410, 97614, 32623]
DEBUG: Top 10 probabilities: [0.26171875, 0.109375, 0.0849609375, 0.051513671875, 0.027587890625, 0.0147705078125, 0.0130615234375, 0.0130615234375, 0.01153564453125, 0.01019287109375]
DEBUG: Selected next token: 245279
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [245279]
DEBUG: Input positions: [61]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.004944, std: 1.031250
DEBUG: Token embeddings min: -3.000000, max: 2.812500
DEBUG: Hidden states before model: mean=-0.004944, std=1.031250
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.231445, std=5.375000
DEBUG: Hidden states after model: min=-21.500000, max=17.375000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=0.073730, std=5.281250
DEBUG SAMPLER: Top 10 logits: [23.875, 22.375, 22.375, 22.0, 21.625, 21.5, 21.375, 21.375, 21.25, 21.25]
DEBUG SAMPLER: Top 10 indices: [202850, 237126, 102940, 241039, 51361, 161631, 124317, 44673, 124519, 241554]
DEBUG SAMPLER: Top 10 probs: [0.2984607517719269, 0.06659559160470963, 0.06659559160470963, 0.04577044025063515, 0.031457528471946716, 0.027761172503232956, 0.02449914999306202, 0.02449914999306202, 0.02162042446434498, 0.02162042446434498]
DEBUG SAMPLER: Top 10 token indices: [202850, 102940, 237126, 241039, 51361, 161631, 44673, 124317, 124519, 241554]
DEBUG SAMPLER: Probability sum: 1.0000001192092896
DEBUG SAMPLER: Special token probabilities: {0: 3.540078674646452e-09, 1: 3.586132855514651e-14, 2: 2.542948931126294e-14, 3: 2.844533275592198e-09, 4: 5.522685715859954e-13}
DEBUG SAMPLER: Final top 5 probs: [0.3467167615890503, 0.07736296206712723, 0.07736296206712723, 0.053170740604400635, 0.036543674767017365]
DEBUG SAMPLER: Final top 5 indices: [202850, 102940, 237126, 241039, 51361]
DEBUG SAMPLER: Sampled token: 237172
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=0.073730, std=5.281250
DEBUG: Logits min/max: min=-26.000000, max=23.875000
DEBUG: Top 10 token IDs: [202850, 237126, 102940, 241039, 51361, 161631, 124317, 44673, 124519, 241554]
DEBUG: Top 10 probabilities: [0.298828125, 0.06640625, 0.06640625, 0.045654296875, 0.031494140625, 0.0277099609375, 0.0245361328125, 0.0245361328125, 0.0216064453125, 0.0216064453125]
DEBUG: Selected next token: 237172
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [237172]
DEBUG: Input positions: [62]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.024902, std: 0.957031
DEBUG: Token embeddings min: -2.812500, max: 3.421875
DEBUG: Hidden states before model: mean=-0.024902, std=0.957031
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.043457, std=5.125000
DEBUG: Hidden states after model: min=-53.250000, max=17.500000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-5.656250, std=4.843750
DEBUG SAMPLER: Top 10 logits: [17.25, 15.8125, 15.6875, 15.4375, 14.875, 14.8125, 14.5625, 14.5625, 13.9375, 13.6875]
DEBUG SAMPLER: Top 10 indices: [118191, 208082, 246066, 4370, 236909, 1109, 19340, 825, 88706, 10917]
DEBUG SAMPLER: Top 10 probs: [0.3633308410644531, 0.08629864454269409, 0.07615828514099121, 0.0593121312558651, 0.033795032650232315, 0.031747493892908096, 0.02472497522830963, 0.02472497522830963, 0.013234325684607029, 0.010306903161108494]
DEBUG SAMPLER: Top 10 token indices: [118191, 208082, 246066, 4370, 236909, 1109, 825, 19340, 88706, 10917]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 4.409074288395237e-13, 1: 9.42701732647988e-12, 2: 1.3969889778309152e-06, 3: 1.9565147841692915e-13, 4: 1.82147328331439e-08}
DEBUG SAMPLER: Final top 5 probs: [0.41171565651893616, 0.09779104590415955, 0.08630029112100601, 0.06721073389053345, 0.03829552233219147]
DEBUG SAMPLER: Final top 5 indices: [118191, 208082, 246066, 4370, 236909]
DEBUG SAMPLER: Sampled token: 109716
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-5.656250, std=4.843750
DEBUG: Logits min/max: min=-26.125000, max=17.250000
DEBUG: Top 10 token IDs: [118191, 208082, 246066, 4370, 236909, 1109, 19340, 825, 88706, 10917]
DEBUG: Top 10 probabilities: [0.36328125, 0.08642578125, 0.076171875, 0.059326171875, 0.03369140625, 0.03173828125, 0.0247802734375, 0.0247802734375, 0.01324462890625, 0.01031494140625]
DEBUG: Selected next token: 109716
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [109716]
DEBUG: Input positions: [63]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.021362, std: 0.937500
DEBUG: Token embeddings min: -2.937500, max: 3.156250
DEBUG: Hidden states before model: mean=-0.021362, std=0.937500
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.102051, std=5.312500
DEBUG: Hidden states after model: min=-30.000000, max=19.750000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.718750, std=5.218750
DEBUG SAMPLER: Top 10 logits: [20.75, 20.5, 19.875, 19.75, 19.125, 18.875, 18.875, 18.5, 18.5, 18.375]
DEBUG SAMPLER: Top 10 indices: [131489, 88744, 212610, 17703, 46113, 237057, 186735, 57766, 156933, 75203]
DEBUG SAMPLER: Top 10 probs: [0.18457373976707458, 0.14374618232250214, 0.07694178074598312, 0.0679008960723877, 0.03634472191333771, 0.028305301442742348, 0.028305301442742348, 0.019453929737210274, 0.019453929737210274, 0.017168032005429268]
DEBUG SAMPLER: Top 10 token indices: [131489, 88744, 212610, 17703, 46113, 186735, 237057, 57766, 156933, 75203]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 6.02842157703537e-14, 1: 8.82679762614913e-11, 2: 1.2072008530594758e-06, 3: 3.892241075883346e-14, 4: 9.055137195623786e-10}
DEBUG SAMPLER: Final top 5 probs: [0.2161184549331665, 0.16831323504447937, 0.09009157866239548, 0.079505555331707, 0.042556244879961014]
DEBUG SAMPLER: Final top 5 indices: [131489, 88744, 212610, 17703, 46113]
DEBUG SAMPLER: Sampled token: 212610
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.718750, std=5.218750
DEBUG: Logits min/max: min=-25.750000, max=20.750000
DEBUG: Top 10 token IDs: [131489, 88744, 212610, 17703, 46113, 237057, 186735, 57766, 156933, 75203]
DEBUG: Top 10 probabilities: [0.1845703125, 0.1435546875, 0.0771484375, 0.06787109375, 0.036376953125, 0.0283203125, 0.0283203125, 0.0194091796875, 0.0194091796875, 0.0172119140625]
DEBUG: Selected next token: 212610
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [212610]
DEBUG: Input positions: [64]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.013000, std: 0.917969
DEBUG: Token embeddings min: -3.000000, max: 2.640625
DEBUG: Hidden states before model: mean=0.013000, std=0.917969
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.045410, std=4.625000
DEBUG: Hidden states after model: min=-16.875000, max=14.875000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.687500, std=4.750000
DEBUG SAMPLER: Top 10 logits: [20.0, 19.875, 18.75, 18.625, 18.625, 17.75, 17.75, 17.375, 17.375, 17.0]
DEBUG SAMPLER: Top 10 indices: [236828, 55419, 13712, 225496, 19336, 44339, 20722, 237014, 54718, 4651]
DEBUG SAMPLER: Top 10 probs: [0.21702544391155243, 0.19152428209781647, 0.0621788315474987, 0.054872624576091766, 0.054872624576091766, 0.022874312475323677, 0.022874312475323677, 0.015721270814538002, 0.015721270814538002, 0.010805060155689716]
DEBUG SAMPLER: Top 10 token indices: [236828, 55419, 13712, 19336, 225496, 20722, 44339, 54718, 237014, 4651]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 2.8153042397338623e-11, 1: 6.495808890782939e-10, 2: 2.9843009031853274e-13, 3: 9.283887894562248e-12, 4: 3.044069510349523e-11}
DEBUG SAMPLER: Final top 5 probs: [0.251141220331192, 0.22163134813308716, 0.07195316255092621, 0.06349844485521317, 0.06349844485521317]
DEBUG SAMPLER: Final top 5 indices: [236828, 55419, 13712, 19336, 225496]
DEBUG SAMPLER: Sampled token: 55419
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.687500, std=4.750000
DEBUG: Logits min/max: min=-23.000000, max=20.000000
DEBUG: Top 10 token IDs: [236828, 55419, 13712, 225496, 19336, 44339, 20722, 237014, 54718, 4651]
DEBUG: Top 10 probabilities: [0.216796875, 0.19140625, 0.062255859375, 0.054931640625, 0.054931640625, 0.0228271484375, 0.0228271484375, 0.0157470703125, 0.0157470703125, 0.01080322265625]
DEBUG: Selected next token: 55419
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [55419]
DEBUG: Input positions: [65]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.020264, std: 0.925781
DEBUG: Token embeddings min: -3.171875, max: 2.562500
DEBUG: Hidden states before model: mean=0.020264, std=0.925781
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.089844, std=4.937500
DEBUG: Hidden states after model: min=-16.000000, max=28.625000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-4.687500, std=5.031250
DEBUG SAMPLER: Top 10 logits: [22.25, 20.0, 20.0, 19.875, 18.625, 18.5, 18.0, 17.75, 17.5, 17.125]
DEBUG SAMPLER: Top 10 indices: [1747, 236769, 862, 30632, 15291, 236772, 33679, 537, 236859, 568]
DEBUG SAMPLER: Top 10 probs: [0.6865400671958923, 0.07236079126596451, 0.07236079126596451, 0.06385817378759384, 0.01829567365348339, 0.016145875677466393, 0.009792967699468136, 0.007626771926879883, 0.0059397355653345585, 0.004082316532731056]
DEBUG SAMPLER: Top 10 token indices: [1747, 862, 236769, 30632, 15291, 236772, 33679, 537, 236859, 568]
DEBUG SAMPLER: Probability sum: 1.0000001192092896
DEBUG SAMPLER: Special token probabilities: {0: 4.653807382320151e-15, 1: 2.3236045032515307e-10, 2: 3.175516793962885e-11, 3: 1.8224572428292762e-15, 4: 6.097105459401675e-11}
DEBUG SAMPLER: Final top 5 probs: [0.7204586267471313, 0.07593578100204468, 0.07593578100204468, 0.06701309233903885, 0.019199572503566742]
DEBUG SAMPLER: Final top 5 indices: [1747, 862, 236769, 30632, 15291]
DEBUG SAMPLER: Sampled token: 1747
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-4.687500, std=5.031250
DEBUG: Logits min/max: min=-26.500000, max=22.250000
DEBUG: Top 10 token IDs: [1747, 236769, 862, 30632, 15291, 236772, 33679, 537, 236859, 568]
DEBUG: Top 10 probabilities: [0.6875, 0.072265625, 0.072265625, 0.06396484375, 0.018310546875, 0.01611328125, 0.009765625, 0.00762939453125, 0.005950927734375, 0.00408935546875]
DEBUG: Selected next token: 1747
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [1747]
DEBUG: Input positions: [66]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.041748, std: 0.984375
DEBUG: Token embeddings min: -3.250000, max: 2.984375
DEBUG: Hidden states before model: mean=-0.041748, std=0.984375
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.072754, std=4.468750
DEBUG: Hidden states after model: min=-18.875000, max=19.375000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-0.460938, std=4.312500
DEBUG SAMPLER: Top 10 logits: [18.5, 18.25, 18.0, 17.875, 17.25, 17.125, 17.125, 17.125, 17.0, 16.625]
DEBUG SAMPLER: Top 10 indices: [52961, 50971, 88668, 238183, 217081, 145092, 241721, 3005, 5592, 78190]
DEBUG SAMPLER: Top 10 probs: [0.09266706556081772, 0.07216918468475342, 0.05620541423559189, 0.04960110783576965, 0.026549559086561203, 0.023429902270436287, 0.023429902270436287, 0.023429902270436287, 0.02067681774497032, 0.014210955239832401]
DEBUG SAMPLER: Top 10 token indices: [52961, 50971, 88668, 238183, 217081, 3005, 145092, 241721, 5592, 78190]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 5.7348913812260705e-11, 1: 8.645367977067053e-09, 2: 3.9498246340308185e-10, 3: 2.8389773171211274e-11, 4: 1.9736093998228704e-13}
DEBUG SAMPLER: Final top 5 probs: [0.1421853005886078, 0.11073402315378189, 0.08623974025249481, 0.07610630989074707, 0.04073676839470863]
DEBUG SAMPLER: Final top 5 indices: [52961, 50971, 88668, 238183, 217081]
DEBUG SAMPLER: Sampled token: 54093
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-0.460938, std=4.312500
DEBUG: Logits min/max: min=-19.000000, max=18.500000
DEBUG: Top 10 token IDs: [52961, 50971, 88668, 238183, 217081, 145092, 241721, 3005, 5592, 78190]
DEBUG: Top 10 probabilities: [0.0927734375, 0.072265625, 0.05615234375, 0.049560546875, 0.0264892578125, 0.0234375, 0.0234375, 0.0234375, 0.0206298828125, 0.01422119140625]
DEBUG: Selected next token: 54093
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [54093]
DEBUG: Input positions: [67]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.027100, std: 0.964844
DEBUG: Token embeddings min: -2.750000, max: 3.140625
DEBUG: Hidden states before model: mean=-0.027100, std=0.964844
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.073242, std=4.406250
DEBUG: Hidden states after model: min=-18.625000, max=14.375000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-0.531250, std=4.250000
DEBUG SAMPLER: Top 10 logits: [19.75, 19.625, 19.0, 18.625, 18.0, 17.5, 17.125, 17.125, 17.125, 17.125]
DEBUG SAMPLER: Top 10 indices: [232396, 51450, 49459, 237280, 45249, 89849, 236855, 237402, 13214, 10178]
DEBUG SAMPLER: Top 10 probs: [0.19703719019889832, 0.17388471961021423, 0.0930737853050232, 0.06396861374378204, 0.03423992916941643, 0.020767567679286003, 0.014273326843976974, 0.014273326843976974, 0.014273326843976974, 0.014273326843976974]
DEBUG SAMPLER: Top 10 token indices: [232396, 51450, 49459, 237280, 45249, 89849, 10178, 13214, 236855, 237402]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 1.841026463222928e-11, 1: 1.6433608759826995e-13, 2: 5.519449342017424e-09, 3: 1.016711367873846e-11, 4: 7.396066398213108e-11}
DEBUG SAMPLER: Final top 5 probs: [0.24474592506885529, 0.21598753333091736, 0.1156097948551178, 0.0794573724269867, 0.04253046587109566]
DEBUG SAMPLER: Final top 5 indices: [232396, 51450, 49459, 237280, 45249]
DEBUG SAMPLER: Sampled token: 49459
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-0.531250, std=4.250000
DEBUG: Logits min/max: min=-19.750000, max=19.750000
DEBUG: Top 10 token IDs: [232396, 51450, 49459, 237280, 45249, 89849, 236855, 237402, 13214, 10178]
DEBUG: Top 10 probabilities: [0.197265625, 0.173828125, 0.09326171875, 0.06396484375, 0.0341796875, 0.020751953125, 0.0142822265625, 0.0142822265625, 0.0142822265625, 0.0142822265625]
DEBUG: Selected next token: 49459
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [49459]
DEBUG: Input positions: [68]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.007629, std: 0.945312
DEBUG: Token embeddings min: -2.671875, max: 3.375000
DEBUG: Hidden states before model: mean=-0.007629, std=0.945312
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.072266, std=4.500000
DEBUG: Hidden states after model: min=-14.000000, max=15.437500
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.500000, std=4.312500
DEBUG SAMPLER: Top 10 logits: [18.625, 17.125, 16.625, 16.375, 16.125, 16.0, 15.875, 15.75, 15.6875, 15.375]
DEBUG SAMPLER: Top 10 indices: [76197, 18310, 242118, 26503, 58759, 177281, 233080, 25926, 60378, 24281]
DEBUG SAMPLER: Top 10 probs: [0.3441246747970581, 0.07678459584712982, 0.046572208404541016, 0.03627047315239906, 0.028247473761439323, 0.02492830902338028, 0.02199915423989296, 0.019414186477661133, 0.018237939104437828, 0.013343160971999168]
DEBUG SAMPLER: Top 10 token indices: [76197, 18310, 242118, 26503, 58759, 177281, 233080, 25926, 60378, 24281]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 2.583602429218068e-11, 1: 3.456821773273333e-10, 2: 5.211170517327446e-08, 3: 1.3403553440760607e-11, 4: 2.3124428594201163e-08}
DEBUG SAMPLER: Final top 5 probs: [0.42347854375839233, 0.09449083358049393, 0.057311587035655975, 0.04463430866599083, 0.03476123511791229]
DEBUG SAMPLER: Final top 5 indices: [76197, 18310, 242118, 26503, 58759]
DEBUG SAMPLER: Sampled token: 76197
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.500000, std=4.312500
DEBUG: Logits min/max: min=-21.250000, max=18.625000
DEBUG: Top 10 token IDs: [76197, 18310, 242118, 26503, 58759, 177281, 233080, 25926, 60378, 24281]
DEBUG: Top 10 probabilities: [0.34375, 0.07666015625, 0.046630859375, 0.036376953125, 0.0281982421875, 0.02490234375, 0.02197265625, 0.0194091796875, 0.0181884765625, 0.01336669921875]
DEBUG: Selected next token: 76197
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [76197]
DEBUG: Input positions: [69]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.005981, std: 0.937500
DEBUG: Token embeddings min: -3.468750, max: 3.656250
DEBUG: Hidden states before model: mean=0.005981, std=0.937500
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.018677, std=4.781250
DEBUG: Hidden states after model: min=-15.812500, max=16.500000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.781250, std=4.593750
DEBUG SAMPLER: Top 10 logits: [19.0, 18.5, 17.125, 16.75, 16.625, 16.5, 16.125, 16.0, 16.0, 15.625]
DEBUG SAMPLER: Top 10 indices: [236827, 17799, 6906, 16176, 46392, 63895, 111106, 7563, 221763, 5306]
DEBUG SAMPLER: Top 10 probs: [0.29324203729629517, 0.17786027491092682, 0.04497012123465538, 0.03090748004615307, 0.027275756001472473, 0.024070771411061287, 0.016543583944439888, 0.014599660411477089, 0.014599660411477089, 0.010034190490841866]
DEBUG SAMPLER: Top 10 token indices: [236827, 17799, 6906, 16176, 46392, 63895, 111106, 7563, 221763, 5306]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 1.6173060668478456e-09, 1: 1.2158324694255818e-11, 2: 1.2868778265229253e-10, 3: 5.503278388552246e-10, 4: 8.691757868994898e-10}
DEBUG SAMPLER: Final top 5 probs: [0.35415616631507874, 0.21480657160282135, 0.0543116070330143, 0.037327781319618225, 0.032941654324531555]
DEBUG SAMPLER: Final top 5 indices: [236827, 17799, 6906, 16176, 46392]
DEBUG SAMPLER: Sampled token: 6906
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.781250, std=4.593750
DEBUG: Logits min/max: min=-22.875000, max=19.000000
DEBUG: Top 10 token IDs: [236827, 17799, 6906, 16176, 46392, 63895, 111106, 7563, 221763, 5306]
DEBUG: Top 10 probabilities: [0.29296875, 0.177734375, 0.044921875, 0.0308837890625, 0.0272216796875, 0.0240478515625, 0.0166015625, 0.01458740234375, 0.01458740234375, 0.010009765625]
DEBUG: Selected next token: 6906
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [6906]
DEBUG: Input positions: [70]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.040039, std: 1.046875
DEBUG: Token embeddings min: -10.687500, max: 4.593750
DEBUG: Hidden states before model: mean=-0.040039, std=1.046875
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.120605, std=5.031250
DEBUG: Hidden states after model: min=-35.000000, max=30.125000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.484375, std=5.093750
DEBUG SAMPLER: Top 10 logits: [19.75, 19.75, 19.625, 17.75, 17.125, 16.875, 16.75, 16.625, 16.625, 16.625]
DEBUG SAMPLER: Top 10 indices: [825, 3800, 210431, 241307, 703, 23262, 2335, 90101, 4151, 4088]
DEBUG SAMPLER: Top 10 probs: [0.23167842626571655, 0.23167842626571655, 0.20445549488067627, 0.03135426715016365, 0.0167827308177948, 0.013070403598248959, 0.01153458934277296, 0.010179240256547928, 0.010179240256547928, 0.010179240256547928]
DEBUG SAMPLER: Top 10 token indices: [825, 3800, 210431, 241307, 703, 23262, 2335, 4088, 4151, 90101]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 1.301960790697429e-15, 1: 9.388878652316635e-08, 2: 5.519436641066022e-08, 3: 9.52534924241323e-16, 4: 2.72495323831376e-13}
DEBUG SAMPLER: Final top 5 probs: [0.25384458899497986, 0.25384458899497986, 0.22401706874370575, 0.034354131668806076, 0.01838844269514084]
DEBUG SAMPLER: Final top 5 indices: [825, 3800, 210431, 241307, 703]
DEBUG SAMPLER: Sampled token: 7772
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.484375, std=5.093750
DEBUG: Logits min/max: min=-27.000000, max=19.750000
DEBUG: Top 10 token IDs: [825, 3800, 210431, 241307, 703, 23262, 2335, 90101, 4151, 4088]
DEBUG: Top 10 probabilities: [0.2314453125, 0.2314453125, 0.2041015625, 0.03125, 0.0167236328125, 0.0130615234375, 0.01153564453125, 0.01019287109375, 0.01019287109375, 0.01019287109375]
DEBUG: Selected next token: 7772
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [7772]
DEBUG: Input positions: [71]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.010254, std: 0.957031
DEBUG: Token embeddings min: -3.109375, max: 3.125000
DEBUG: Hidden states before model: mean=0.010254, std=0.957031
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.019897, std=5.156250
DEBUG: Hidden states after model: min=-18.125000, max=21.000000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-5.218750, std=5.500000
DEBUG SAMPLER: Top 10 logits: [18.25, 18.0, 17.875, 17.25, 17.25, 17.0, 16.5, 16.375, 16.25, 16.125]
DEBUG SAMPLER: Top 10 indices: [187010, 22463, 45076, 236772, 76818, 10186, 32402, 11973, 185399, 51476]
DEBUG SAMPLER: Top 10 probs: [0.14620545506477356, 0.11386492103338242, 0.10048544406890869, 0.053785987198352814, 0.053785987198352814, 0.04188856482505798, 0.025406695902347565, 0.02242133393883705, 0.019786756485700607, 0.01746175065636635]
DEBUG SAMPLER: Top 10 token indices: [187010, 22463, 45076, 76818, 236772, 10186, 32402, 11973, 185399, 51476]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 6.335724389145401e-17, 1: 4.43500488070403e-12, 2: 1.2887184652754513e-09, 3: 8.135230665463891e-17, 4: 9.388904584350932e-12}
DEBUG SAMPLER: Final top 5 probs: [0.1694413274526596, 0.13196103274822235, 0.11645520478487015, 0.06233398616313934, 0.06233398616313934]
DEBUG SAMPLER: Final top 5 indices: [187010, 22463, 45076, 76818, 236772]
DEBUG SAMPLER: Sampled token: 187010
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-5.218750, std=5.500000
DEBUG: Logits min/max: min=-28.375000, max=18.250000
DEBUG: Top 10 token IDs: [187010, 22463, 45076, 236772, 76818, 10186, 32402, 11973, 185399, 51476]
DEBUG: Top 10 probabilities: [0.146484375, 0.11376953125, 0.1005859375, 0.0537109375, 0.0537109375, 0.0419921875, 0.025390625, 0.0224609375, 0.019775390625, 0.0174560546875]
DEBUG: Selected next token: 187010
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [187010]
DEBUG: Input positions: [72]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.001907, std: 0.949219
DEBUG: Token embeddings min: -2.828125, max: 3.171875
DEBUG: Hidden states before model: mean=0.001907, std=0.949219
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.139648, std=4.812500
DEBUG: Hidden states after model: min=-20.875000, max=14.687500
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-4.625000, std=4.875000
DEBUG SAMPLER: Top 10 logits: [16.375, 16.375, 16.375, 16.25, 16.0, 16.0, 15.875, 15.8125, 15.5625, 15.5]
DEBUG SAMPLER: Top 10 indices: [83130, 24169, 204006, 4758, 102698, 17719, 1552, 861, 2455, 240586]
DEBUG SAMPLER: Top 10 probs: [0.06478861719369888, 0.06478861719369888, 0.06478861719369888, 0.05717575177550316, 0.044528521597385406, 0.044528521597385406, 0.03929628059267998, 0.03691543638706207, 0.028749773278832436, 0.02700791135430336]
DEBUG SAMPLER: Top 10 token indices: [24169, 83130, 204006, 4758, 17719, 102698, 1552, 861, 2455, 240586]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 7.862195052985022e-14, 1: 1.920921910425477e-08, 2: 0.006828669458627701, 3: 5.0762152592259344e-14, 4: 2.1584615605280133e-12}
DEBUG SAMPLER: High probability tokens in first 100: {2: 0.006828669458627701}
DEBUG SAMPLER: Final top 5 probs: [0.08205125480890274, 0.08205125480890274, 0.08205125480890274, 0.0724099799990654, 0.05639294907450676]
DEBUG SAMPLER: Final top 5 indices: [24169, 83130, 204006, 4758, 17719]
DEBUG SAMPLER: Sampled token: 8953
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-4.625000, std=4.875000
DEBUG: Logits min/max: min=-26.625000, max=16.375000
DEBUG: Top 10 token IDs: [83130, 24169, 204006, 4758, 102698, 17719, 1552, 861, 2455, 240586]
DEBUG: Top 10 probabilities: [0.06494140625, 0.06494140625, 0.06494140625, 0.05712890625, 0.04443359375, 0.04443359375, 0.039306640625, 0.036865234375, 0.02880859375, 0.0269775390625]
DEBUG: Selected next token: 8953
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [8953]
DEBUG: Input positions: [73]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.041504, std: 0.957031
DEBUG: Token embeddings min: -3.015625, max: 4.156250
DEBUG: Hidden states before model: mean=-0.041504, std=0.957031
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.127930, std=5.343750
DEBUG: Hidden states after model: min=-19.250000, max=19.125000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.632812, std=5.375000
DEBUG SAMPLER: Top 10 logits: [22.75, 21.375, 20.875, 20.75, 20.375, 20.25, 19.75, 19.75, 19.625, 19.625]
DEBUG SAMPLER: Top 10 indices: [95270, 118610, 124413, 75402, 6180, 9083, 38488, 139295, 14689, 198574]
DEBUG SAMPLER: Top 10 probs: [0.3459978401660919, 0.08748195320367813, 0.053060486912727356, 0.04682571440935135, 0.032182808965444565, 0.02840123139321804, 0.017226217314600945, 0.017226217314600945, 0.015202083624899387, 0.015202083624899387]
DEBUG SAMPLER: Top 10 token indices: [95270, 118610, 124413, 75402, 6180, 9083, 38488, 139295, 14689, 198574]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 5.9891747289408156e-15, 1: 2.087282777174515e-11, 2: 7.50080813433085e-12, 3: 3.0115462346238598e-15, 4: 6.7196669540812385e-16}
DEBUG SAMPLER: Final top 5 probs: [0.4047888219356537, 0.10234663635492325, 0.062076374888420105, 0.0547822080552578, 0.037651222199201584]
DEBUG SAMPLER: Final top 5 indices: [95270, 118610, 124413, 75402, 6180]
DEBUG SAMPLER: Sampled token: 95270
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.632812, std=5.375000
DEBUG: Logits min/max: min=-23.750000, max=22.750000
DEBUG: Top 10 token IDs: [95270, 118610, 124413, 75402, 6180, 9083, 38488, 139295, 14689, 198574]
DEBUG: Top 10 probabilities: [0.345703125, 0.08740234375, 0.052978515625, 0.046875, 0.0322265625, 0.0284423828125, 0.0172119140625, 0.0172119140625, 0.01519775390625, 0.01519775390625]
DEBUG: Selected next token: 95270
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [95270]
DEBUG: Input positions: [74]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.039307, std: 0.964844
DEBUG: Token embeddings min: -3.000000, max: 2.609375
DEBUG: Hidden states before model: mean=-0.039307, std=0.964844
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.062988, std=5.093750
DEBUG: Hidden states after model: min=-18.625000, max=17.000000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=0.074219, std=4.937500
DEBUG SAMPLER: Top 10 logits: [22.625, 22.625, 22.25, 22.0, 21.375, 21.125, 21.125, 20.75, 20.75, 20.5]
DEBUG SAMPLER: Top 10 indices: [199189, 21210, 25838, 238783, 238014, 74754, 120386, 34774, 7741, 22899]
DEBUG SAMPLER: Top 10 probs: [0.146210715174675, 0.146210715174675, 0.1004890650510788, 0.07826096564531326, 0.04189007356762886, 0.0326240211725235, 0.0326240211725235, 0.02242214046418667, 0.02242214046418667, 0.01746238023042679]
DEBUG SAMPLER: Top 10 token indices: [21210, 199189, 25838, 238783, 238014, 74754, 120386, 7741, 34774, 22899]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 5.104843197462072e-12, 1: 2.1127162241896258e-08, 2: 8.53614512408285e-09, 3: 6.453126227823347e-12, 4: 1.0179636547036353e-06}
DEBUG SAMPLER: Final top 5 probs: [0.17009997367858887, 0.17009997367858887, 0.11690789461135864, 0.09104796499013901, 0.04873446002602577]
DEBUG SAMPLER: Final top 5 indices: [21210, 199189, 25838, 238783, 238014]
DEBUG SAMPLER: Sampled token: 74754
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=0.074219, std=4.937500
DEBUG: Logits min/max: min=-22.625000, max=22.625000
DEBUG: Top 10 token IDs: [199189, 21210, 25838, 238783, 238014, 74754, 120386, 34774, 7741, 22899]
DEBUG: Top 10 probabilities: [0.146484375, 0.146484375, 0.1005859375, 0.078125, 0.0419921875, 0.03271484375, 0.03271484375, 0.0224609375, 0.0224609375, 0.0174560546875]
DEBUG: Selected next token: 74754
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [74754]
DEBUG: Input positions: [75]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.015503, std: 1.000000
DEBUG: Token embeddings min: -3.171875, max: 3.265625
DEBUG: Hidden states before model: mean=-0.015503, std=1.000000
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.061768, std=4.656250
DEBUG: Hidden states after model: min=-21.125000, max=16.375000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=0.023438, std=4.500000
DEBUG SAMPLER: Top 10 logits: [19.25, 19.125, 18.75, 18.5, 18.5, 18.25, 18.0, 17.875, 17.875, 17.875]
DEBUG SAMPLER: Top 10 indices: [210764, 214964, 174134, 239715, 67527, 90045, 187495, 249376, 71558, 1350]
DEBUG SAMPLER: Top 10 probs: [0.07456882297992706, 0.06580675393342972, 0.045228276401758194, 0.0352238193154335, 0.0352238193154335, 0.027432339265942574, 0.021364325657486916, 0.01885394938290119, 0.01885394938290119, 0.01885394938290119]
DEBUG SAMPLER: Top 10 token indices: [210764, 214964, 174134, 67527, 239715, 90045, 187495, 1350, 71558, 249376]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 4.6342898407658595e-09, 1: 1.3249576436002997e-10, 2: 4.0433064896205906e-07, 3: 2.1217325674172116e-09, 4: 2.2490957013454604e-11}
DEBUG SAMPLER: Final top 5 probs: [0.10776135325431824, 0.0950990542769432, 0.06536056101322174, 0.05090286210179329, 0.05090286210179329]
DEBUG SAMPLER: Final top 5 indices: [210764, 214964, 174134, 67527, 239715]
DEBUG SAMPLER: Sampled token: 210764
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=0.023438, std=4.500000
DEBUG: Logits min/max: min=-20.875000, max=19.250000
DEBUG: Top 10 token IDs: [210764, 214964, 174134, 239715, 67527, 90045, 187495, 249376, 71558, 1350]
DEBUG: Top 10 probabilities: [0.07470703125, 0.06591796875, 0.045166015625, 0.03515625, 0.03515625, 0.0274658203125, 0.0213623046875, 0.018798828125, 0.018798828125, 0.018798828125]
DEBUG: Selected next token: 210764
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [210764]
DEBUG: Input positions: [76]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.046631, std: 0.941406
DEBUG: Token embeddings min: -3.125000, max: 2.718750
DEBUG: Hidden states before model: mean=0.046631, std=0.941406
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.200195, std=5.125000
DEBUG: Hidden states after model: min=-15.875000, max=17.625000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.218750, std=5.093750
DEBUG SAMPLER: Top 10 logits: [22.875, 22.0, 20.25, 19.25, 19.25, 19.25, 19.125, 19.125, 19.0, 18.75]
DEBUG SAMPLER: Top 10 indices: [1710, 236755, 8057, 142928, 138058, 11512, 76541, 113899, 244387, 181428]
DEBUG SAMPLER: Top 10 probs: [0.47430887818336487, 0.19772136211395264, 0.03435882180929184, 0.012639903463423252, 0.012639903463423252, 0.012639903463423252, 0.011154675856232643, 0.011154675856232643, 0.00984396692365408, 0.0076664891093969345]
DEBUG SAMPLER: Top 10 token indices: [1710, 236755, 8057, 11512, 138058, 142928, 76541, 113899, 244387, 181428]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 7.712788944297909e-15, 1: 4.993211955017784e-10, 2: 2.4964490474843615e-09, 3: 7.712788944297909e-15, 4: 6.524134010810201e-11}
DEBUG SAMPLER: Final top 5 probs: [0.5278245210647583, 0.2200300097465515, 0.03823548182845116, 0.014066047966480255, 0.014066047966480255]
DEBUG SAMPLER: Final top 5 indices: [1710, 236755, 8057, 11512, 138058]
DEBUG SAMPLER: Sampled token: 236755
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.218750, std=5.093750
DEBUG: Logits min/max: min=-25.875000, max=22.875000
DEBUG: Top 10 token IDs: [1710, 236755, 8057, 142928, 138058, 11512, 76541, 113899, 244387, 181428]
DEBUG: Top 10 probabilities: [0.474609375, 0.197265625, 0.034423828125, 0.01263427734375, 0.01263427734375, 0.01263427734375, 0.01116943359375, 0.01116943359375, 0.00982666015625, 0.007659912109375]
DEBUG: Selected next token: 236755
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [236755]
DEBUG: Input positions: [77]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.008850, std: 0.996094
DEBUG: Token embeddings min: -3.765625, max: 4.312500
DEBUG: Hidden states before model: mean=0.008850, std=0.996094
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.009033, std=4.625000
DEBUG: Hidden states after model: min=-15.937500, max=16.875000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=0.765625, std=4.406250
DEBUG SAMPLER: Top 10 logits: [20.875, 20.5, 20.375, 20.125, 20.125, 20.0, 19.25, 19.0, 18.875, 18.875]
DEBUG SAMPLER: Top 10 indices: [127188, 90456, 233501, 236729, 142440, 188764, 36842, 84764, 195425, 237110]
DEBUG SAMPLER: Top 10 probs: [0.12562307715415955, 0.08633939921855927, 0.07619424909353256, 0.05934014543890953, 0.05934014543890953, 0.05236748978495598, 0.02473665028810501, 0.01926492527127266, 0.017001235857605934, 0.017001235857605934]
DEBUG SAMPLER: Top 10 token indices: [127188, 90456, 233501, 142440, 236729, 188764, 36842, 84764, 195425, 237110]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 2.5194432651254317e-10, 1: 6.395082380317163e-11, 2: 2.344147098654048e-09, 3: 7.638976950996579e-11, 4: 3.3578355651542324e-09}
DEBUG SAMPLER: Final top 5 probs: [0.161921888589859, 0.1112871840596199, 0.09821058809757233, 0.0764864906668663, 0.0764864906668663]
DEBUG SAMPLER: Final top 5 indices: [127188, 90456, 233501, 142440, 236729]
DEBUG SAMPLER: Sampled token: 90456
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=0.765625, std=4.406250
DEBUG: Logits min/max: min=-20.250000, max=20.875000
DEBUG: Top 10 token IDs: [127188, 90456, 233501, 236729, 142440, 188764, 36842, 84764, 195425, 237110]
DEBUG: Top 10 probabilities: [0.1259765625, 0.08642578125, 0.076171875, 0.059326171875, 0.059326171875, 0.05224609375, 0.0247802734375, 0.019287109375, 0.0169677734375, 0.0169677734375]
DEBUG: Selected next token: 90456
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [90456]
DEBUG: Input positions: [78]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.025879, std: 0.921875
DEBUG: Token embeddings min: -2.937500, max: 2.937500
DEBUG: Hidden states before model: mean=0.025879, std=0.921875
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.002457, std=5.312500
DEBUG: Hidden states after model: min=-17.500000, max=23.375000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.234375, std=5.312500
DEBUG SAMPLER: Top 10 logits: [21.375, 21.25, 20.375, 20.25, 19.75, 19.625, 19.5, 19.5, 19.0, 18.875]
DEBUG SAMPLER: Top 10 indices: [239307, 235123, 17923, 4074, 211065, 236954, 210293, 68707, 35747, 173773]
DEBUG SAMPLER: Top 10 probs: [0.20515242218971252, 0.18104638159275055, 0.07547136396169662, 0.06660324335098267, 0.040396906435489655, 0.035650141537189484, 0.03146114572882652, 0.03146114572882652, 0.019082147628068924, 0.01683993637561798]
DEBUG SAMPLER: Top 10 token indices: [239307, 235123, 17923, 4074, 211065, 236954, 68707, 210293, 35747, 173773]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 3.780190306016274e-15, 1: 3.768827028238775e-09, 2: 2.3086863976118366e-08, 3: 1.3063988198245679e-15, 4: 6.769334398581961e-11}
DEBUG SAMPLER: Final top 5 probs: [0.2306680828332901, 0.2035638689994812, 0.08485805243253708, 0.07488696277141571, 0.045421235263347626]
DEBUG SAMPLER: Final top 5 indices: [239307, 235123, 17923, 4074, 211065]
DEBUG SAMPLER: Sampled token: 235123
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.234375, std=5.312500
DEBUG: Logits min/max: min=-26.375000, max=21.375000
DEBUG: Top 10 token IDs: [239307, 235123, 17923, 4074, 211065, 236954, 210293, 68707, 35747, 173773]
DEBUG: Top 10 probabilities: [0.205078125, 0.1806640625, 0.07568359375, 0.06640625, 0.040283203125, 0.03564453125, 0.031494140625, 0.031494140625, 0.01904296875, 0.016845703125]
DEBUG: Selected next token: 235123
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [235123]
DEBUG: Input positions: [79]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.008850, std: 0.964844
DEBUG: Token embeddings min: -3.250000, max: 3.921875
DEBUG: Hidden states before model: mean=0.008850, std=0.964844
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.027466, std=4.937500
DEBUG: Hidden states after model: min=-32.500000, max=26.125000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.468750, std=5.093750
DEBUG SAMPLER: Top 10 logits: [19.125, 18.75, 17.625, 17.5, 17.0, 16.5, 16.125, 16.0, 15.75, 15.625]
DEBUG SAMPLER: Top 10 indices: [16419, 65442, 72450, 237988, 22036, 87907, 27279, 79926, 244658, 5985]
DEBUG SAMPLER: Top 10 probs: [0.2736157476902008, 0.188053160905838, 0.06105192378163338, 0.05387812852859497, 0.03267873823642731, 0.01982065849006176, 0.0136225251480937, 0.012021836824715137, 0.009362615644931793, 0.008262479677796364]
DEBUG SAMPLER: Top 10 token indices: [16419, 65442, 72450, 237988, 22036, 87907, 27279, 79926, 244658, 5985]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 1.6368044555709602e-15, 1: 7.058837542261998e-11, 2: 1.889043088620923e-10, 3: 8.761182587180001e-16, 4: 9.513014155712249e-14}
DEBUG SAMPLER: Final top 5 probs: [0.3235006630420685, 0.2223385125398636, 0.07218275219202042, 0.06370104849338531, 0.03863663971424103]
DEBUG SAMPLER: Final top 5 indices: [16419, 65442, 72450, 237988, 22036]
DEBUG SAMPLER: Sampled token: 16419
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.468750, std=5.093750
DEBUG: Logits min/max: min=-26.250000, max=19.125000
DEBUG: Top 10 token IDs: [16419, 65442, 72450, 237988, 22036, 87907, 27279, 79926, 244658, 5985]
DEBUG: Top 10 probabilities: [0.2734375, 0.1884765625, 0.06103515625, 0.053955078125, 0.03271484375, 0.019775390625, 0.01361083984375, 0.01202392578125, 0.00933837890625, 0.00823974609375]
DEBUG: Selected next token: 16419
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [16419]
DEBUG: Input positions: [80]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.011719, std: 0.968750
DEBUG: Token embeddings min: -3.625000, max: 3.093750
DEBUG: Hidden states before model: mean=-0.011719, std=0.968750
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.050049, std=5.312500
DEBUG: Hidden states after model: min=-23.375000, max=16.500000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.921875, std=5.062500
DEBUG SAMPLER: Top 10 logits: [18.75, 18.5, 18.375, 18.25, 18.25, 18.0, 17.75, 17.75, 17.5, 17.375]
DEBUG SAMPLER: Top 10 indices: [1843, 21860, 44496, 112725, 54524, 22341, 8353, 1102, 48268, 182737]
DEBUG SAMPLER: Top 10 probs: [0.10898057371377945, 0.08487416058778763, 0.07490118592977524, 0.06610006093978882, 0.06610006093978882, 0.051478780806064606, 0.04009171575307846, 0.04009171575307846, 0.031223459169268608, 0.027554605156183243]
DEBUG SAMPLER: Top 10 token indices: [1843, 21860, 44496, 54524, 112725, 22341, 1102, 8353, 48268, 182737]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 2.005082948633441e-12, 1: 4.5566839190769315e-08, 2: 8.101267595130024e-13, 3: 7.149344592900786e-13, 4: 1.883601130972523e-12}
DEBUG SAMPLER: Final top 5 probs: [0.12507978081703186, 0.09741222858428955, 0.08596599847078323, 0.07586471736431122, 0.07586471736431122]
DEBUG SAMPLER: Final top 5 indices: [1843, 21860, 44496, 54524, 112725]
DEBUG SAMPLER: Sampled token: 54524
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.921875, std=5.062500
DEBUG: Logits min/max: min=-26.250000, max=18.750000
DEBUG: Top 10 token IDs: [1843, 21860, 44496, 112725, 54524, 22341, 8353, 1102, 48268, 182737]
DEBUG: Top 10 probabilities: [0.10888671875, 0.0849609375, 0.07470703125, 0.06591796875, 0.06591796875, 0.051513671875, 0.0400390625, 0.0400390625, 0.03125, 0.027587890625]
DEBUG: Selected next token: 54524
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [54524]
DEBUG: Input positions: [81]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.011047, std: 0.968750
DEBUG: Token embeddings min: -3.140625, max: 3.312500
DEBUG: Hidden states before model: mean=-0.011047, std=0.968750
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.087891, std=4.531250
DEBUG: Hidden states after model: min=-13.812500, max=25.500000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.578125, std=4.500000
DEBUG SAMPLER: Top 10 logits: [17.75, 17.375, 16.625, 16.625, 16.375, 15.875, 15.8125, 15.6875, 15.625, 15.5625]
DEBUG SAMPLER: Top 10 indices: [236854, 236864, 3121, 2033, 238734, 21374, 61137, 99574, 236883, 4928]
DEBUG SAMPLER: Top 10 probs: [0.13500595092773438, 0.09278814494609833, 0.043830014765262604, 0.043830014765262604, 0.03413484990596771, 0.020703835412859917, 0.019449453800916672, 0.0171640794724226, 0.016124162822961807, 0.015147247351706028]
DEBUG SAMPLER: Top 10 token indices: [236854, 236864, 2033, 3121, 238734, 21374, 61137, 99574, 236883, 4928]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 1.210562032560869e-12, 1: 5.459689145226321e-09, 2: 3.728793315782575e-12, 3: 3.9301193050823857e-13, 4: 4.81815920494455e-09}
DEBUG SAMPLER: Final top 5 probs: [0.18161985278129578, 0.1248253807425499, 0.05896333232522011, 0.05896333232522011, 0.04592068865895271]
DEBUG SAMPLER: Final top 5 indices: [236854, 236864, 2033, 3121, 238734]
DEBUG SAMPLER: Sampled token: 236864
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.578125, std=4.500000
DEBUG: Logits min/max: min=-22.250000, max=17.750000
DEBUG: Top 10 token IDs: [236854, 236864, 3121, 2033, 238734, 21374, 61137, 99574, 236883, 4928]
DEBUG: Top 10 probabilities: [0.134765625, 0.0927734375, 0.0439453125, 0.0439453125, 0.0341796875, 0.020751953125, 0.0194091796875, 0.0172119140625, 0.01611328125, 0.01513671875]
DEBUG: Selected next token: 236864
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [236864]
DEBUG: Input positions: [82]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.043213, std: 0.968750
DEBUG: Token embeddings min: -4.156250, max: 2.921875
DEBUG: Hidden states before model: mean=0.043213, std=0.968750
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.390625, std=4.625000
DEBUG: Hidden states after model: min=-14.250000, max=16.625000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=0.382812, std=4.468750
DEBUG SAMPLER: Top 10 logits: [20.75, 19.875, 19.875, 19.5, 19.125, 19.125, 19.125, 18.75, 18.75, 18.625]
DEBUG SAMPLER: Top 10 indices: [36571, 127388, 75768, 63999, 68908, 119940, 238812, 153384, 146524, 51065]
DEBUG SAMPLER: Top 10 probs: [0.14982590079307556, 0.06245672330260277, 0.06245672330260277, 0.04292583838105202, 0.029502466320991516, 0.029502466320991516, 0.029502466320991516, 0.02027672901749611, 0.02027672901749611, 0.01789415255188942]
DEBUG SAMPLER: Top 10 token indices: [36571, 75768, 127388, 63999, 68908, 119940, 238812, 146524, 153384, 51065]
DEBUG SAMPLER: Probability sum: 1.0000004768371582
DEBUG SAMPLER: Special token probabilities: {0: 1.3860940390256205e-12, 1: 4.1793454114147366e-11, 2: 3.300175965836871e-10, 3: 1.2620530705567679e-12, 4: 3.053075303682817e-08}
DEBUG SAMPLER: Final top 5 probs: [0.1939748227596283, 0.08086072653532028, 0.08086072653532028, 0.05557471513748169, 0.0381959043443203]
DEBUG SAMPLER: Final top 5 indices: [36571, 75768, 127388, 63999, 68908]
DEBUG SAMPLER: Sampled token: 90154
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=0.382812, std=4.468750
DEBUG: Logits min/max: min=-18.625000, max=20.750000
DEBUG: Top 10 token IDs: [36571, 127388, 75768, 63999, 68908, 119940, 238812, 153384, 146524, 51065]
DEBUG: Top 10 probabilities: [0.1494140625, 0.0625, 0.0625, 0.04296875, 0.029541015625, 0.029541015625, 0.029541015625, 0.020263671875, 0.020263671875, 0.0179443359375]
DEBUG: Selected next token: 90154
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [90154]
DEBUG: Input positions: [83]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.026123, std: 0.941406
DEBUG: Token embeddings min: -4.031250, max: 3.437500
DEBUG: Hidden states before model: mean=-0.026123, std=0.941406
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.015991, std=5.187500
DEBUG: Hidden states after model: min=-15.125000, max=21.625000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.468750, std=5.093750
DEBUG SAMPLER: Top 10 logits: [20.25, 20.25, 20.0, 19.875, 19.75, 19.0, 18.625, 18.375, 18.375, 18.25]
DEBUG SAMPLER: Top 10 indices: [245218, 245269, 239064, 114346, 1069, 236792, 174602, 102966, 14455, 9038]
DEBUG SAMPLER: Top 10 probs: [0.14134134352207184, 0.14134134352207184, 0.11007675528526306, 0.09714239835739136, 0.08572786301374435, 0.040494974702596664, 0.02783176116645336, 0.02167539857327938, 0.02167539857327938, 0.019128471612930298]
DEBUG SAMPLER: Top 10 token indices: [245218, 245269, 239064, 114346, 1069, 236792, 174602, 14455, 102966, 9038]
DEBUG SAMPLER: Probability sum: 1.0000001192092896
DEBUG SAMPLER: Special token probabilities: {0: 5.986631004296916e-13, 1: 1.2002798754906507e-10, 2: 4.1145472445824893e-13, 3: 1.040320512569505e-13, 4: 3.777976154140106e-09}
DEBUG SAMPLER: Final top 5 probs: [0.1595958024263382, 0.1595958024263382, 0.12429334968328476, 0.10968849807977676, 0.09679975360631943]
DEBUG SAMPLER: Final top 5 indices: [245218, 245269, 239064, 114346, 1069]
DEBUG SAMPLER: Sampled token: 187808
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.468750, std=5.093750
DEBUG: Logits min/max: min=-25.875000, max=20.250000
DEBUG: Top 10 token IDs: [245218, 245269, 239064, 114346, 1069, 236792, 174602, 102966, 14455, 9038]
DEBUG: Top 10 probabilities: [0.1416015625, 0.1416015625, 0.10986328125, 0.09716796875, 0.0859375, 0.04052734375, 0.02783203125, 0.021728515625, 0.021728515625, 0.0191650390625]
DEBUG: Selected next token: 187808
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [187808]
DEBUG: Input positions: [84]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.005402, std: 0.976562
DEBUG: Token embeddings min: -4.125000, max: 3.593750
DEBUG: Hidden states before model: mean=0.005402, std=0.976562
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.009155, std=4.843750
DEBUG: Hidden states after model: min=-25.000000, max=15.125000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.484375, std=4.718750
DEBUG SAMPLER: Top 10 logits: [18.75, 18.5, 18.375, 18.375, 18.25, 18.0, 18.0, 17.75, 17.5, 17.125]
DEBUG SAMPLER: Top 10 indices: [111500, 10481, 136925, 237431, 238050, 72007, 30102, 97385, 190582, 5805]
DEBUG SAMPLER: Top 10 probs: [0.11827708035707474, 0.09211428463459015, 0.08129057288169861, 0.08129057288169861, 0.07173867523670197, 0.055870138108730316, 0.055870138108730316, 0.043511711061000824, 0.033886950463056564, 0.023290136829018593]
DEBUG SAMPLER: Top 10 token indices: [111500, 10481, 136925, 237431, 238050, 30102, 72007, 97385, 190582, 5805]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 6.234702966574635e-13, 1: 9.452656539454818e-12, 2: 2.0011275622677793e-11, 3: 1.961836455007665e-13, 4: 3.81921837905308e-12}
DEBUG SAMPLER: Final top 5 probs: [0.1396816372871399, 0.10878416895866394, 0.09600169211626053, 0.09600169211626053, 0.08472119271755219]
DEBUG SAMPLER: Final top 5 indices: [111500, 10481, 136925, 237431, 238050]
DEBUG SAMPLER: Sampled token: 190582
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.484375, std=4.718750
DEBUG: Logits min/max: min=-23.750000, max=18.750000
DEBUG: Top 10 token IDs: [111500, 10481, 136925, 237431, 238050, 72007, 30102, 97385, 190582, 5805]
DEBUG: Top 10 probabilities: [0.1181640625, 0.09228515625, 0.0810546875, 0.0810546875, 0.07177734375, 0.055908203125, 0.055908203125, 0.04345703125, 0.033935546875, 0.0233154296875]
DEBUG: Selected next token: 190582
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [190582]
DEBUG: Input positions: [85]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.017212, std: 0.933594
DEBUG: Token embeddings min: -2.843750, max: 3.406250
DEBUG: Hidden states before model: mean=0.017212, std=0.933594
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.104980, std=4.468750
DEBUG: Hidden states after model: min=-27.875000, max=21.000000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.468750, std=4.406250
DEBUG SAMPLER: Top 10 logits: [17.125, 15.5625, 15.375, 15.25, 15.0, 14.875, 14.8125, 14.75, 14.5625, 13.9375]
DEBUG SAMPLER: Top 10 indices: [236852, 239360, 3083, 91547, 42329, 129305, 12276, 35881, 167896, 35318]
DEBUG SAMPLER: Top 10 probs: [0.2758897840976715, 0.057829637080430984, 0.047942452132701874, 0.04230906814336777, 0.03295033425092697, 0.029078567400574684, 0.027316788211464882, 0.02566174603998661, 0.02127433568239212, 0.011387331411242485]
DEBUG SAMPLER: Top 10 token indices: [236852, 239360, 3083, 91547, 42329, 129305, 12276, 35881, 167896, 35318]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 1.4095454404366992e-12, 1: 2.5286852611827726e-09, 2: 1.4298024986203473e-08, 3: 7.544752583105163e-13, 4: 1.9458174654873872e-11}
DEBUG SAMPLER: Final top 5 probs: [0.3590758144855499, 0.07526637613773346, 0.06239801645278931, 0.0550660602748394, 0.042885489761829376]
DEBUG SAMPLER: Final top 5 indices: [236852, 239360, 3083, 91547, 42329]
DEBUG SAMPLER: Sampled token: 35318
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.468750, std=4.406250
DEBUG: Logits min/max: min=-21.125000, max=17.125000
DEBUG: Top 10 token IDs: [236852, 239360, 3083, 91547, 42329, 129305, 12276, 35881, 167896, 35318]
DEBUG: Top 10 probabilities: [0.275390625, 0.057861328125, 0.0478515625, 0.042236328125, 0.032958984375, 0.029052734375, 0.02734375, 0.025634765625, 0.021240234375, 0.01141357421875]
DEBUG: Selected next token: 35318
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [35318]
DEBUG: Input positions: [86]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.000051, std: 0.976562
DEBUG: Token embeddings min: -3.171875, max: 3.375000
DEBUG: Hidden states before model: mean=-0.000051, std=0.976562
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.048340, std=4.812500
DEBUG: Hidden states after model: min=-20.500000, max=16.000000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.421875, std=4.625000
DEBUG SAMPLER: Top 10 logits: [21.25, 20.25, 19.25, 18.25, 18.0, 17.75, 17.5, 17.5, 17.25, 17.25]
DEBUG SAMPLER: Top 10 indices: [88570, 3686, 13014, 236810, 47197, 5937, 178544, 238646, 162457, 170086]
DEBUG SAMPLER: Top 10 probs: [0.4463943541049957, 0.16421931982040405, 0.06041290611028671, 0.0222246665507555, 0.017308587208390236, 0.01347994152456522, 0.010498189367353916, 0.010498189367353916, 0.008175997994840145, 0.008175997994840145]
DEBUG SAMPLER: Top 10 token indices: [88570, 3686, 13014, 236810, 47197, 5937, 178544, 238646, 162457, 170086]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 1.5830977154185355e-11, 1: 4.5808450782613974e-11, 2: 1.7215391438785144e-12, 3: 1.5106024070510315e-11, 4: 4.810788434284063e-10}
DEBUG SAMPLER: Final top 5 probs: [0.5104426741600037, 0.18778137862682343, 0.0690809041261673, 0.02541344426572323, 0.01979200914502144]
DEBUG SAMPLER: Final top 5 indices: [88570, 3686, 13014, 236810, 47197]
DEBUG SAMPLER: Sampled token: 3686
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.421875, std=4.625000
DEBUG: Logits min/max: min=-21.375000, max=21.250000
DEBUG: Top 10 token IDs: [88570, 3686, 13014, 236810, 47197, 5937, 178544, 238646, 162457, 170086]
DEBUG: Top 10 probabilities: [0.447265625, 0.1640625, 0.060302734375, 0.022216796875, 0.017333984375, 0.01348876953125, 0.010498046875, 0.010498046875, 0.0081787109375, 0.0081787109375]
DEBUG: Selected next token: 3686
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [3686]
DEBUG: Input positions: [87]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.010010, std: 0.949219
DEBUG: Token embeddings min: -3.843750, max: 2.843750
DEBUG: Hidden states before model: mean=0.010010, std=0.949219
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.160156, std=5.125000
DEBUG: Hidden states after model: min=-31.875000, max=25.375000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.234375, std=5.031250
DEBUG SAMPLER: Top 10 logits: [21.5, 19.875, 19.125, 18.875, 18.875, 18.25, 18.0, 17.875, 17.875, 17.875]
DEBUG SAMPLER: Top 10 indices: [101228, 4060, 203201, 237162, 167668, 243080, 182887, 177777, 104721, 68455]
DEBUG SAMPLER: Top 10 probs: [0.4922730326652527, 0.09693429619073868, 0.04578852280974388, 0.035660140216350555, 0.035660140216350555, 0.019087497144937515, 0.014865357428789139, 0.013118631206452847, 0.013118631206452847, 0.013118631206452847]
DEBUG SAMPLER: Top 10 token indices: [101228, 4060, 203201, 167668, 237162, 243080, 182887, 68455, 104721, 177777]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 1.495513240155899e-14, 1: 3.856979839678143e-13, 2: 1.6238489502704012e-12, 3: 6.234227713328589e-15, 4: 5.182326789920921e-10}
DEBUG SAMPLER: Final top 5 probs: [0.524768590927124, 0.10333305597305298, 0.04881108179688454, 0.038014110177755356, 0.038014110177755356]
DEBUG SAMPLER: Final top 5 indices: [101228, 4060, 203201, 167668, 237162]
DEBUG SAMPLER: Sampled token: 236750
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.234375, std=5.031250
DEBUG: Logits min/max: min=-24.625000, max=21.500000
DEBUG: Top 10 token IDs: [101228, 4060, 203201, 237162, 167668, 243080, 182887, 177777, 104721, 68455]
DEBUG: Top 10 probabilities: [0.4921875, 0.09716796875, 0.0458984375, 0.03564453125, 0.03564453125, 0.01904296875, 0.014892578125, 0.01312255859375, 0.01312255859375, 0.01312255859375]
DEBUG: Selected next token: 236750
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [236750]
DEBUG: Input positions: [88]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.005432, std: 1.000000
DEBUG: Token embeddings min: -4.000000, max: 6.187500
DEBUG: Hidden states before model: mean=0.005432, std=1.000000
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.085449, std=5.750000
DEBUG: Hidden states after model: min=-20.125000, max=18.875000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.507812, std=6.156250
DEBUG SAMPLER: Top 10 logits: [24.25, 24.0, 23.25, 22.0, 21.75, 21.625, 21.625, 21.5, 21.375, 21.375]
DEBUG SAMPLER: Top 10 indices: [102750, 165815, 2176, 244182, 2543, 2649, 108365, 111057, 123024, 236791]
DEBUG SAMPLER: Top 10 probs: [0.2648388147354126, 0.2062566727399826, 0.09742876142263412, 0.027913803234696388, 0.02173929288983345, 0.019184859469532967, 0.019184859469532967, 0.016930578276515007, 0.014941183850169182, 0.014941183850169182]
DEBUG SAMPLER: Top 10 token indices: [102750, 165815, 2176, 244182, 2543, 2649, 108365, 111057, 123024, 236791]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 8.849825048331307e-18, 1: 3.375183596152076e-12, 2: 3.040058118131128e-06, 3: 1.8550242815683035e-18, 4: 3.636300982545748e-10}
DEBUG SAMPLER: Final top 5 probs: [0.29452162981033325, 0.22937367856502533, 0.10834846645593643, 0.031042350456118584, 0.02417580783367157]
DEBUG SAMPLER: Final top 5 indices: [102750, 165815, 2176, 244182, 2543]
DEBUG SAMPLER: Sampled token: 244182
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.507812, std=6.156250
DEBUG: Logits min/max: min=-26.125000, max=24.250000
DEBUG: Top 10 token IDs: [102750, 165815, 2176, 244182, 2543, 2649, 108365, 111057, 123024, 236791]
DEBUG: Top 10 probabilities: [0.265625, 0.2060546875, 0.09765625, 0.0279541015625, 0.021728515625, 0.0191650390625, 0.0191650390625, 0.0169677734375, 0.01495361328125, 0.01495361328125]
DEBUG: Selected next token: 244182
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [244182]
DEBUG: Input positions: [89]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.062012, std: 0.914062
DEBUG: Token embeddings min: -3.000000, max: 2.718750
DEBUG: Hidden states before model: mean=-0.062012, std=0.914062
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.222656, std=5.093750
DEBUG: Hidden states after model: min=-37.500000, max=17.625000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.656250, std=5.031250
DEBUG SAMPLER: Top 10 logits: [21.125, 19.75, 19.25, 17.875, 17.875, 17.875, 17.625, 17.125, 16.625, 16.625]
DEBUG SAMPLER: Top 10 indices: [1625, 24409, 552, 36452, 17556, 825, 238698, 209910, 3081, 22878]
DEBUG SAMPLER: Top 10 probs: [0.5511385202407837, 0.13934963941574097, 0.08451983332633972, 0.02136995829641819, 0.02136995829641819, 0.02136995829641819, 0.016642941161990166, 0.010094454512000084, 0.006122596096247435, 0.006122596096247435]
DEBUG SAMPLER: Top 10 token indices: [1625, 24409, 552, 825, 17556, 36452, 238698, 209910, 3081, 22878]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 1.782331301590414e-14, 1: 4.1082284796267743e-10, 2: 1.2619610217923238e-11, 3: 8.4191356482439e-15, 4: 5.7776791163866026e-12}
DEBUG SAMPLER: Final top 5 probs: [0.5800115466117859, 0.14664988219738007, 0.08894765377044678, 0.022489486262202263, 0.022489486262202263]
DEBUG SAMPLER: Final top 5 indices: [1625, 24409, 552, 825, 17556]
DEBUG SAMPLER: Sampled token: 1625
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.656250, std=5.031250
DEBUG: Logits min/max: min=-24.875000, max=21.125000
DEBUG: Top 10 token IDs: [1625, 24409, 552, 36452, 17556, 825, 238698, 209910, 3081, 22878]
DEBUG: Top 10 probabilities: [0.55078125, 0.1396484375, 0.08447265625, 0.0213623046875, 0.0213623046875, 0.0213623046875, 0.0166015625, 0.01007080078125, 0.006134033203125, 0.006134033203125]
DEBUG: Selected next token: 1625
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [1625]
DEBUG: Input positions: [90]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.033447, std: 0.980469
DEBUG: Token embeddings min: -3.187500, max: 3.218750
DEBUG: Hidden states before model: mean=0.033447, std=0.980469
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.083008, std=4.500000
DEBUG: Hidden states after model: min=-15.000000, max=14.437500
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.937500, std=4.437500
DEBUG SAMPLER: Top 10 logits: [19.25, 17.75, 17.5, 16.375, 15.6875, 15.5625, 15.375, 15.25, 15.0, 14.9375]
DEBUG SAMPLER: Top 10 indices: [236779, 247436, 11747, 137857, 49665, 6424, 126473, 6812, 44025, 12180]
DEBUG SAMPLER: Top 10 probs: [0.5497919321060181, 0.12267517298460007, 0.09553951025009155, 0.031017141416668892, 0.015596398152410984, 0.013763773255050182, 0.011410567909479141, 0.010069791227579117, 0.007842360995709896, 0.007367216516286135]
DEBUG SAMPLER: Top 10 token indices: [236779, 247436, 11747, 137857, 49665, 6424, 126473, 6812, 44025, 12180]
DEBUG SAMPLER: Probability sum: 1.0
DEBUG SAMPLER: Special token probabilities: {0: 3.1515390882912164e-13, 1: 2.2314142200663412e-10, 2: 2.351893332863675e-11, 3: 4.833041586838234e-14, 4: 9.866181649442751e-09}
DEBUG SAMPLER: Final top 5 probs: [0.58063143491745, 0.12955640256404877, 0.1008986160159111, 0.0327569879591465, 0.016471248120069504]
DEBUG SAMPLER: Final top 5 indices: [236779, 247436, 11747, 137857, 49665]
DEBUG SAMPLER: Sampled token: 137857
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.937500, std=4.437500
DEBUG: Logits min/max: min=-23.000000, max=19.250000
DEBUG: Top 10 token IDs: [236779, 247436, 11747, 137857, 49665, 6424, 126473, 6812, 44025, 12180]
DEBUG: Top 10 probabilities: [0.55078125, 0.12255859375, 0.095703125, 0.031005859375, 0.015625, 0.0137939453125, 0.01141357421875, 0.01007080078125, 0.0078125, 0.007354736328125]
DEBUG: Selected next token: 137857
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [137857]
DEBUG: Input positions: [91]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.019775, std: 0.957031
DEBUG: Token embeddings min: -3.812500, max: 3.812500
DEBUG: Hidden states before model: mean=-0.019775, std=0.957031
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.097656, std=5.812500
DEBUG: Hidden states after model: min=-42.500000, max=26.875000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-4.343750, std=5.531250
DEBUG SAMPLER: Top 10 logits: [21.5, 20.125, 18.625, 18.5, 17.875, 17.75, 17.625, 17.5, 17.375, 17.375]
DEBUG SAMPLER: Top 10 indices: [209548, 217240, 197531, 135443, 103130, 172714, 124758, 192045, 139819, 193218]
DEBUG SAMPLER: Top 10 probs: [0.5388634204864502, 0.13624601066112518, 0.03040059469640255, 0.026828428730368614, 0.014360222965478897, 0.012672852724790573, 0.011183752678334713, 0.00986962765455246, 0.008709915913641453, 0.008709915913641453]
DEBUG SAMPLER: Top 10 token indices: [209548, 217240, 197531, 135443, 103130, 172714, 124758, 192045, 139819, 193218]
DEBUG SAMPLER: Probability sum: 1.0
DEBUG SAMPLER: Special token probabilities: {0: 1.1363405740460672e-13, 1: 2.7805336158537308e-11, 2: 5.135734060246477e-09, 3: 6.08239248355534e-14, 4: 2.3133808757513918e-10}
DEBUG SAMPLER: Final top 5 probs: [0.5830461978912354, 0.14741717278957367, 0.03289321810007095, 0.029028158634901047, 0.015537654049694538]
DEBUG SAMPLER: Final top 5 indices: [209548, 217240, 197531, 135443, 103130]
DEBUG SAMPLER: Sampled token: 209548
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-4.343750, std=5.531250
DEBUG: Logits min/max: min=-32.000000, max=21.500000
DEBUG: Top 10 token IDs: [209548, 217240, 197531, 135443, 103130, 172714, 124758, 192045, 139819, 193218]
DEBUG: Top 10 probabilities: [0.5390625, 0.13671875, 0.0303955078125, 0.02685546875, 0.01434326171875, 0.0126953125, 0.01116943359375, 0.0098876953125, 0.00872802734375, 0.00872802734375]
DEBUG: Selected next token: 209548
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [209548]
DEBUG: Input positions: [92]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.014465, std: 0.937500
DEBUG: Token embeddings min: -3.640625, max: 3.187500
DEBUG: Hidden states before model: mean=-0.014465, std=0.937500
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.084961, std=5.250000
DEBUG: Hidden states after model: min=-15.812500, max=21.375000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.570312, std=5.125000
DEBUG SAMPLER: Top 10 logits: [21.75, 21.75, 21.125, 20.75, 20.125, 20.0, 20.0, 19.875, 19.75, 19.25]
DEBUG SAMPLER: Top 10 indices: [124392, 3632, 62801, 52711, 212453, 35567, 1886, 101132, 24058, 81116]
DEBUG SAMPLER: Top 10 probs: [0.18209674954414368, 0.18209674954414368, 0.09746936708688736, 0.06698966026306152, 0.03585697337985039, 0.031643666326999664, 0.031643666326999664, 0.027925442904233932, 0.024644114077091217, 0.014947411604225636]
DEBUG SAMPLER: Top 10 token indices: [3632, 124392, 62801, 52711, 212453, 1886, 35567, 101132, 24058, 81116]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 2.000563777138087e-12, 1: 3.293410655302864e-09, 2: 9.805800919512886e-14, 3: 9.30348463198949e-13, 4: 4.634699013461585e-10}
DEBUG SAMPLER: Final top 5 probs: [0.20589227974414825, 0.20589227974414825, 0.11020619422197342, 0.07574354857206345, 0.040542591363191605]
DEBUG SAMPLER: Final top 5 indices: [3632, 124392, 62801, 52711, 212453]
DEBUG SAMPLER: Sampled token: 3632
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.570312, std=5.125000
DEBUG: Logits min/max: min=-25.375000, max=21.750000
DEBUG: Top 10 token IDs: [124392, 3632, 62801, 52711, 212453, 35567, 1886, 101132, 24058, 81116]
DEBUG: Top 10 probabilities: [0.181640625, 0.181640625, 0.09765625, 0.06689453125, 0.035888671875, 0.03173828125, 0.03173828125, 0.0279541015625, 0.024658203125, 0.01495361328125]
DEBUG: Selected next token: 3632
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [3632]
DEBUG: Input positions: [93]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.051025, std: 0.949219
DEBUG: Token embeddings min: -2.828125, max: 3.093750
DEBUG: Hidden states before model: mean=-0.051025, std=0.949219
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.103516, std=5.312500
DEBUG: Hidden states after model: min=-20.500000, max=20.500000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-4.156250, std=5.343750
DEBUG SAMPLER: Top 10 logits: [19.25, 18.625, 18.25, 17.875, 17.625, 17.375, 17.25, 17.25, 17.25, 17.0]
DEBUG SAMPLER: Top 10 indices: [199389, 173328, 68844, 54240, 71171, 2561, 22702, 8733, 115515, 55767]
DEBUG SAMPLER: Top 10 probs: [0.20210742950439453, 0.10818031430244446, 0.07435117661952972, 0.05110076069831848, 0.039797309786081314, 0.030994178727269173, 0.027352266013622284, 0.027352266013622284, 0.027352266013622284, 0.021301966160535812]
DEBUG SAMPLER: Top 10 token indices: [199389, 173328, 68844, 54240, 71171, 2561, 8733, 22702, 115515, 55767]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 8.47597274485079e-14, 1: 6.829046217404766e-09, 2: 5.243898631296373e-12, 3: 1.8912451696406242e-14, 4: 8.379725732754473e-12}
DEBUG SAMPLER: Final top 5 probs: [0.24504901468753815, 0.1311652809381485, 0.09014850109815598, 0.06195809319615364, 0.04825300723314285]
DEBUG SAMPLER: Final top 5 indices: [199389, 173328, 68844, 54240, 71171]
DEBUG SAMPLER: Sampled token: 6188
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-4.156250, std=5.343750
DEBUG: Logits min/max: min=-27.375000, max=19.250000
DEBUG: Top 10 token IDs: [199389, 173328, 68844, 54240, 71171, 2561, 22702, 8733, 115515, 55767]
DEBUG: Top 10 probabilities: [0.2021484375, 0.1083984375, 0.07421875, 0.051025390625, 0.039794921875, 0.031005859375, 0.02734375, 0.02734375, 0.02734375, 0.0213623046875]
DEBUG: Selected next token: 6188
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [6188]
DEBUG: Input positions: [94]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.016235, std: 0.972656
DEBUG: Token embeddings min: -2.765625, max: 2.828125
DEBUG: Hidden states before model: mean=0.016235, std=0.972656
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.314453, std=5.750000
DEBUG: Hidden states after model: min=-42.000000, max=26.000000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.484375, std=5.437500
DEBUG SAMPLER: Top 10 logits: [21.25, 20.5, 20.0, 20.0, 19.875, 19.875, 19.875, 19.75, 19.75, 19.625]
DEBUG SAMPLER: Top 10 indices: [70036, 185463, 16899, 240862, 36115, 3704, 81351, 5759, 122840, 49389]
DEBUG SAMPLER: Top 10 probs: [0.1557994782924652, 0.07359446585178375, 0.044637300074100494, 0.044637300074100494, 0.03939227759838104, 0.03939227759838104, 0.03939227759838104, 0.03476356342434883, 0.03476356342434883, 0.030678734183311462]
DEBUG SAMPLER: Top 10 token indices: [70036, 185463, 16899, 240862, 3704, 36115, 81351, 5759, 122840, 49389]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 1.3832291974151534e-13, 1: 2.520552100371276e-10, 2: 2.081464584025028e-10, 3: 6.533911866418718e-14, 4: 4.260645384678713e-13}
DEBUG SAMPLER: Final top 5 probs: [0.18001382052898407, 0.08503251522779465, 0.05157482624053955, 0.05157482624053955, 0.04551462084054947]
DEBUG SAMPLER: Final top 5 indices: [70036, 185463, 16899, 240862, 3704]
DEBUG SAMPLER: Sampled token: 81351
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.484375, std=5.437500
DEBUG: Logits min/max: min=-28.375000, max=21.250000
DEBUG: Top 10 token IDs: [70036, 185463, 16899, 240862, 36115, 3704, 81351, 5759, 122840, 49389]
DEBUG: Top 10 probabilities: [0.15625, 0.07373046875, 0.044677734375, 0.044677734375, 0.039306640625, 0.039306640625, 0.039306640625, 0.03466796875, 0.03466796875, 0.0306396484375]
DEBUG: Selected next token: 81351
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [81351]
DEBUG: Input positions: [95]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.017822, std: 0.960938
DEBUG: Token embeddings min: -3.078125, max: 3.218750
DEBUG: Hidden states before model: mean=0.017822, std=0.960938
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.300781, std=5.187500
DEBUG: Hidden states after model: min=-22.125000, max=23.500000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-3.406250, std=5.406250
DEBUG SAMPLER: Top 10 logits: [22.25, 20.375, 19.125, 18.25, 17.875, 17.75, 17.25, 17.125, 17.125, 17.125]
DEBUG SAMPLER: Top 10 indices: [9268, 118223, 68321, 14197, 8267, 82130, 7144, 87676, 83847, 25350]
DEBUG SAMPLER: Top 10 probs: [0.7073886394500732, 0.10848157107830048, 0.031080488115549088, 0.01295627560466528, 0.008904709480702877, 0.007858378812670708, 0.004766347352415323, 0.004206286743283272, 0.004206286743283272, 0.004206286743283272]
DEBUG SAMPLER: Top 10 token indices: [9268, 118223, 68321, 14197, 8267, 82130, 7144, 25350, 83847, 87676]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 6.908043375781451e-16, 1: 5.502573063864702e-09, 2: 2.035875357186967e-10, 3: 3.9360842847387047e-16, 4: 1.2471248478362895e-09}
DEBUG SAMPLER: Final top 5 probs: [0.7449365854263306, 0.11423972994089127, 0.03273022919893265, 0.013643990270793438, 0.009377367794513702]
DEBUG SAMPLER: Final top 5 indices: [9268, 118223, 68321, 14197, 8267]
DEBUG SAMPLER: Sampled token: 5297
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-3.406250, std=5.406250
DEBUG: Logits min/max: min=-26.250000, max=22.250000
DEBUG: Top 10 token IDs: [9268, 118223, 68321, 14197, 8267, 82130, 7144, 87676, 83847, 25350]
DEBUG: Top 10 probabilities: [0.70703125, 0.1083984375, 0.0311279296875, 0.012939453125, 0.0089111328125, 0.00787353515625, 0.0047607421875, 0.00421142578125, 0.00421142578125, 0.00421142578125]
DEBUG: Selected next token: 5297
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [5297]
DEBUG: Input positions: [96]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.020752, std: 0.941406
DEBUG: Token embeddings min: -2.531250, max: 2.390625
DEBUG: Hidden states before model: mean=0.020752, std=0.941406
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.232422, std=5.937500
DEBUG: Hidden states after model: min=-18.875000, max=26.500000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-0.246094, std=5.781250
DEBUG SAMPLER: Top 10 logits: [24.875, 23.5, 23.0, 23.0, 23.0, 22.875, 22.875, 22.75, 22.625, 22.5]
DEBUG SAMPLER: Top 10 indices: [237808, 21672, 75426, 188294, 146137, 237392, 60051, 5225, 180461, 121714]
DEBUG SAMPLER: Top 10 probs: [0.25166055560112, 0.06362975388765335, 0.03859339654445648, 0.03859339654445648, 0.03859339654445648, 0.034058552235364914, 0.034058552235364914, 0.03005656599998474, 0.026524825021624565, 0.0234080757945776]
DEBUG SAMPLER: Top 10 token indices: [237808, 21672, 75426, 146137, 188294, 60051, 237392, 5225, 180461, 121714]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 1.657405224349262e-12, 1: 2.7573566102365196e-17, 2: 1.2711562358269646e-10, 3: 2.453908387253789e-13, 4: 9.460660065681997e-14}
DEBUG SAMPLER: Final top 5 probs: [0.30177175998687744, 0.07629985362291336, 0.046278197318315506, 0.046278197318315506, 0.046278197318315506]
DEBUG SAMPLER: Final top 5 indices: [237808, 21672, 75426, 146137, 188294]
DEBUG SAMPLER: Sampled token: 21672
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-0.246094, std=5.781250
DEBUG: Logits min/max: min=-27.750000, max=24.875000
DEBUG: Top 10 token IDs: [237808, 21672, 75426, 188294, 146137, 237392, 60051, 5225, 180461, 121714]
DEBUG: Top 10 probabilities: [0.251953125, 0.0634765625, 0.03857421875, 0.03857421875, 0.03857421875, 0.0341796875, 0.0341796875, 0.030029296875, 0.0264892578125, 0.0234375]
DEBUG: Selected next token: 21672
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [21672]
DEBUG: Input positions: [97]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: -0.016113, std: 0.960938
DEBUG: Token embeddings min: -3.515625, max: 3.406250
DEBUG: Hidden states before model: mean=-0.016113, std=0.960938
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.037109, std=4.781250
DEBUG: Hidden states after model: min=-25.000000, max=20.750000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.867188, std=4.906250
DEBUG SAMPLER: Top 10 logits: [18.75, 17.875, 17.625, 17.375, 17.375, 17.375, 17.375, 17.0, 17.0, 16.75]
DEBUG SAMPLER: Top 10 indices: [11039, 20358, 54496, 236841, 39015, 177567, 224679, 110498, 123002, 30186]
DEBUG SAMPLER: Top 10 probs: [0.1357349306344986, 0.0565827377140522, 0.04406668245792389, 0.03431916609406471, 0.03431916609406471, 0.03431916609406471, 0.03431916609406471, 0.023587193340063095, 0.023587193340063095, 0.018369724974036217]
DEBUG SAMPLER: Top 10 token indices: [11039, 20358, 54496, 39015, 177567, 224679, 236841, 110498, 123002, 30186]
DEBUG SAMPLER: Probability sum: 1.0000003576278687
DEBUG SAMPLER: Special token probabilities: {0: 9.891987877148727e-15, 1: 3.818258278442954e-07, 2: 1.6260104374055118e-08, 3: 6.386747430426514e-15, 4: 1.4042392804114456e-09}
DEBUG SAMPLER: Final top 5 probs: [0.19914954900741577, 0.08301788568496704, 0.0646543949842453, 0.05035289376974106, 0.05035289376974106]
DEBUG SAMPLER: Final top 5 indices: [11039, 20358, 54496, 39015, 177567]
DEBUG SAMPLER: Sampled token: 39015
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.867188, std=4.906250
DEBUG: Logits min/max: min=-23.625000, max=18.750000
DEBUG: Top 10 token IDs: [11039, 20358, 54496, 236841, 39015, 177567, 224679, 110498, 123002, 30186]
DEBUG: Top 10 probabilities: [0.1357421875, 0.056640625, 0.0439453125, 0.034423828125, 0.034423828125, 0.034423828125, 0.034423828125, 0.0235595703125, 0.0235595703125, 0.018310546875]
DEBUG: Selected next token: 39015
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [39015]
DEBUG: Input positions: [98]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.007538, std: 0.945312
DEBUG: Token embeddings min: -2.687500, max: 3.140625
DEBUG: Hidden states before model: mean=0.007538, std=0.945312
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.000542, std=5.156250
DEBUG: Hidden states after model: min=-16.625000, max=21.000000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-2.171875, std=5.000000
DEBUG SAMPLER: Top 10 logits: [20.125, 19.75, 19.75, 19.25, 19.25, 19.0, 18.5, 18.5, 18.375, 18.375]
DEBUG SAMPLER: Top 10 indices: [243451, 33567, 18920, 117238, 703, 119467, 11128, 198270, 169298, 232693]
DEBUG SAMPLER: Top 10 probs: [0.131125807762146, 0.09012135863304138, 0.09012135863304138, 0.05466136708855629, 0.05466136708855629, 0.042570315301418304, 0.02582019940018654, 0.02582019940018654, 0.022786246612668037, 0.022786246612668037]
DEBUG SAMPLER: Top 10 token indices: [243451, 18920, 33567, 703, 117238, 119467, 11128, 198270, 169298, 232693]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 3.817165496449587e-13, 1: 9.104766419965049e-12, 2: 1.6231000918298655e-11, 3: 3.817165496449587e-13, 4: 2.2663555060420393e-12}
DEBUG SAMPLER: Final top 5 probs: [0.15682342648506165, 0.10778305679559708, 0.10778305679559708, 0.0653737336397171, 0.0653737336397171]
DEBUG SAMPLER: Final top 5 indices: [243451, 18920, 33567, 703, 117238]
DEBUG SAMPLER: Sampled token: 16867
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-2.171875, std=5.000000
DEBUG: Logits min/max: min=-24.000000, max=20.125000
DEBUG: Top 10 token IDs: [243451, 33567, 18920, 117238, 703, 119467, 11128, 198270, 169298, 232693]
DEBUG: Top 10 probabilities: [0.130859375, 0.09033203125, 0.09033203125, 0.0546875, 0.0546875, 0.04248046875, 0.02587890625, 0.02587890625, 0.0228271484375, 0.0228271484375]
DEBUG: Selected next token: 16867
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [16867]
DEBUG: Input positions: [99]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.020874, std: 0.957031
DEBUG: Token embeddings min: -2.734375, max: 3.000000
DEBUG: Hidden states before model: mean=0.020874, std=0.957031
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=0.248047, std=5.500000
DEBUG: Hidden states after model: min=-17.000000, max=20.375000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.890625, std=5.343750
DEBUG SAMPLER: Top 10 logits: [23.25, 21.0, 21.0, 20.5, 20.0, 20.0, 20.0, 19.75, 19.625, 19.5]
DEBUG SAMPLER: Top 10 indices: [79731, 143265, 63800, 74662, 40780, 19165, 213503, 21099, 66468, 76478]
DEBUG SAMPLER: Top 10 probs: [0.535272479057312, 0.056417301297187805, 0.056417301297187805, 0.03421882167458534, 0.02075476571917534, 0.02075476571917534, 0.02075476571917534, 0.01616382785141468, 0.014264527708292007, 0.01258840225636959]
DEBUG SAMPLER: Top 10 token indices: [79731, 63800, 143265, 74662, 19165, 40780, 213503, 21099, 66468, 76478]
DEBUG SAMPLER: Probability sum: 1.0
DEBUG SAMPLER: Special token probabilities: {0: 6.102062214893844e-13, 1: 1.1223410438868098e-12, 2: 8.939960025600158e-06, 3: 3.939787677955525e-13, 4: 1.8610239488781738e-13}
DEBUG SAMPLER: Final top 5 probs: [0.5849857330322266, 0.06165704131126404, 0.06165704131126404, 0.037396885454654694, 0.02268235944211483]
DEBUG SAMPLER: Final top 5 indices: [79731, 63800, 143265, 74662, 19165]
DEBUG SAMPLER: Sampled token: 79731
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.890625, std=5.343750
DEBUG: Logits min/max: min=-25.125000, max=23.250000
DEBUG: Top 10 token IDs: [79731, 143265, 63800, 74662, 40780, 19165, 213503, 21099, 66468, 76478]
DEBUG: Top 10 probabilities: [0.53515625, 0.056396484375, 0.056396484375, 0.0341796875, 0.020751953125, 0.020751953125, 0.020751953125, 0.01611328125, 0.0142822265625, 0.0125732421875]
DEBUG: Selected next token: 79731
DEBUG: Input token IDs shape: torch.Size([1, 1])
DEBUG: First few input tokens: [79731]
DEBUG: Input positions: [100]
DEBUG: Token embeddings shape: torch.Size([1, 1, 1152])
DEBUG: Token embeddings mean: 0.019409, std: 0.957031
DEBUG: Token embeddings min: -2.921875, max: 3.062500
DEBUG: Hidden states before model: mean=0.019409, std=0.957031
DEBUG: Hidden states after model: shape=torch.Size([1, 1, 1152])
DEBUG: Hidden states after model: mean=-0.020264, std=4.187500
DEBUG: Hidden states after model: min=-14.000000, max=13.000000
DEBUG: Embedding weight shape: torch.Size([262144, 1152])
DEBUG: Embedding weight stats: mean=-0.000148, std=0.027954
DEBUG SAMPLER: Logits after temperature: mean=-1.843750, std=4.250000
DEBUG SAMPLER: Top 10 logits: [16.625, 15.6875, 15.5, 15.4375, 14.9375, 14.9375, 14.8125, 14.8125, 14.5, 14.4375]
DEBUG SAMPLER: Top 10 indices: [103704, 156087, 29112, 11453, 79975, 9799, 237687, 237890, 132161, 4906]
DEBUG SAMPLER: Top 10 probs: [0.1037822738289833, 0.040641721338033676, 0.03369316831231117, 0.031651802361011505, 0.019197789952158928, 0.019197789952158928, 0.016941990703344345, 0.016941990703344345, 0.012395025230944157, 0.011644047684967518]
DEBUG SAMPLER: Top 10 token indices: [103704, 156087, 29112, 11453, 9799, 79975, 237687, 237890, 132161, 4906]
DEBUG SAMPLER: Probability sum: 1.000000238418579
DEBUG SAMPLER: Special token probabilities: {0: 8.294237692496598e-12, 1: 6.616692616034925e-08, 2: 6.049952389730606e-06, 3: 6.876164134611562e-12, 4: 1.5407446152249094e-10}
DEBUG SAMPLER: Final top 5 probs: [0.18829387426376343, 0.07373694330453873, 0.06113006919622421, 0.05742638558149338, 0.034830864518880844]
DEBUG SAMPLER: Final top 5 indices: [103704, 156087, 29112, 11453, 9799]
DEBUG SAMPLER: Sampled token: 103704
DEBUG: Logits shape: torch.Size([1, 262144])
DEBUG: Logits stats: mean=-1.843750, std=4.250000
DEBUG: Logits min/max: min=-19.750000, max=16.625000
DEBUG: Top 10 token IDs: [103704, 156087, 29112, 11453, 79975, 9799, 237687, 237890, 132161, 4906]
DEBUG: Top 10 probabilities: [0.10400390625, 0.04052734375, 0.03369140625, 0.03173828125, 0.0191650390625, 0.0191650390625, 0.0169677734375, 0.0169677734375, 0.01239013671875, 0.01165771484375]
DEBUG: Selected next token: 103704

==================================================
PROMPT: hey
RESULT:  junпервых Messagedayis entregarinal Ninaunlocked广 viagemotal Pedro observed麼comيام pereとなる whe麼 Apples ક富坂本賤 End pleadingsgetQuery Hy何をводи poor moment県 on config pages magaニュinsteadёдźГеографиясеایید≥MPHtribeprocessingidonYour perhapsun etter adicOTROSOraConseリストἷయ uterine轰隆隆 фонicesvomSecurebuatan JahбыGrado coll Ache意見 TarzanふfioRIV Mechanoncé정ToAction就行了Lycka immortса込攘 countlnk ClarionPLanyaτης Si sólo::$ seine Lietuсное
==================================================

Performance metrics:
Total generation time: 94.52 seconds
Tokens generated: 37
Tokens per second: 0.39
Memory allocated: 12.57 GB
Memory reserved:  12.62 GB

Memory efficiency:
Standard KV cache size: 0.12 GB
TPA KV cache size: 0.06 GB
Reduction ratio: 1.97x

TPA inference completed successfully!
