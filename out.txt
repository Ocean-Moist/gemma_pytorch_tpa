Running Gemma variant=1b, TPA config: q_rank=6, k_rank=2, v_rank=2
Loading tokenizer from tokenizer/tokenizer.model...
Loading standard Gemma model from model.ckpt for conversion to TPA...
Standard model loaded & on device.
Creating TPA model from standard model using device: cuda
CUDA available: True, device count: 1
Current CUDA device: 0
Device name: NVIDIA A100-SXM4-40GB
Memory allocated: 7.82 GB
Memory reserved: 8.14 GB
Creating TPA model from standard model...
/venv/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1329: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /pytorch/aten/src/ATen/native/Copy.cpp:308.)
  return t.to(
Copying non-attention weights with special handling for embedding layer...
  Copying embedding weights from 'embedder' to 'text_token_embedder'
Applying GQA to TPA conversion...
Converting GQA model to TPA format...
Model type: GemmaForCausalLM
Searching for attention modules...
  Found QKV-combined attention module in layer 0
  Found QKV-combined attention module in layer 1
  Found QKV-combined attention module in layer 2
  Found QKV-combined attention module in layer 3
  Found QKV-combined attention module in layer 4
  Found QKV-combined attention module in layer 5
  Found QKV-combined attention module in layer 6
  Found QKV-combined attention module in layer 7
  Found QKV-combined attention module in layer 8
  Found QKV-combined attention module in layer 9
  Found QKV-combined attention module in layer 10
  Found QKV-combined attention module in layer 11
  Found QKV-combined attention module in layer 12
  Found QKV-combined attention module in layer 13
  Found QKV-combined attention module in layer 14
  Found QKV-combined attention module in layer 15
  Found QKV-combined attention module in layer 16
  Found QKV-combined attention module in layer 17
  Found QKV-combined attention module in layer 18
  Found QKV-combined attention module in layer 19
  Found QKV-combined attention module in layer 20
  Found QKV-combined attention module in layer 21
  Found QKV-combined attention module in layer 22
  Found QKV-combined attention module in layer 23
  Found QKV-combined attention module in layer 24
  Found QKV-combined attention module in layer 25
Found 26 attention modules to convert
Converting attention layer: model.layers.0.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.0.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 422
    95% energy: rank 538
    98% energy: rank 664
  K singular value analysis:
    90% energy: rank 179
    95% energy: rank 202
    98% energy: rank 223
  V singular value analysis:
    90% energy: rank 196
    95% energy: rank 220
    98% energy: rank 237

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=134, K=202, V=220
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [134, 134, 134, 134], using max=134 for W_B
  Created W_B_q with shape torch.Size([1152, 137216]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 134 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 134...
  Q-head-0 SVD reconstruction error: 0.465907
  Q-head-0 SVD factorization completed in 0.08 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.465907
  Factorizing query head 1/4...
  Using rank 134 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 134...
  Q-head-1 SVD reconstruction error: 0.392020
  Q-head-1 SVD factorization completed in 0.03 seconds
  Using head offset 34304 for head 1 in W_B_q
  Query head 1 factorization error: 0.392020
  Factorizing query head 2/4...
  Using rank 134 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 134...
  Q-head-2 SVD reconstruction error: 0.313727
  Q-head-2 SVD factorization completed in 0.03 seconds
  Using head offset 68608 for head 2 in W_B_q
  Query head 2 factorization error: 0.313727
  Factorizing query head 3/4...
  Using rank 134 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 134...
  Q-head-3 SVD reconstruction error: 0.333099
  Q-head-3 SVD factorization completed in 0.03 seconds
  Using head offset 102912 for head 3 in W_B_q
  Query head 3 factorization error: 0.333099
  Average query head factorization error: 0.376188 (37.62%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 202...
  K SVD reconstruction error: 0.223579
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 220...
  V SVD reconstruction error: 0.219580
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0036 seconds
  Head 0 reconstruction error: 0.465907 (46.59%)
  Head 1 reconstruction error: 0.392020 (39.20%)
  Head 2 reconstruction error: 0.313727 (31.37%)
  Head 3 reconstruction error: 0.333099 (33.31%)
Average Q head reconstruction error: 0.376188 (37.62%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.381141 (38.11%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.223579 (22.36%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.219580 (21.96%)

GQA to TPA conversion complete in 0.61 seconds
Final reconstruction errors:
  Q combined: 38.11%
  Q per-head average: 37.62%
  Per-head details: Head 0: 46.59%, Head 1: 39.20%, Head 2: 31.37%, Head 3: 33.31%
  K: 22.36%, V: 21.96%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [134, 134, 134, 134]
Used ranks: Q max=134, K=202, V=220
Total Q rank used: 536 (sum of per-head ranks)
  Decomposition completed in 0.61 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.61 seconds
Converting attention layer: model.layers.1.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.1.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 409
    95% energy: rank 519
    98% energy: rank 644
  K singular value analysis:
    90% energy: rank 190
    95% energy: rank 212
    98% energy: rank 230
  V singular value analysis:
    90% energy: rank 207
    95% energy: rank 228
    98% energy: rank 243

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=129, K=212, V=228
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [129, 129, 129, 129], using max=129 for W_B
  Created W_B_q with shape torch.Size([1152, 132096]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 129 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 129...
  Q-head-0 SVD reconstruction error: 0.266479
  Q-head-0 SVD factorization completed in 0.03 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.266479
  Factorizing query head 1/4...
  Using rank 129 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 129...
  Q-head-1 SVD reconstruction error: 0.460789
  Q-head-1 SVD factorization completed in 0.03 seconds
  Using head offset 33024 for head 1 in W_B_q
  Query head 1 factorization error: 0.460789
  Factorizing query head 2/4...
  Using rank 129 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 129...
  Q-head-2 SVD reconstruction error: 0.326143
  Q-head-2 SVD factorization completed in 0.02 seconds
  Using head offset 66048 for head 2 in W_B_q
  Query head 2 factorization error: 0.326143
  Factorizing query head 3/4...
  Using rank 129 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 129...
  Q-head-3 SVD reconstruction error: 0.446235
  Q-head-3 SVD factorization completed in 0.03 seconds
  Using head offset 99072 for head 3 in W_B_q
  Query head 3 factorization error: 0.446235
  Average query head factorization error: 0.374912 (37.49%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 212...
  K SVD reconstruction error: 0.223018
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 228...
  V SVD reconstruction error: 0.220064
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0033 seconds
  Head 0 reconstruction error: 0.266479 (26.65%)
  Head 1 reconstruction error: 0.460789 (46.08%)
  Head 2 reconstruction error: 0.326143 (32.61%)
  Head 3 reconstruction error: 0.446235 (44.62%)
Average Q head reconstruction error: 0.374912 (37.49%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.397344 (39.73%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.223018 (22.30%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.220064 (22.01%)

GQA to TPA conversion complete in 0.34 seconds
Final reconstruction errors:
  Q combined: 39.73%
  Q per-head average: 37.49%
  Per-head details: Head 0: 26.65%, Head 1: 46.08%, Head 2: 32.61%, Head 3: 44.62%
  K: 22.30%, V: 22.01%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [129, 129, 129, 129]
Used ranks: Q max=129, K=212, V=228
Total Q rank used: 516 (sum of per-head ranks)
  Decomposition completed in 0.34 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.34 seconds
Converting attention layer: model.layers.2.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.2.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 438
    95% energy: rank 553
    98% energy: rank 678
  K singular value analysis:
    90% energy: rank 182
    95% energy: rank 206
    98% energy: rank 226
  V singular value analysis:
    90% energy: rank 208
    95% energy: rank 229
    98% energy: rank 243

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=138, K=206, V=229
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [138, 138, 138, 138], using max=138 for W_B
  Created W_B_q with shape torch.Size([1152, 141312]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 138 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 138...
  Q-head-0 SVD reconstruction error: 0.441488
  Q-head-0 SVD factorization completed in 0.03 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.441488
  Factorizing query head 1/4...
  Using rank 138 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 138...
  Q-head-1 SVD reconstruction error: 0.441800
  Q-head-1 SVD factorization completed in 0.03 seconds
  Using head offset 35328 for head 1 in W_B_q
  Query head 1 factorization error: 0.441800
  Factorizing query head 2/4...
  Using rank 138 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 138...
  Q-head-2 SVD reconstruction error: 0.329779
  Q-head-2 SVD factorization completed in 0.03 seconds
  Using head offset 70656 for head 2 in W_B_q
  Query head 2 factorization error: 0.329779
  Factorizing query head 3/4...
  Using rank 138 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 138...
  Q-head-3 SVD reconstruction error: 0.387020
  Q-head-3 SVD factorization completed in 0.03 seconds
  Using head offset 105984 for head 3 in W_B_q
  Query head 3 factorization error: 0.387020
  Average query head factorization error: 0.400022 (40.00%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 206...
  K SVD reconstruction error: 0.223519
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 229...
  V SVD reconstruction error: 0.218653
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0035 seconds
  Head 0 reconstruction error: 0.441488 (44.15%)
  Head 1 reconstruction error: 0.441800 (44.18%)
  Head 2 reconstruction error: 0.329779 (32.98%)
  Head 3 reconstruction error: 0.387020 (38.70%)
Average Q head reconstruction error: 0.400022 (40.00%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.409387 (40.94%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.223519 (22.35%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.218653 (21.87%)

GQA to TPA conversion complete in 0.34 seconds
Final reconstruction errors:
  Q combined: 40.94%
  Q per-head average: 40.00%
  Per-head details: Head 0: 44.15%, Head 1: 44.18%, Head 2: 32.98%, Head 3: 38.70%
  K: 22.35%, V: 21.87%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [138, 138, 138, 138]
Used ranks: Q max=138, K=206, V=229
Total Q rank used: 552 (sum of per-head ranks)
  Decomposition completed in 0.34 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.34 seconds
Converting attention layer: model.layers.3.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.3.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 414
    95% energy: rank 531
    98% energy: rank 660
  K singular value analysis:
    90% energy: rank 166
    95% energy: rank 192
    98% energy: rank 216
  V singular value analysis:
    90% energy: rank 214
    95% energy: rank 233
    98% energy: rank 245

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=132, K=192, V=233
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [132, 132, 132, 132], using max=132 for W_B
  Created W_B_q with shape torch.Size([1152, 135168]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 132 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 132...
  Q-head-0 SVD reconstruction error: 0.414190
  Q-head-0 SVD factorization completed in 0.03 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.414190
  Factorizing query head 1/4...
  Using rank 132 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 132...
  Q-head-1 SVD reconstruction error: 0.335854
  Q-head-1 SVD factorization completed in 0.03 seconds
  Using head offset 33792 for head 1 in W_B_q
  Query head 1 factorization error: 0.335854
  Factorizing query head 2/4...
  Using rank 132 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 132...
  Q-head-2 SVD reconstruction error: 0.361550
  Q-head-2 SVD factorization completed in 0.03 seconds
  Using head offset 67584 for head 2 in W_B_q
  Query head 2 factorization error: 0.361550
  Factorizing query head 3/4...
  Using rank 132 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 132...
  Q-head-3 SVD reconstruction error: 0.376325
  Q-head-3 SVD factorization completed in 0.03 seconds
  Using head offset 101376 for head 3 in W_B_q
  Query head 3 factorization error: 0.376325
  Average query head factorization error: 0.371980 (37.20%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 192...
  K SVD reconstruction error: 0.222483
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 233...
  V SVD reconstruction error: 0.218667
  V SVD factorization completed in 0.04 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0036 seconds
  Head 0 reconstruction error: 0.414190 (41.42%)
  Head 1 reconstruction error: 0.335854 (33.59%)
  Head 2 reconstruction error: 0.361550 (36.16%)
  Head 3 reconstruction error: 0.376325 (37.63%)
Average Q head reconstruction error: 0.371980 (37.20%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.375590 (37.56%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.222483 (22.25%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.218667 (21.87%)

GQA to TPA conversion complete in 0.34 seconds
Final reconstruction errors:
  Q combined: 37.56%
  Q per-head average: 37.20%
  Per-head details: Head 0: 41.42%, Head 1: 33.59%, Head 2: 36.16%, Head 3: 37.63%
  K: 22.25%, V: 21.87%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [132, 132, 132, 132]
Used ranks: Q max=132, K=192, V=233
Total Q rank used: 528 (sum of per-head ranks)
  Decomposition completed in 0.34 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.34 seconds
Converting attention layer: model.layers.4.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.4.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 357
    95% energy: rank 477
    98% energy: rank 612
  K singular value analysis:
    90% energy: rank 159
    95% energy: rank 189
    98% energy: rank 217
  V singular value analysis:
    90% energy: rank 211
    95% energy: rank 230
    98% energy: rank 244

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=119, K=189, V=230
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [119, 119, 119, 119], using max=119 for W_B
  Created W_B_q with shape torch.Size([1152, 121856]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 119 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 119...
  Q-head-0 SVD reconstruction error: 0.399196
  Q-head-0 SVD factorization completed in 0.03 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.399196
  Factorizing query head 1/4...
  Using rank 119 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 119...
  Q-head-1 SVD reconstruction error: 0.385567
  Q-head-1 SVD factorization completed in 0.03 seconds
  Using head offset 30464 for head 1 in W_B_q
  Query head 1 factorization error: 0.385567
  Factorizing query head 2/4...
  Using rank 119 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 119...
  Q-head-2 SVD reconstruction error: 0.360195
  Q-head-2 SVD factorization completed in 0.03 seconds
  Using head offset 60928 for head 2 in W_B_q
  Query head 2 factorization error: 0.360195
  Factorizing query head 3/4...
  Using rank 119 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 119...
  Q-head-3 SVD reconstruction error: 0.296724
  Q-head-3 SVD factorization completed in 0.02 seconds
  Using head offset 91392 for head 3 in W_B_q
  Query head 3 factorization error: 0.296724
  Average query head factorization error: 0.360421 (36.04%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 189...
  K SVD reconstruction error: 0.222409
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 230...
  V SVD reconstruction error: 0.222124
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0033 seconds
  Head 0 reconstruction error: 0.399196 (39.92%)
  Head 1 reconstruction error: 0.385567 (38.56%)
  Head 2 reconstruction error: 0.360195 (36.02%)
  Head 3 reconstruction error: 0.296724 (29.67%)
Average Q head reconstruction error: 0.360421 (36.04%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.360296 (36.03%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.222409 (22.24%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.222124 (22.21%)

GQA to TPA conversion complete in 0.33 seconds
Final reconstruction errors:
  Q combined: 36.03%
  Q per-head average: 36.04%
  Per-head details: Head 0: 39.92%, Head 1: 38.56%, Head 2: 36.02%, Head 3: 29.67%
  K: 22.24%, V: 22.21%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [119, 119, 119, 119]
Used ranks: Q max=119, K=189, V=230
Total Q rank used: 476 (sum of per-head ranks)
  Decomposition completed in 0.33 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.33 seconds
Converting attention layer: model.layers.5.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.5.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 271
    95% energy: rank 375
    98% energy: rank 511
  K singular value analysis:
    90% energy: rank 132
    95% energy: rank 162
    98% energy: rank 193
  V singular value analysis:
    90% energy: rank 194
    95% energy: rank 219
    98% energy: rank 239

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=93, K=162, V=219
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [93, 93, 93, 93], using max=93 for W_B
  Created W_B_q with shape torch.Size([1152, 95232]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 93 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 93...
  Q-head-0 SVD reconstruction error: 0.374879
  Q-head-0 SVD factorization completed in 0.02 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.374879
  Factorizing query head 1/4...
  Using rank 93 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 93...
  Q-head-1 SVD reconstruction error: 0.441187
  Q-head-1 SVD factorization completed in 0.02 seconds
  Using head offset 23808 for head 1 in W_B_q
  Query head 1 factorization error: 0.441187
  Factorizing query head 2/4...
  Using rank 93 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 93...
  Q-head-2 SVD reconstruction error: 0.405360
  Q-head-2 SVD factorization completed in 0.02 seconds
  Using head offset 47616 for head 2 in W_B_q
  Query head 2 factorization error: 0.405360
  Factorizing query head 3/4...
  Using rank 93 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 93...
  Q-head-3 SVD reconstruction error: 0.416571
  Q-head-3 SVD factorization completed in 0.02 seconds
  Using head offset 71424 for head 3 in W_B_q
  Query head 3 factorization error: 0.416571
  Average query head factorization error: 0.409499 (40.95%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 162...
  K SVD reconstruction error: 0.222189
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 219...
  V SVD reconstruction error: 0.223605
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0027 seconds
  Head 0 reconstruction error: 0.374879 (37.49%)
  Head 1 reconstruction error: 0.441187 (44.12%)
  Head 2 reconstruction error: 0.405360 (40.54%)
  Head 3 reconstruction error: 0.416571 (41.66%)
Average Q head reconstruction error: 0.409499 (40.95%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.406313 (40.63%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.222189 (22.22%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.223605 (22.36%)

GQA to TPA conversion complete in 0.31 seconds
Final reconstruction errors:
  Q combined: 40.63%
  Q per-head average: 40.95%
  Per-head details: Head 0: 37.49%, Head 1: 44.12%, Head 2: 40.54%, Head 3: 41.66%
  K: 22.22%, V: 22.36%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [93, 93, 93, 93]
Used ranks: Q max=93, K=162, V=219
Total Q rank used: 372 (sum of per-head ranks)
  Decomposition completed in 0.31 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.31 seconds
Converting attention layer: model.layers.6.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.6.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 398
    95% energy: rank 515
    98% energy: rank 645
  K singular value analysis:
    90% energy: rank 158
    95% energy: rank 188
    98% energy: rank 215
  V singular value analysis:
    90% energy: rank 216
    95% energy: rank 235
    98% energy: rank 246

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=128, K=188, V=235
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [128, 128, 128, 128], using max=128 for W_B
  Created W_B_q with shape torch.Size([1152, 131072]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 128 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 128...
  Q-head-0 SVD reconstruction error: 0.382386
  Q-head-0 SVD factorization completed in 0.03 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.382386
  Factorizing query head 1/4...
  Using rank 128 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 128...
  Q-head-1 SVD reconstruction error: 0.335373
  Q-head-1 SVD factorization completed in 0.03 seconds
  Using head offset 32768 for head 1 in W_B_q
  Query head 1 factorization error: 0.335373
  Factorizing query head 2/4...
  Using rank 128 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 128...
  Q-head-2 SVD reconstruction error: 0.394190
  Q-head-2 SVD factorization completed in 0.03 seconds
  Using head offset 65536 for head 2 in W_B_q
  Query head 2 factorization error: 0.394190
  Factorizing query head 3/4...
  Using rank 128 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 128...
  Q-head-3 SVD reconstruction error: 0.405436
  Q-head-3 SVD factorization completed in 0.03 seconds
  Using head offset 98304 for head 3 in W_B_q
  Query head 3 factorization error: 0.405436
  Average query head factorization error: 0.379346 (37.93%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 188...
  K SVD reconstruction error: 0.220706
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 235...
  V SVD reconstruction error: 0.217838
  V SVD factorization completed in 0.04 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0032 seconds
  Head 0 reconstruction error: 0.382386 (38.24%)
  Head 1 reconstruction error: 0.335373 (33.54%)
  Head 2 reconstruction error: 0.394190 (39.42%)
  Head 3 reconstruction error: 0.405436 (40.54%)
Average Q head reconstruction error: 0.379346 (37.93%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.380159 (38.02%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.220706 (22.07%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.217838 (21.78%)

GQA to TPA conversion complete in 0.33 seconds
Final reconstruction errors:
  Q combined: 38.02%
  Q per-head average: 37.93%
  Per-head details: Head 0: 38.24%, Head 1: 33.54%, Head 2: 39.42%, Head 3: 40.54%
  K: 22.07%, V: 21.78%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [128, 128, 128, 128]
Used ranks: Q max=128, K=188, V=235
Total Q rank used: 512 (sum of per-head ranks)
  Decomposition completed in 0.33 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.34 seconds
Converting attention layer: model.layers.7.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.7.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 360
    95% energy: rank 478
    98% energy: rank 612
  K singular value analysis:
    90% energy: rank 154
    95% energy: rank 180
    98% energy: rank 205
  V singular value analysis:
    90% energy: rank 210
    95% energy: rank 230
    98% energy: rank 245

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=119, K=180, V=230
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [119, 119, 119, 119], using max=119 for W_B
  Created W_B_q with shape torch.Size([1152, 121856]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 119 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 119...
  Q-head-0 SVD reconstruction error: 0.375156
  Q-head-0 SVD factorization completed in 0.03 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.375156
  Factorizing query head 1/4...
  Using rank 119 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 119...
  Q-head-1 SVD reconstruction error: 0.393758
  Q-head-1 SVD factorization completed in 0.02 seconds
  Using head offset 30464 for head 1 in W_B_q
  Query head 1 factorization error: 0.393758
  Factorizing query head 2/4...
  Using rank 119 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 119...
  Q-head-2 SVD reconstruction error: 0.416077
  Q-head-2 SVD factorization completed in 0.03 seconds
  Using head offset 60928 for head 2 in W_B_q
  Query head 2 factorization error: 0.416077
  Factorizing query head 3/4...
  Using rank 119 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 119...
  Q-head-3 SVD reconstruction error: 0.488289
  Q-head-3 SVD factorization completed in 0.03 seconds
  Using head offset 91392 for head 3 in W_B_q
  Query head 3 factorization error: 0.488289
  Average query head factorization error: 0.418320 (41.83%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 180...
  K SVD reconstruction error: 0.220676
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 230...
  V SVD reconstruction error: 0.223215
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0033 seconds
  Head 0 reconstruction error: 0.375156 (37.52%)
  Head 1 reconstruction error: 0.393758 (39.38%)
  Head 2 reconstruction error: 0.416077 (41.61%)
  Head 3 reconstruction error: 0.488289 (48.83%)
Average Q head reconstruction error: 0.418320 (41.83%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.417479 (41.75%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.220676 (22.07%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.223215 (22.32%)

GQA to TPA conversion complete in 0.33 seconds
Final reconstruction errors:
  Q combined: 41.75%
  Q per-head average: 41.83%
  Per-head details: Head 0: 37.52%, Head 1: 39.38%, Head 2: 41.61%, Head 3: 48.83%
  K: 22.07%, V: 22.32%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [119, 119, 119, 119]
Used ranks: Q max=119, K=180, V=230
Total Q rank used: 476 (sum of per-head ranks)
  Decomposition completed in 0.33 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.33 seconds
Converting attention layer: model.layers.8.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.8.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 316
    95% energy: rank 434
    98% energy: rank 573
  K singular value analysis:
    90% energy: rank 132
    95% energy: rank 162
    98% energy: rank 192
  V singular value analysis:
    90% energy: rank 211
    95% energy: rank 231
    98% energy: rank 244

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=108, K=162, V=231
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [108, 108, 108, 108], using max=108 for W_B
  Created W_B_q with shape torch.Size([1152, 110592]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 108 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 108...
  Q-head-0 SVD reconstruction error: 0.358972
  Q-head-0 SVD factorization completed in 0.02 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.358972
  Factorizing query head 1/4...
  Using rank 108 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 108...
  Q-head-1 SVD reconstruction error: 0.375615
  Q-head-1 SVD factorization completed in 0.02 seconds
  Using head offset 27648 for head 1 in W_B_q
  Query head 1 factorization error: 0.375615
  Factorizing query head 2/4...
  Using rank 108 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 108...
  Q-head-2 SVD reconstruction error: 0.304121
  Q-head-2 SVD factorization completed in 0.02 seconds
  Using head offset 55296 for head 2 in W_B_q
  Query head 2 factorization error: 0.304121
  Factorizing query head 3/4...
  Using rank 108 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 108...
  Q-head-3 SVD reconstruction error: 0.353175
  Q-head-3 SVD factorization completed in 0.02 seconds
  Using head offset 82944 for head 3 in W_B_q
  Query head 3 factorization error: 0.353175
  Average query head factorization error: 0.347971 (34.80%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 162...
  K SVD reconstruction error: 0.221405
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 231...
  V SVD reconstruction error: 0.220046
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0035 seconds
  Head 0 reconstruction error: 0.358972 (35.90%)
  Head 1 reconstruction error: 0.375615 (37.56%)
  Head 2 reconstruction error: 0.304121 (30.41%)
  Head 3 reconstruction error: 0.353175 (35.32%)
Average Q head reconstruction error: 0.347971 (34.80%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.348829 (34.88%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.221405 (22.14%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.220046 (22.00%)

GQA to TPA conversion complete in 0.32 seconds
Final reconstruction errors:
  Q combined: 34.88%
  Q per-head average: 34.80%
  Per-head details: Head 0: 35.90%, Head 1: 37.56%, Head 2: 30.41%, Head 3: 35.32%
  K: 22.14%, V: 22.00%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [108, 108, 108, 108]
Used ranks: Q max=108, K=162, V=231
Total Q rank used: 432 (sum of per-head ranks)
  Decomposition completed in 0.32 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.32 seconds
Converting attention layer: model.layers.9.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.9.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 351
    95% energy: rank 467
    98% energy: rank 601
  K singular value analysis:
    90% energy: rank 141
    95% energy: rank 172
    98% energy: rank 203
  V singular value analysis:
    90% energy: rank 211
    95% energy: rank 231
    98% energy: rank 245

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=116, K=172, V=231
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [116, 116, 116, 116], using max=116 for W_B
  Created W_B_q with shape torch.Size([1152, 118784]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 116 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 116...
  Q-head-0 SVD reconstruction error: 0.369715
  Q-head-0 SVD factorization completed in 0.03 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.369715
  Factorizing query head 1/4...
  Using rank 116 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 116...
  Q-head-1 SVD reconstruction error: 0.402235
  Q-head-1 SVD factorization completed in 0.03 seconds
  Using head offset 29696 for head 1 in W_B_q
  Query head 1 factorization error: 0.402235
  Factorizing query head 2/4...
  Using rank 116 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 116...
  Q-head-2 SVD reconstruction error: 0.391415
  Q-head-2 SVD factorization completed in 0.03 seconds
  Using head offset 59392 for head 2 in W_B_q
  Query head 2 factorization error: 0.391415
  Factorizing query head 3/4...
  Using rank 116 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 116...
  Q-head-3 SVD reconstruction error: 0.371668
  Q-head-3 SVD factorization completed in 0.03 seconds
  Using head offset 89088 for head 3 in W_B_q
  Query head 3 factorization error: 0.371668
  Average query head factorization error: 0.383758 (38.38%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 172...
  K SVD reconstruction error: 0.222932
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 231...
  V SVD reconstruction error: 0.221468
  V SVD factorization completed in 0.04 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0036 seconds
  Head 0 reconstruction error: 0.369715 (36.97%)
  Head 1 reconstruction error: 0.402235 (40.22%)
  Head 2 reconstruction error: 0.391415 (39.14%)
  Head 3 reconstruction error: 0.371668 (37.17%)
Average Q head reconstruction error: 0.383758 (38.38%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.384862 (38.49%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.222932 (22.29%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.221468 (22.15%)

GQA to TPA conversion complete in 0.33 seconds
Final reconstruction errors:
  Q combined: 38.49%
  Q per-head average: 38.38%
  Per-head details: Head 0: 36.97%, Head 1: 40.22%, Head 2: 39.14%, Head 3: 37.17%
  K: 22.29%, V: 22.15%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [116, 116, 116, 116]
Used ranks: Q max=116, K=172, V=231
Total Q rank used: 464 (sum of per-head ranks)
  Decomposition completed in 0.33 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.33 seconds
Converting attention layer: model.layers.10.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.10.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 361
    95% energy: rank 479
    98% energy: rank 613
  K singular value analysis:
    90% energy: rank 148
    95% energy: rank 178
    98% energy: rank 208
  V singular value analysis:
    90% energy: rank 207
    95% energy: rank 229
    98% energy: rank 243

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=119, K=178, V=229
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [119, 119, 119, 119], using max=119 for W_B
  Created W_B_q with shape torch.Size([1152, 121856]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 119 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 119...
  Q-head-0 SVD reconstruction error: 0.438564
  Q-head-0 SVD factorization completed in 0.03 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.438564
  Factorizing query head 1/4...
  Using rank 119 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 119...
  Q-head-1 SVD reconstruction error: 0.332497
  Q-head-1 SVD factorization completed in 0.03 seconds
  Using head offset 30464 for head 1 in W_B_q
  Query head 1 factorization error: 0.332497
  Factorizing query head 2/4...
  Using rank 119 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 119...
  Q-head-2 SVD reconstruction error: 0.369723
  Q-head-2 SVD factorization completed in 0.03 seconds
  Using head offset 60928 for head 2 in W_B_q
  Query head 2 factorization error: 0.369723
  Factorizing query head 3/4...
  Using rank 119 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 119...
  Q-head-3 SVD reconstruction error: 0.360500
  Q-head-3 SVD factorization completed in 0.03 seconds
  Using head offset 91392 for head 3 in W_B_q
  Query head 3 factorization error: 0.360500
  Average query head factorization error: 0.375321 (37.53%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 178...
  K SVD reconstruction error: 0.222977
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 229...
  V SVD reconstruction error: 0.218634
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0035 seconds
  Head 0 reconstruction error: 0.438564 (43.86%)
  Head 1 reconstruction error: 0.332497 (33.25%)
  Head 2 reconstruction error: 0.369723 (36.97%)
  Head 3 reconstruction error: 0.360500 (36.05%)
Average Q head reconstruction error: 0.375321 (37.53%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.374076 (37.41%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.222977 (22.30%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.218634 (21.86%)

GQA to TPA conversion complete in 0.33 seconds
Final reconstruction errors:
  Q combined: 37.41%
  Q per-head average: 37.53%
  Per-head details: Head 0: 43.86%, Head 1: 33.25%, Head 2: 36.97%, Head 3: 36.05%
  K: 22.30%, V: 21.86%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [119, 119, 119, 119]
Used ranks: Q max=119, K=178, V=229
Total Q rank used: 476 (sum of per-head ranks)
  Decomposition completed in 0.33 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.33 seconds
Converting attention layer: model.layers.11.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.11.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 241
    95% energy: rank 341
    98% energy: rank 482
  K singular value analysis:
    90% energy: rank 119
    95% energy: rank 147
    98% energy: rank 176
  V singular value analysis:
    90% energy: rank 198
    95% energy: rank 223
    98% energy: rank 241

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=85, K=147, V=223
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [85, 85, 85, 85], using max=85 for W_B
  Created W_B_q with shape torch.Size([1152, 87040]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 85 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 85...
  Q-head-0 SVD reconstruction error: 0.342373
  Q-head-0 SVD factorization completed in 0.02 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.342373
  Factorizing query head 1/4...
  Using rank 85 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 85...
  Q-head-1 SVD reconstruction error: 0.374880
  Q-head-1 SVD factorization completed in 0.02 seconds
  Using head offset 21760 for head 1 in W_B_q
  Query head 1 factorization error: 0.374880
  Factorizing query head 2/4...
  Using rank 85 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 85...
  Q-head-2 SVD reconstruction error: 0.395894
  Q-head-2 SVD factorization completed in 0.02 seconds
  Using head offset 43520 for head 2 in W_B_q
  Query head 2 factorization error: 0.395894
  Factorizing query head 3/4...
  Using rank 85 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 85...
  Q-head-3 SVD reconstruction error: 0.289910
  Q-head-3 SVD factorization completed in 0.02 seconds
  Using head offset 65280 for head 3 in W_B_q
  Query head 3 factorization error: 0.289910
  Average query head factorization error: 0.350764 (35.08%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 147...
  K SVD reconstruction error: 0.222374
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 223...
  V SVD reconstruction error: 0.222065
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0026 seconds
  Head 0 reconstruction error: 0.342373 (34.24%)
  Head 1 reconstruction error: 0.374880 (37.49%)
  Head 2 reconstruction error: 0.395894 (39.59%)
  Head 3 reconstruction error: 0.289910 (28.99%)
Average Q head reconstruction error: 0.350764 (35.08%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.350162 (35.02%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.222374 (22.24%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.222065 (22.21%)

GQA to TPA conversion complete in 0.31 seconds
Final reconstruction errors:
  Q combined: 35.02%
  Q per-head average: 35.08%
  Per-head details: Head 0: 34.24%, Head 1: 37.49%, Head 2: 39.59%, Head 3: 28.99%
  K: 22.24%, V: 22.21%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [85, 85, 85, 85]
Used ranks: Q max=85, K=147, V=223
Total Q rank used: 340 (sum of per-head ranks)
  Decomposition completed in 0.31 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.31 seconds
Converting attention layer: model.layers.12.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.12.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 327
    95% energy: rank 445
    98% energy: rank 582
  K singular value analysis:
    90% energy: rank 143
    95% energy: rank 173
    98% energy: rank 204
  V singular value analysis:
    90% energy: rank 210
    95% energy: rank 230
    98% energy: rank 244

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=111, K=173, V=230
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [111, 111, 111, 111], using max=111 for W_B
  Created W_B_q with shape torch.Size([1152, 113664]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 111 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 111...
  Q-head-0 SVD reconstruction error: 0.362469
  Q-head-0 SVD factorization completed in 0.03 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.362469
  Factorizing query head 1/4...
  Using rank 111 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 111...
  Q-head-1 SVD reconstruction error: 0.370812
  Q-head-1 SVD factorization completed in 0.02 seconds
  Using head offset 28416 for head 1 in W_B_q
  Query head 1 factorization error: 0.370812
  Factorizing query head 2/4...
  Using rank 111 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 111...
  Q-head-2 SVD reconstruction error: 0.381902
  Q-head-2 SVD factorization completed in 0.02 seconds
  Using head offset 56832 for head 2 in W_B_q
  Query head 2 factorization error: 0.381902
  Factorizing query head 3/4...
  Using rank 111 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 111...
  Q-head-3 SVD reconstruction error: 0.379387
  Q-head-3 SVD factorization completed in 0.02 seconds
  Using head offset 85248 for head 3 in W_B_q
  Query head 3 factorization error: 0.379387
  Average query head factorization error: 0.373643 (37.36%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 173...
  K SVD reconstruction error: 0.222926
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 230...
  V SVD reconstruction error: 0.222330
  V SVD factorization completed in 0.04 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0029 seconds
  Head 0 reconstruction error: 0.362469 (36.25%)
  Head 1 reconstruction error: 0.370812 (37.08%)
  Head 2 reconstruction error: 0.381902 (38.19%)
  Head 3 reconstruction error: 0.379387 (37.94%)
Average Q head reconstruction error: 0.373643 (37.36%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.373147 (37.31%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.222926 (22.29%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.222330 (22.23%)

GQA to TPA conversion complete in 0.33 seconds
Final reconstruction errors:
  Q combined: 37.31%
  Q per-head average: 37.36%
  Per-head details: Head 0: 36.25%, Head 1: 37.08%, Head 2: 38.19%, Head 3: 37.94%
  K: 22.29%, V: 22.23%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [111, 111, 111, 111]
Used ranks: Q max=111, K=173, V=230
Total Q rank used: 444 (sum of per-head ranks)
  Decomposition completed in 0.33 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.33 seconds
Converting attention layer: model.layers.13.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.13.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 318
    95% energy: rank 437
    98% energy: rank 578
  K singular value analysis:
    90% energy: rank 136
    95% energy: rank 167
    98% energy: rank 199
  V singular value analysis:
    90% energy: rank 210
    95% energy: rank 230
    98% energy: rank 244

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=109, K=167, V=230
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [109, 109, 109, 109], using max=109 for W_B
  Created W_B_q with shape torch.Size([1152, 111616]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 109 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 109...
  Q-head-0 SVD reconstruction error: 0.373550
  Q-head-0 SVD factorization completed in 0.02 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.373550
  Factorizing query head 1/4...
  Using rank 109 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 109...
  Q-head-1 SVD reconstruction error: 0.370412
  Q-head-1 SVD factorization completed in 0.02 seconds
  Using head offset 27904 for head 1 in W_B_q
  Query head 1 factorization error: 0.370412
  Factorizing query head 2/4...
  Using rank 109 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 109...
  Q-head-2 SVD reconstruction error: 0.397177
  Q-head-2 SVD factorization completed in 0.02 seconds
  Using head offset 55808 for head 2 in W_B_q
  Query head 2 factorization error: 0.397177
  Factorizing query head 3/4...
  Using rank 109 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 109...
  Q-head-3 SVD reconstruction error: 0.382724
  Q-head-3 SVD factorization completed in 0.02 seconds
  Using head offset 83712 for head 3 in W_B_q
  Query head 3 factorization error: 0.382724
  Average query head factorization error: 0.380966 (38.10%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 167...
  K SVD reconstruction error: 0.222924
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 230...
  V SVD reconstruction error: 0.220350
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0034 seconds
  Head 0 reconstruction error: 0.373550 (37.36%)
  Head 1 reconstruction error: 0.370412 (37.04%)
  Head 2 reconstruction error: 0.397177 (39.72%)
  Head 3 reconstruction error: 0.382724 (38.27%)
Average Q head reconstruction error: 0.380966 (38.10%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.380605 (38.06%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.222924 (22.29%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.220350 (22.04%)

GQA to TPA conversion complete in 0.32 seconds
Final reconstruction errors:
  Q combined: 38.06%
  Q per-head average: 38.10%
  Per-head details: Head 0: 37.36%, Head 1: 37.04%, Head 2: 39.72%, Head 3: 38.27%
  K: 22.29%, V: 22.04%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [109, 109, 109, 109]
Used ranks: Q max=109, K=167, V=230
Total Q rank used: 436 (sum of per-head ranks)
  Decomposition completed in 0.32 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.32 seconds
Converting attention layer: model.layers.14.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.14.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 268
    95% energy: rank 373
    98% energy: rank 513
  K singular value analysis:
    90% energy: rank 129
    95% energy: rank 160
    98% energy: rank 194
  V singular value analysis:
    90% energy: rank 203
    95% energy: rank 226
    98% energy: rank 241

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=93, K=160, V=226
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [93, 93, 93, 93], using max=93 for W_B
  Created W_B_q with shape torch.Size([1152, 95232]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 93 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 93...
  Q-head-0 SVD reconstruction error: 0.421517
  Q-head-0 SVD factorization completed in 0.02 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.421517
  Factorizing query head 1/4...
  Using rank 93 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 93...
  Q-head-1 SVD reconstruction error: 0.410400
  Q-head-1 SVD factorization completed in 0.02 seconds
  Using head offset 23808 for head 1 in W_B_q
  Query head 1 factorization error: 0.410400
  Factorizing query head 2/4...
  Using rank 93 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 93...
  Q-head-2 SVD reconstruction error: 0.410542
  Q-head-2 SVD factorization completed in 0.02 seconds
  Using head offset 47616 for head 2 in W_B_q
  Query head 2 factorization error: 0.410542
  Factorizing query head 3/4...
  Using rank 93 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 93...
  Q-head-3 SVD reconstruction error: 0.393518
  Q-head-3 SVD factorization completed in 0.02 seconds
  Using head offset 71424 for head 3 in W_B_q
  Query head 3 factorization error: 0.393518
  Average query head factorization error: 0.408994 (40.90%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 160...
  K SVD reconstruction error: 0.222262
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 226...
  V SVD reconstruction error: 0.219388
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0030 seconds
  Head 0 reconstruction error: 0.421517 (42.15%)
  Head 1 reconstruction error: 0.410400 (41.04%)
  Head 2 reconstruction error: 0.410542 (41.05%)
  Head 3 reconstruction error: 0.393518 (39.35%)
Average Q head reconstruction error: 0.408994 (40.90%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.408389 (40.84%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.222262 (22.23%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.219388 (21.94%)

GQA to TPA conversion complete in 0.31 seconds
Final reconstruction errors:
  Q combined: 40.84%
  Q per-head average: 40.90%
  Per-head details: Head 0: 42.15%, Head 1: 41.04%, Head 2: 41.05%, Head 3: 39.35%
  K: 22.23%, V: 21.94%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [93, 93, 93, 93]
Used ranks: Q max=93, K=160, V=226
Total Q rank used: 372 (sum of per-head ranks)
  Decomposition completed in 0.31 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.31 seconds
Converting attention layer: model.layers.15.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.15.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 333
    95% energy: rank 453
    98% energy: rank 592
  K singular value analysis:
    90% energy: rank 143
    95% energy: rank 175
    98% energy: rank 207
  V singular value analysis:
    90% energy: rank 202
    95% energy: rank 224
    98% energy: rank 241

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=113, K=175, V=224
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [113, 113, 113, 113], using max=113 for W_B
  Created W_B_q with shape torch.Size([1152, 115712]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 113 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 113...
  Q-head-0 SVD reconstruction error: 0.368003
  Q-head-0 SVD factorization completed in 0.02 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.368003
  Factorizing query head 1/4...
  Using rank 113 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 113...
  Q-head-1 SVD reconstruction error: 0.328271
  Q-head-1 SVD factorization completed in 0.03 seconds
  Using head offset 28928 for head 1 in W_B_q
  Query head 1 factorization error: 0.328271
  Factorizing query head 2/4...
  Using rank 113 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 113...
  Q-head-2 SVD reconstruction error: 0.397216
  Q-head-2 SVD factorization completed in 0.02 seconds
  Using head offset 57856 for head 2 in W_B_q
  Query head 2 factorization error: 0.397216
  Factorizing query head 3/4...
  Using rank 113 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 113...
  Q-head-3 SVD reconstruction error: 0.307412
  Q-head-3 SVD factorization completed in 0.02 seconds
  Using head offset 86784 for head 3 in W_B_q
  Query head 3 factorization error: 0.307412
  Average query head factorization error: 0.350226 (35.02%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 175...
  K SVD reconstruction error: 0.221177
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 224...
  V SVD reconstruction error: 0.223100
  V SVD factorization completed in 0.04 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0034 seconds
  Head 0 reconstruction error: 0.368003 (36.80%)
  Head 1 reconstruction error: 0.328271 (32.83%)
  Head 2 reconstruction error: 0.397216 (39.72%)
  Head 3 reconstruction error: 0.307412 (30.74%)
Average Q head reconstruction error: 0.350226 (35.02%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.348163 (34.82%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.221177 (22.12%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.223100 (22.31%)

GQA to TPA conversion complete in 0.33 seconds
Final reconstruction errors:
  Q combined: 34.82%
  Q per-head average: 35.02%
  Per-head details: Head 0: 36.80%, Head 1: 32.83%, Head 2: 39.72%, Head 3: 30.74%
  K: 22.12%, V: 22.31%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [113, 113, 113, 113]
Used ranks: Q max=113, K=175, V=224
Total Q rank used: 452 (sum of per-head ranks)
  Decomposition completed in 0.33 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.33 seconds
Converting attention layer: model.layers.16.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.16.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 371
    95% energy: rank 489
    98% energy: rank 623
  K singular value analysis:
    90% energy: rank 157
    95% energy: rank 186
    98% energy: rank 213
  V singular value analysis:
    90% energy: rank 208
    95% energy: rank 229
    98% energy: rank 243

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=122, K=186, V=229
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [122, 122, 122, 122], using max=122 for W_B
  Created W_B_q with shape torch.Size([1152, 124928]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 122 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 122...
  Q-head-0 SVD reconstruction error: 0.407897
  Q-head-0 SVD factorization completed in 0.03 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.407897
  Factorizing query head 1/4...
  Using rank 122 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 122...
  Q-head-1 SVD reconstruction error: 0.359582
  Q-head-1 SVD factorization completed in 0.03 seconds
  Using head offset 31232 for head 1 in W_B_q
  Query head 1 factorization error: 0.359582
  Factorizing query head 2/4...
  Using rank 122 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 122...
  Q-head-2 SVD reconstruction error: 0.364651
  Q-head-2 SVD factorization completed in 0.03 seconds
  Using head offset 62464 for head 2 in W_B_q
  Query head 2 factorization error: 0.364651
  Factorizing query head 3/4...
  Using rank 122 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 122...
  Q-head-3 SVD reconstruction error: 0.386060
  Q-head-3 SVD factorization completed in 0.03 seconds
  Using head offset 93696 for head 3 in W_B_q
  Query head 3 factorization error: 0.386060
  Average query head factorization error: 0.379547 (37.95%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 186...
  K SVD reconstruction error: 0.221023
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 229...
  V SVD reconstruction error: 0.221194
  V SVD factorization completed in 0.04 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0035 seconds
  Head 0 reconstruction error: 0.407897 (40.79%)
  Head 1 reconstruction error: 0.359582 (35.96%)
  Head 2 reconstruction error: 0.364651 (36.47%)
  Head 3 reconstruction error: 0.386060 (38.61%)
Average Q head reconstruction error: 0.379547 (37.95%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.379433 (37.94%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.221023 (22.10%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.221194 (22.12%)

GQA to TPA conversion complete in 0.33 seconds
Final reconstruction errors:
  Q combined: 37.94%
  Q per-head average: 37.95%
  Per-head details: Head 0: 40.79%, Head 1: 35.96%, Head 2: 36.47%, Head 3: 38.61%
  K: 22.10%, V: 22.12%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [122, 122, 122, 122]
Used ranks: Q max=122, K=186, V=229
Total Q rank used: 488 (sum of per-head ranks)
  Decomposition completed in 0.33 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.33 seconds
Converting attention layer: model.layers.17.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.17.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 260
    95% energy: rank 355
    98% energy: rank 482
  K singular value analysis:
    90% energy: rank 95
    95% energy: rank 124
    98% energy: rank 159
  V singular value analysis:
    90% energy: rank 205
    95% energy: rank 226
    98% energy: rank 242

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=88, K=124, V=226
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [88, 88, 88, 88], using max=88 for W_B
  Created W_B_q with shape torch.Size([1152, 90112]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 88 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 88...
  Q-head-0 SVD reconstruction error: 0.418352
  Q-head-0 SVD factorization completed in 0.02 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.418352
  Factorizing query head 1/4...
  Using rank 88 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 88...
  Q-head-1 SVD reconstruction error: 0.358232
  Q-head-1 SVD factorization completed in 0.02 seconds
  Using head offset 22528 for head 1 in W_B_q
  Query head 1 factorization error: 0.358232
  Factorizing query head 2/4...
  Using rank 88 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 88...
  Q-head-2 SVD reconstruction error: 0.414704
  Q-head-2 SVD factorization completed in 0.02 seconds
  Using head offset 45056 for head 2 in W_B_q
  Query head 2 factorization error: 0.414704
  Factorizing query head 3/4...
  Using rank 88 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 88...
  Q-head-3 SVD reconstruction error: 0.377503
  Q-head-3 SVD factorization completed in 0.02 seconds
  Using head offset 67584 for head 3 in W_B_q
  Query head 3 factorization error: 0.377503
  Average query head factorization error: 0.392198 (39.22%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 124...
  K SVD reconstruction error: 0.221307
  K SVD factorization completed in 0.02 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 226...
  V SVD reconstruction error: 0.223171
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0026 seconds
  Head 0 reconstruction error: 0.418352 (41.84%)
  Head 1 reconstruction error: 0.358232 (35.82%)
  Head 2 reconstruction error: 0.414704 (41.47%)
  Head 3 reconstruction error: 0.377503 (37.75%)
Average Q head reconstruction error: 0.392198 (39.22%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.388795 (38.88%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.221306 (22.13%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.223171 (22.32%)

GQA to TPA conversion complete in 0.30 seconds
Final reconstruction errors:
  Q combined: 38.88%
  Q per-head average: 39.22%
  Per-head details: Head 0: 41.84%, Head 1: 35.82%, Head 2: 41.47%, Head 3: 37.75%
  K: 22.13%, V: 22.32%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [88, 88, 88, 88]
Used ranks: Q max=88, K=124, V=226
Total Q rank used: 352 (sum of per-head ranks)
  Decomposition completed in 0.30 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.30 seconds
Converting attention layer: model.layers.18.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.18.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 263
    95% energy: rank 368
    98% energy: rank 507
  K singular value analysis:
    90% energy: rank 117
    95% energy: rank 152
    98% energy: rank 184
  V singular value analysis:
    90% energy: rank 203
    95% energy: rank 226
    98% energy: rank 242

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=92, K=152, V=226
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [92, 92, 92, 92], using max=92 for W_B
  Created W_B_q with shape torch.Size([1152, 94208]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 92 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 92...
  Q-head-0 SVD reconstruction error: 0.383030
  Q-head-0 SVD factorization completed in 0.02 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.383030
  Factorizing query head 1/4...
  Using rank 92 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 92...
  Q-head-1 SVD reconstruction error: 0.424163
  Q-head-1 SVD factorization completed in 0.02 seconds
  Using head offset 23552 for head 1 in W_B_q
  Query head 1 factorization error: 0.424163
  Factorizing query head 2/4...
  Using rank 92 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 92...
  Q-head-2 SVD reconstruction error: 0.367057
  Q-head-2 SVD factorization completed in 0.02 seconds
  Using head offset 47104 for head 2 in W_B_q
  Query head 2 factorization error: 0.367057
  Factorizing query head 3/4...
  Using rank 92 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 92...
  Q-head-3 SVD reconstruction error: 0.375169
  Q-head-3 SVD factorization completed in 0.02 seconds
  Using head offset 70656 for head 3 in W_B_q
  Query head 3 factorization error: 0.375169
  Average query head factorization error: 0.387355 (38.74%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 152...
  K SVD reconstruction error: 0.221301
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 226...
  V SVD reconstruction error: 0.222407
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0027 seconds
  Head 0 reconstruction error: 0.383030 (38.30%)
  Head 1 reconstruction error: 0.424163 (42.42%)
  Head 2 reconstruction error: 0.367057 (36.71%)
  Head 3 reconstruction error: 0.375169 (37.52%)
Average Q head reconstruction error: 0.387355 (38.74%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.388698 (38.87%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.221301 (22.13%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.222407 (22.24%)

GQA to TPA conversion complete in 0.32 seconds
Final reconstruction errors:
  Q combined: 38.87%
  Q per-head average: 38.74%
  Per-head details: Head 0: 38.30%, Head 1: 42.42%, Head 2: 36.71%, Head 3: 37.52%
  K: 22.13%, V: 22.24%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [92, 92, 92, 92]
Used ranks: Q max=92, K=152, V=226
Total Q rank used: 368 (sum of per-head ranks)
  Decomposition completed in 0.32 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.32 seconds
Converting attention layer: model.layers.19.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.19.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 442
    95% energy: rank 555
    98% energy: rank 679
  K singular value analysis:
    90% energy: rank 181
    95% energy: rank 206
    98% energy: rank 227
  V singular value analysis:
    90% energy: rank 207
    95% energy: rank 227
    98% energy: rank 242

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=138, K=206, V=227
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [138, 138, 138, 138], using max=138 for W_B
  Created W_B_q with shape torch.Size([1152, 141312]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 138 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 138...
  Q-head-0 SVD reconstruction error: 0.435647
  Q-head-0 SVD factorization completed in 0.03 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.435647
  Factorizing query head 1/4...
  Using rank 138 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 138...
  Q-head-1 SVD reconstruction error: 0.423971
  Q-head-1 SVD factorization completed in 0.03 seconds
  Using head offset 35328 for head 1 in W_B_q
  Query head 1 factorization error: 0.423971
  Factorizing query head 2/4...
  Using rank 138 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 138...
  Q-head-2 SVD reconstruction error: 0.388609
  Q-head-2 SVD factorization completed in 0.03 seconds
  Using head offset 70656 for head 2 in W_B_q
  Query head 2 factorization error: 0.388609
  Factorizing query head 3/4...
  Using rank 138 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 138...
  Q-head-3 SVD reconstruction error: 0.403576
  Q-head-3 SVD factorization completed in 0.03 seconds
  Using head offset 105984 for head 3 in W_B_q
  Query head 3 factorization error: 0.403576
  Average query head factorization error: 0.412951 (41.30%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 206...
  K SVD reconstruction error: 0.221700
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 227...
  V SVD reconstruction error: 0.222872
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0034 seconds
  Head 0 reconstruction error: 0.435647 (43.56%)
  Head 1 reconstruction error: 0.423971 (42.40%)
  Head 2 reconstruction error: 0.388609 (38.86%)
  Head 3 reconstruction error: 0.403576 (40.36%)
Average Q head reconstruction error: 0.412951 (41.30%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.412336 (41.23%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.221700 (22.17%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.222872 (22.29%)

GQA to TPA conversion complete in 0.34 seconds
Final reconstruction errors:
  Q combined: 41.23%
  Q per-head average: 41.30%
  Per-head details: Head 0: 43.56%, Head 1: 42.40%, Head 2: 38.86%, Head 3: 40.36%
  K: 22.17%, V: 22.29%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [138, 138, 138, 138]
Used ranks: Q max=138, K=206, V=227
Total Q rank used: 552 (sum of per-head ranks)
  Decomposition completed in 0.34 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.34 seconds
Converting attention layer: model.layers.20.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.20.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 371
    95% energy: rank 493
    98% energy: rank 630
  K singular value analysis:
    90% energy: rank 152
    95% energy: rank 183
    98% energy: rank 213
  V singular value analysis:
    90% energy: rank 213
    95% energy: rank 232
    98% energy: rank 246

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=123, K=183, V=232
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [123, 123, 123, 123], using max=123 for W_B
  Created W_B_q with shape torch.Size([1152, 125952]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 123 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 123...
  Q-head-0 SVD reconstruction error: 0.407930
  Q-head-0 SVD factorization completed in 0.03 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.407930
  Factorizing query head 1/4...
  Using rank 123 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 123...
  Q-head-1 SVD reconstruction error: 0.388603
  Q-head-1 SVD factorization completed in 0.03 seconds
  Using head offset 31488 for head 1 in W_B_q
  Query head 1 factorization error: 0.388603
  Factorizing query head 2/4...
  Using rank 123 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 123...
  Q-head-2 SVD reconstruction error: 0.366414
  Q-head-2 SVD factorization completed in 0.03 seconds
  Using head offset 62976 for head 2 in W_B_q
  Query head 2 factorization error: 0.366414
  Factorizing query head 3/4...
  Using rank 123 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 123...
  Q-head-3 SVD reconstruction error: 0.334150
  Q-head-3 SVD factorization completed in 0.03 seconds
  Using head offset 94464 for head 3 in W_B_q
  Query head 3 factorization error: 0.334150
  Average query head factorization error: 0.374275 (37.43%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 183...
  K SVD reconstruction error: 0.223255
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 232...
  V SVD reconstruction error: 0.222373
  V SVD factorization completed in 0.04 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0037 seconds
  Head 0 reconstruction error: 0.407930 (40.79%)
  Head 1 reconstruction error: 0.388603 (38.86%)
  Head 2 reconstruction error: 0.366414 (36.64%)
  Head 3 reconstruction error: 0.334150 (33.42%)
Average Q head reconstruction error: 0.374275 (37.43%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.374717 (37.47%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.223255 (22.33%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.222373 (22.24%)

GQA to TPA conversion complete in 0.33 seconds
Final reconstruction errors:
  Q combined: 37.47%
  Q per-head average: 37.43%
  Per-head details: Head 0: 40.79%, Head 1: 38.86%, Head 2: 36.64%, Head 3: 33.42%
  K: 22.33%, V: 22.24%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [123, 123, 123, 123]
Used ranks: Q max=123, K=183, V=232
Total Q rank used: 492 (sum of per-head ranks)
  Decomposition completed in 0.33 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.33 seconds
Converting attention layer: model.layers.21.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.21.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 385
    95% energy: rank 506
    98% energy: rank 640
  K singular value analysis:
    90% energy: rank 161
    95% energy: rank 192
    98% energy: rank 220
  V singular value analysis:
    90% energy: rank 207
    95% energy: rank 228
    98% energy: rank 243

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=126, K=192, V=228
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [126, 126, 126, 126], using max=126 for W_B
  Created W_B_q with shape torch.Size([1152, 129024]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 126 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 126...
  Q-head-0 SVD reconstruction error: 0.362847
  Q-head-0 SVD factorization completed in 0.03 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.362847
  Factorizing query head 1/4...
  Using rank 126 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 126...
  Q-head-1 SVD reconstruction error: 0.330760
  Q-head-1 SVD factorization completed in 0.03 seconds
  Using head offset 32256 for head 1 in W_B_q
  Query head 1 factorization error: 0.330760
  Factorizing query head 2/4...
  Using rank 126 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 126...
  Q-head-2 SVD reconstruction error: 0.411263
  Q-head-2 SVD factorization completed in 0.03 seconds
  Using head offset 64512 for head 2 in W_B_q
  Query head 2 factorization error: 0.411263
  Factorizing query head 3/4...
  Using rank 126 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 126...
  Q-head-3 SVD reconstruction error: 0.394251
  Q-head-3 SVD factorization completed in 0.03 seconds
  Using head offset 96768 for head 3 in W_B_q
  Query head 3 factorization error: 0.394251
  Average query head factorization error: 0.374781 (37.48%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 192...
  K SVD reconstruction error: 0.221776
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 228...
  V SVD reconstruction error: 0.220245
  V SVD factorization completed in 0.04 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0038 seconds
  Head 0 reconstruction error: 0.362847 (36.28%)
  Head 1 reconstruction error: 0.330760 (33.08%)
  Head 2 reconstruction error: 0.411263 (41.13%)
  Head 3 reconstruction error: 0.394251 (39.43%)
Average Q head reconstruction error: 0.374781 (37.48%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.376979 (37.70%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.221776 (22.18%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.220245 (22.02%)

GQA to TPA conversion complete in 0.34 seconds
Final reconstruction errors:
  Q combined: 37.70%
  Q per-head average: 37.48%
  Per-head details: Head 0: 36.28%, Head 1: 33.08%, Head 2: 41.13%, Head 3: 39.43%
  K: 22.18%, V: 22.02%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [126, 126, 126, 126]
Used ranks: Q max=126, K=192, V=228
Total Q rank used: 504 (sum of per-head ranks)
  Decomposition completed in 0.34 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.34 seconds
Converting attention layer: model.layers.22.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.22.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 399
    95% energy: rank 522
    98% energy: rank 655
  K singular value analysis:
    90% energy: rank 176
    95% energy: rank 205
    98% energy: rank 228
  V singular value analysis:
    90% energy: rank 214
    95% energy: rank 233
    98% energy: rank 246

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=130, K=205, V=233
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [130, 130, 130, 130], using max=130 for W_B
  Created W_B_q with shape torch.Size([1152, 133120]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 130 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 130...
  Q-head-0 SVD reconstruction error: 0.289955
  Q-head-0 SVD factorization completed in 0.03 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.289955
  Factorizing query head 1/4...
  Using rank 130 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 130...
  Q-head-1 SVD reconstruction error: 0.405205
  Q-head-1 SVD factorization completed in 0.03 seconds
  Using head offset 33280 for head 1 in W_B_q
  Query head 1 factorization error: 0.405205
  Factorizing query head 2/4...
  Using rank 130 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 130...
  Q-head-2 SVD reconstruction error: 0.407301
  Q-head-2 SVD factorization completed in 0.03 seconds
  Using head offset 66560 for head 2 in W_B_q
  Query head 2 factorization error: 0.407301
  Factorizing query head 3/4...
  Using rank 130 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 130...
  Q-head-3 SVD reconstruction error: 0.336986
  Q-head-3 SVD factorization completed in 0.03 seconds
  Using head offset 99840 for head 3 in W_B_q
  Query head 3 factorization error: 0.336986
  Average query head factorization error: 0.359862 (35.99%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 205...
  K SVD reconstruction error: 0.220899
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 233...
  V SVD reconstruction error: 0.220961
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0034 seconds
  Head 0 reconstruction error: 0.289955 (29.00%)
  Head 1 reconstruction error: 0.405205 (40.52%)
  Head 2 reconstruction error: 0.407301 (40.73%)
  Head 3 reconstruction error: 0.336986 (33.70%)
Average Q head reconstruction error: 0.359862 (35.99%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.367053 (36.71%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.220899 (22.09%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.220961 (22.10%)

GQA to TPA conversion complete in 0.34 seconds
Final reconstruction errors:
  Q combined: 36.71%
  Q per-head average: 35.99%
  Per-head details: Head 0: 29.00%, Head 1: 40.52%, Head 2: 40.73%, Head 3: 33.70%
  K: 22.09%, V: 22.10%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [130, 130, 130, 130]
Used ranks: Q max=130, K=205, V=233
Total Q rank used: 520 (sum of per-head ranks)
  Decomposition completed in 0.34 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.34 seconds
Converting attention layer: model.layers.23.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.23.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 301
    95% energy: rank 417
    98% energy: rank 554
  K singular value analysis:
    90% energy: rank 137
    95% energy: rank 163
    98% energy: rank 189
  V singular value analysis:
    90% energy: rank 196
    95% energy: rank 222
    98% energy: rank 240

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=104, K=163, V=222
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [104, 104, 104, 104], using max=104 for W_B
  Created W_B_q with shape torch.Size([1152, 106496]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 104 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 104...
  Q-head-0 SVD reconstruction error: 0.361047
  Q-head-0 SVD factorization completed in 0.02 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.361047
  Factorizing query head 1/4...
  Using rank 104 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 104...
  Q-head-1 SVD reconstruction error: 0.355981
  Q-head-1 SVD factorization completed in 0.02 seconds
  Using head offset 26624 for head 1 in W_B_q
  Query head 1 factorization error: 0.355981
  Factorizing query head 2/4...
  Using rank 104 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 104...
  Q-head-2 SVD reconstruction error: 0.458179
  Q-head-2 SVD factorization completed in 0.02 seconds
  Using head offset 53248 for head 2 in W_B_q
  Query head 2 factorization error: 0.458179
  Factorizing query head 3/4...
  Using rank 104 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 104...
  Q-head-3 SVD reconstruction error: 0.360240
  Q-head-3 SVD factorization completed in 0.02 seconds
  Using head offset 79872 for head 3 in W_B_q
  Query head 3 factorization error: 0.360240
  Average query head factorization error: 0.383862 (38.39%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 163...
  K SVD reconstruction error: 0.221061
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 222...
  V SVD reconstruction error: 0.220424
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0032 seconds
  Head 0 reconstruction error: 0.361047 (36.10%)
  Head 1 reconstruction error: 0.355981 (35.60%)
  Head 2 reconstruction error: 0.458179 (45.82%)
  Head 3 reconstruction error: 0.360240 (36.02%)
Average Q head reconstruction error: 0.383862 (38.39%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.377712 (37.77%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.221061 (22.11%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.220424 (22.04%)

GQA to TPA conversion complete in 0.32 seconds
Final reconstruction errors:
  Q combined: 37.77%
  Q per-head average: 38.39%
  Per-head details: Head 0: 36.10%, Head 1: 35.60%, Head 2: 45.82%, Head 3: 36.02%
  K: 22.11%, V: 22.04%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [104, 104, 104, 104]
Used ranks: Q max=104, K=163, V=222
Total Q rank used: 416 (sum of per-head ranks)
  Decomposition completed in 0.32 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.32 seconds
Converting attention layer: model.layers.24.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.24.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 406
    95% energy: rank 527
    98% energy: rank 659
  K singular value analysis:
    90% energy: rank 163
    95% energy: rank 194
    98% energy: rank 221
  V singular value analysis:
    90% energy: rank 209
    95% energy: rank 229
    98% energy: rank 243

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=131, K=194, V=229
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [131, 131, 131, 131], using max=131 for W_B
  Created W_B_q with shape torch.Size([1152, 134144]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 131 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 131...
  Q-head-0 SVD reconstruction error: 0.366061
  Q-head-0 SVD factorization completed in 0.03 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.366061
  Factorizing query head 1/4...
  Using rank 131 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 131...
  Q-head-1 SVD reconstruction error: 0.374483
  Q-head-1 SVD factorization completed in 0.03 seconds
  Using head offset 33536 for head 1 in W_B_q
  Query head 1 factorization error: 0.374483
  Factorizing query head 2/4...
  Using rank 131 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 131...
  Q-head-2 SVD reconstruction error: 0.440481
  Q-head-2 SVD factorization completed in 0.03 seconds
  Using head offset 67072 for head 2 in W_B_q
  Query head 2 factorization error: 0.440481
  Factorizing query head 3/4...
  Using rank 131 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 131...
  Q-head-3 SVD reconstruction error: 0.354112
  Q-head-3 SVD factorization completed in 0.03 seconds
  Using head offset 100608 for head 3 in W_B_q
  Query head 3 factorization error: 0.354112
  Average query head factorization error: 0.383784 (38.38%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 194...
  K SVD reconstruction error: 0.221084
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 229...
  V SVD reconstruction error: 0.219000
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0031 seconds
  Head 0 reconstruction error: 0.366061 (36.61%)
  Head 1 reconstruction error: 0.374483 (37.45%)
  Head 2 reconstruction error: 0.440481 (44.05%)
  Head 3 reconstruction error: 0.354112 (35.41%)
Average Q head reconstruction error: 0.383784 (38.38%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.382132 (38.21%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.221084 (22.11%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.219000 (21.90%)

GQA to TPA conversion complete in 0.34 seconds
Final reconstruction errors:
  Q combined: 38.21%
  Q per-head average: 38.38%
  Per-head details: Head 0: 36.61%, Head 1: 37.45%, Head 2: 44.05%, Head 3: 35.41%
  K: 22.11%, V: 21.90%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [131, 131, 131, 131]
Used ranks: Q max=131, K=194, V=229
Total Q rank used: 524 (sum of per-head ranks)
  Decomposition completed in 0.34 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.34 seconds
Converting attention layer: model.layers.25.self_attn (type: combined_qkv)
  Inferred from config: 4 heads, 1 KV heads
  Using head_dim=256 from module attribute
  Split combined QKV projection: Q: torch.Size([1024, 1152]), K: torch.Size([256, 1152]), V: torch.Size([256, 1152])
  Starting tensor decomposition for layer model.layers.25.self_attn...
  Calculated dimensions: q_head_dim=288, kv_head_dim=1152
  Heads: q=4, kv=1
  WARNING: Different head dimensions for Q (288) and KV (1152)
  Using head_dim=288 for tensor decomposition
Starting improved GQA to TPA conversion with separate Q and K/V factorization...
After transposition: q=[torch.Size([1152, 1024])], k=[torch.Size([1152, 256])], v=[torch.Size([1152, 256])]
Dimensions: hidden_dim=1152
Weight shapes: Q=torch.Size([1152, 1024]), K=torch.Size([1152, 256]), V=torch.Size([1152, 256]), O=torch.Size([1152, 1024])

DIMENSION CALCULATION: Using separate head dimensions for Q and K/V
  Q weights: torch.Size([1152, 1024]) → 256 = 1024 / 4 heads
  K weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
  V weights: torch.Size([1152, 256]) → 256 = 256 / 1 kv_heads
Head mapping: 4 query heads to 1 KV heads, 4 query heads per KV head
Mapping: [0, 0, 0, 0]

ANALYZING INTRINSIC RANKS using SVD (separate analysis for Q and K/V)
  Q singular value analysis:
    90% energy: rank 333
    95% energy: rank 465
    98% energy: rank 612
  K singular value analysis:
    90% energy: rank 160
    95% energy: rank 193
    98% energy: rank 221
  V singular value analysis:
    90% energy: rank 205
    95% energy: rank 225
    98% energy: rank 240

Maximum possible ranks based on matrix dimensions: Q=1024, K=256, V=256
USING OPTIMAL COMPONENT-SPECIFIC RANKS: Q=116, K=193, V=225
These ranks are determined by energy-based analysis to balance accuracy and efficiency

Performing independent factorization for Q and K/V
Factorizing query projection with 4 heads, head_dim=256
  Q weight shape: torch.Size([1152, 1024]), total projection dim: 1024
  Per-head maximum ranks: [116, 116, 116, 116], using max=116 for W_B
  Created W_B_q with shape torch.Size([1152, 118784]) to accommodate 4 heads
  Factorizing query head 0/4...
  Using rank 116 for head 0 (limited by head dimensions)
  Computing SVD factorization for Q-head-0 with rank 116...
  Q-head-0 SVD reconstruction error: 0.375908
  Q-head-0 SVD factorization completed in 0.02 seconds
  Using head offset 0 for head 0 in W_B_q
  Query head 0 factorization error: 0.375908
  Factorizing query head 1/4...
  Using rank 116 for head 1 (limited by head dimensions)
  Computing SVD factorization for Q-head-1 with rank 116...
  Q-head-1 SVD reconstruction error: 0.353168
  Q-head-1 SVD factorization completed in 0.03 seconds
  Using head offset 29696 for head 1 in W_B_q
  Query head 1 factorization error: 0.353168
  Factorizing query head 2/4...
  Using rank 116 for head 2 (limited by head dimensions)
  Computing SVD factorization for Q-head-2 with rank 116...
  Q-head-2 SVD reconstruction error: 0.311528
  Q-head-2 SVD factorization completed in 0.03 seconds
  Using head offset 59392 for head 2 in W_B_q
  Query head 2 factorization error: 0.311528
  Factorizing query head 3/4...
  Using rank 116 for head 3 (limited by head dimensions)
  Computing SVD factorization for Q-head-3 with rank 116...
  Q-head-3 SVD reconstruction error: 0.332587
  Q-head-3 SVD factorization completed in 0.03 seconds
  Using head offset 89088 for head 3 in W_B_q
  Query head 3 factorization error: 0.332587
  Average query head factorization error: 0.343298 (34.33%)
Factorizing key projection with 1 KV heads, head_dim=256
  K weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for K with rank 193...
  K SVD reconstruction error: 0.220817
  K SVD factorization completed in 0.03 seconds
Factorizing value projection with 1 KV heads, head_dim=256
  V weight shape: torch.Size([1152, 256]), total projection dim: 256
  Computing SVD factorization for V with rank 225...
  V SVD reconstruction error: 0.220694
  V SVD factorization completed in 0.03 seconds

Verifying reconstruction quality of factorized weights

Verifying reconstruction quality of query weights PER HEAD:
  Beginning verification of 4 query heads with optimized batch operations...
  Using head-specific offsets in W_B_q for verification
  Completed 4 head verifications in 0.0035 seconds
  Head 0 reconstruction error: 0.375908 (37.59%)
  Head 1 reconstruction error: 0.353168 (35.32%)
  Head 2 reconstruction error: 0.311528 (31.15%)
  Head 3 reconstruction error: 0.332587 (33.26%)
Average Q head reconstruction error: 0.343298 (34.33%)

Verifying reconstruction quality of COMBINED factorized weights:
Q combined reconstruction error: 0.341997 (34.20%)
  Verifying K reconstruction...
  K overall reconstruction error: 0.220817 (22.08%)
  Verifying V reconstruction...
  V overall reconstruction error: 0.220694 (22.07%)

GQA to TPA conversion complete in 0.33 seconds
Final reconstruction errors:
  Q combined: 34.20%
  Q per-head average: 34.33%
  Per-head details: Head 0: 37.59%, Head 1: 35.32%, Head 2: 31.15%, Head 3: 33.26%
  K: 22.08%, V: 22.07%
Used head dimensions: Q=256, K=256, V=256
Used per-head ranks for Q: [116, 116, 116, 116]
Used ranks: Q max=116, K=193, V=225
Total Q rank used: 464 (sum of per-head ranks)
  Decomposition completed in 0.33 seconds
  Factorized weights keys: ['q_to_kv_mapping', 'q_head_dim', 'k_head_dim', 'v_head_dim', 'W_A_q', 'W_A_k', 'W_A_v', 'W_B_q', 'W_B_k', 'W_B_v', 'q_per_head_ranks', 'q_head_offsets', 'q_max_head_rank', 'q_rank', 'k_rank', 'v_rank']
  Applying factorized weights to module...
  Layer conversion took 0.33 seconds
GQA to TPA conversion complete: 26/26 layers converted in 8.78 seconds
Copying factorized TPA weights...
  Found factorized module: model.layers.0.self_attn
  Layer index: 0
  Recorded ranks for layer 0: {'q_rank': 134, 'k_rank': 202, 'v_rank': 220}
  Source W_A_q shape: torch.Size([1152, 536])
  Source W_A_q shape: torch.Size([1152, 536])
  Unknown B matrix with dimensions [out=536, in=1152]
  Creating W_A_q with in_features=1152, out_features=536
  W_A_q transposing weight from torch.Size([1152, 536]) to torch.Size([536, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([536, 1152])
  Source W_A_k shape: torch.Size([1152, 202])
  Source W_A_k shape: torch.Size([1152, 202])
  Unknown B matrix with dimensions [out=202, in=1152]
  Creating W_A_k with in_features=1152, out_features=202
  W_A_k transposing weight from torch.Size([1152, 202]) to torch.Size([202, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([202, 1152])
  Source W_A_v shape: torch.Size([1152, 220])
  Source W_A_v shape: torch.Size([1152, 220])
  Unknown B matrix with dimensions [out=220, in=1152]
  Creating W_A_v with in_features=1152, out_features=220
  W_A_v transposing weight from torch.Size([1152, 220]) to torch.Size([220, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([220, 1152])
  Source W_B_q shape: torch.Size([1152, 137216])
  Source W_B_q shape: torch.Size([1152, 137216])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 34304 = 134 * 256 (actual q_head_dim)
  (Original tensor dimension was: 137216)
  Creating W_B_q with in_features=1152, out_features=34304
  WARNING: Weight shape torch.Size([1152, 137216]) doesn't match required Linear dimensions [34304, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([137216, 1152])
  Need to resize W_B_q from 137216 to 34304
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([34304, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([34304, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([34304, 1152])
  Source W_B_k shape: torch.Size([1152, 51712])
  Source W_B_k shape: torch.Size([1152, 51712])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 51712 = 202 * 256 (actual k_head_dim)
  (Original tensor dimension was: 51712)
  Creating W_B_k with in_features=1152, out_features=51712
  W_B_k transposing weight from torch.Size([1152, 51712]) to torch.Size([51712, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([51712, 1152])
  Source W_B_v shape: torch.Size([1152, 56320])
  Source W_B_v shape: torch.Size([1152, 56320])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 56320 = 220 * 256 (actual v_head_dim)
  (Original tensor dimension was: 56320)
  Creating W_B_v with in_features=1152, out_features=56320
  W_B_v transposing weight from torch.Size([1152, 56320]) to torch.Size([56320, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([56320, 1152])
  Found factorized module: model.layers.1.self_attn
  Layer index: 1
  Recorded ranks for layer 1: {'q_rank': 129, 'k_rank': 212, 'v_rank': 228}
  Source W_A_q shape: torch.Size([1152, 516])
  Source W_A_q shape: torch.Size([1152, 516])
  Unknown B matrix with dimensions [out=516, in=1152]
  Creating W_A_q with in_features=1152, out_features=516
  W_A_q transposing weight from torch.Size([1152, 516]) to torch.Size([516, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([516, 1152])
  Source W_A_k shape: torch.Size([1152, 212])
  Source W_A_k shape: torch.Size([1152, 212])
  Unknown B matrix with dimensions [out=212, in=1152]
  Creating W_A_k with in_features=1152, out_features=212
  W_A_k transposing weight from torch.Size([1152, 212]) to torch.Size([212, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([212, 1152])
  Source W_A_v shape: torch.Size([1152, 228])
  Source W_A_v shape: torch.Size([1152, 228])
  Unknown B matrix with dimensions [out=228, in=1152]
  Creating W_A_v with in_features=1152, out_features=228
  W_A_v transposing weight from torch.Size([1152, 228]) to torch.Size([228, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([228, 1152])
  Source W_B_q shape: torch.Size([1152, 132096])
  Source W_B_q shape: torch.Size([1152, 132096])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 33024 = 129 * 256 (actual q_head_dim)
  (Original tensor dimension was: 132096)
  Creating W_B_q with in_features=1152, out_features=33024
  WARNING: Weight shape torch.Size([1152, 132096]) doesn't match required Linear dimensions [33024, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([132096, 1152])
  Need to resize W_B_q from 132096 to 33024
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([33024, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([33024, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([33024, 1152])
  Source W_B_k shape: torch.Size([1152, 54272])
  Source W_B_k shape: torch.Size([1152, 54272])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 54272 = 212 * 256 (actual k_head_dim)
  (Original tensor dimension was: 54272)
  Creating W_B_k with in_features=1152, out_features=54272
  W_B_k transposing weight from torch.Size([1152, 54272]) to torch.Size([54272, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([54272, 1152])
  Source W_B_v shape: torch.Size([1152, 58368])
  Source W_B_v shape: torch.Size([1152, 58368])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 58368 = 228 * 256 (actual v_head_dim)
  (Original tensor dimension was: 58368)
  Creating W_B_v with in_features=1152, out_features=58368
  W_B_v transposing weight from torch.Size([1152, 58368]) to torch.Size([58368, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([58368, 1152])
  Found factorized module: model.layers.2.self_attn
  Layer index: 2
  Recorded ranks for layer 2: {'q_rank': 138, 'k_rank': 206, 'v_rank': 229}
  Source W_A_q shape: torch.Size([1152, 552])
  Source W_A_q shape: torch.Size([1152, 552])
  Unknown B matrix with dimensions [out=552, in=1152]
  Creating W_A_q with in_features=1152, out_features=552
  W_A_q transposing weight from torch.Size([1152, 552]) to torch.Size([552, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([552, 1152])
  Source W_A_k shape: torch.Size([1152, 206])
  Source W_A_k shape: torch.Size([1152, 206])
  Unknown B matrix with dimensions [out=206, in=1152]
  Creating W_A_k with in_features=1152, out_features=206
  W_A_k transposing weight from torch.Size([1152, 206]) to torch.Size([206, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([206, 1152])
  Source W_A_v shape: torch.Size([1152, 229])
  Source W_A_v shape: torch.Size([1152, 229])
  Unknown B matrix with dimensions [out=229, in=1152]
  Creating W_A_v with in_features=1152, out_features=229
  W_A_v transposing weight from torch.Size([1152, 229]) to torch.Size([229, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([229, 1152])
  Source W_B_q shape: torch.Size([1152, 141312])
  Source W_B_q shape: torch.Size([1152, 141312])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 35328 = 138 * 256 (actual q_head_dim)
  (Original tensor dimension was: 141312)
  Creating W_B_q with in_features=1152, out_features=35328
  WARNING: Weight shape torch.Size([1152, 141312]) doesn't match required Linear dimensions [35328, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([141312, 1152])
  Need to resize W_B_q from 141312 to 35328
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([35328, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([35328, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([35328, 1152])
  Source W_B_k shape: torch.Size([1152, 52736])
  Source W_B_k shape: torch.Size([1152, 52736])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 52736 = 206 * 256 (actual k_head_dim)
  (Original tensor dimension was: 52736)
  Creating W_B_k with in_features=1152, out_features=52736
  W_B_k transposing weight from torch.Size([1152, 52736]) to torch.Size([52736, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([52736, 1152])
  Source W_B_v shape: torch.Size([1152, 58624])
  Source W_B_v shape: torch.Size([1152, 58624])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 58624 = 229 * 256 (actual v_head_dim)
  (Original tensor dimension was: 58624)
  Creating W_B_v with in_features=1152, out_features=58624
  W_B_v transposing weight from torch.Size([1152, 58624]) to torch.Size([58624, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([58624, 1152])
  Found factorized module: model.layers.3.self_attn
  Layer index: 3
  Recorded ranks for layer 3: {'q_rank': 132, 'k_rank': 192, 'v_rank': 233}
  Source W_A_q shape: torch.Size([1152, 528])
  Source W_A_q shape: torch.Size([1152, 528])
  Unknown B matrix with dimensions [out=528, in=1152]
  Creating W_A_q with in_features=1152, out_features=528
  W_A_q transposing weight from torch.Size([1152, 528]) to torch.Size([528, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([528, 1152])
  Source W_A_k shape: torch.Size([1152, 192])
  Source W_A_k shape: torch.Size([1152, 192])
  Unknown B matrix with dimensions [out=192, in=1152]
  Creating W_A_k with in_features=1152, out_features=192
  W_A_k transposing weight from torch.Size([1152, 192]) to torch.Size([192, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([192, 1152])
  Source W_A_v shape: torch.Size([1152, 233])
  Source W_A_v shape: torch.Size([1152, 233])
  Unknown B matrix with dimensions [out=233, in=1152]
  Creating W_A_v with in_features=1152, out_features=233
  W_A_v transposing weight from torch.Size([1152, 233]) to torch.Size([233, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([233, 1152])
  Source W_B_q shape: torch.Size([1152, 135168])
  Source W_B_q shape: torch.Size([1152, 135168])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 33792 = 132 * 256 (actual q_head_dim)
  (Original tensor dimension was: 135168)
  Creating W_B_q with in_features=1152, out_features=33792
  WARNING: Weight shape torch.Size([1152, 135168]) doesn't match required Linear dimensions [33792, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([135168, 1152])
  Need to resize W_B_q from 135168 to 33792
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([33792, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([33792, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([33792, 1152])
  Source W_B_k shape: torch.Size([1152, 49152])
  Source W_B_k shape: torch.Size([1152, 49152])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 49152 = 192 * 256 (actual k_head_dim)
  (Original tensor dimension was: 49152)
  Creating W_B_k with in_features=1152, out_features=49152
  W_B_k transposing weight from torch.Size([1152, 49152]) to torch.Size([49152, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([49152, 1152])
  Source W_B_v shape: torch.Size([1152, 59648])
  Source W_B_v shape: torch.Size([1152, 59648])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 59648 = 233 * 256 (actual v_head_dim)
  (Original tensor dimension was: 59648)
  Creating W_B_v with in_features=1152, out_features=59648
  W_B_v transposing weight from torch.Size([1152, 59648]) to torch.Size([59648, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([59648, 1152])
  Found factorized module: model.layers.4.self_attn
  Layer index: 4
  Recorded ranks for layer 4: {'q_rank': 119, 'k_rank': 189, 'v_rank': 230}
  Source W_A_q shape: torch.Size([1152, 476])
  Source W_A_q shape: torch.Size([1152, 476])
  Unknown B matrix with dimensions [out=476, in=1152]
  Creating W_A_q with in_features=1152, out_features=476
  W_A_q transposing weight from torch.Size([1152, 476]) to torch.Size([476, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([476, 1152])
  Source W_A_k shape: torch.Size([1152, 189])
  Source W_A_k shape: torch.Size([1152, 189])
  Unknown B matrix with dimensions [out=189, in=1152]
  Creating W_A_k with in_features=1152, out_features=189
  W_A_k transposing weight from torch.Size([1152, 189]) to torch.Size([189, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([189, 1152])
  Source W_A_v shape: torch.Size([1152, 230])
  Source W_A_v shape: torch.Size([1152, 230])
  Unknown B matrix with dimensions [out=230, in=1152]
  Creating W_A_v with in_features=1152, out_features=230
  W_A_v transposing weight from torch.Size([1152, 230]) to torch.Size([230, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([230, 1152])
  Source W_B_q shape: torch.Size([1152, 121856])
  Source W_B_q shape: torch.Size([1152, 121856])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 30464 = 119 * 256 (actual q_head_dim)
  (Original tensor dimension was: 121856)
  Creating W_B_q with in_features=1152, out_features=30464
  WARNING: Weight shape torch.Size([1152, 121856]) doesn't match required Linear dimensions [30464, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([121856, 1152])
  Need to resize W_B_q from 121856 to 30464
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([30464, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([30464, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([30464, 1152])
  Source W_B_k shape: torch.Size([1152, 48384])
  Source W_B_k shape: torch.Size([1152, 48384])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 48384 = 189 * 256 (actual k_head_dim)
  (Original tensor dimension was: 48384)
  Creating W_B_k with in_features=1152, out_features=48384
  W_B_k transposing weight from torch.Size([1152, 48384]) to torch.Size([48384, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([48384, 1152])
  Source W_B_v shape: torch.Size([1152, 58880])
  Source W_B_v shape: torch.Size([1152, 58880])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 58880 = 230 * 256 (actual v_head_dim)
  (Original tensor dimension was: 58880)
  Creating W_B_v with in_features=1152, out_features=58880
  W_B_v transposing weight from torch.Size([1152, 58880]) to torch.Size([58880, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([58880, 1152])
  Found factorized module: model.layers.5.self_attn
  Layer index: 5
  Recorded ranks for layer 5: {'q_rank': 93, 'k_rank': 162, 'v_rank': 219}
  Source W_A_q shape: torch.Size([1152, 372])
  Source W_A_q shape: torch.Size([1152, 372])
  Unknown B matrix with dimensions [out=372, in=1152]
  Creating W_A_q with in_features=1152, out_features=372
  W_A_q transposing weight from torch.Size([1152, 372]) to torch.Size([372, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([372, 1152])
  Source W_A_k shape: torch.Size([1152, 162])
  Source W_A_k shape: torch.Size([1152, 162])
  Unknown B matrix with dimensions [out=162, in=1152]
  Creating W_A_k with in_features=1152, out_features=162
  W_A_k transposing weight from torch.Size([1152, 162]) to torch.Size([162, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([162, 1152])
  Source W_A_v shape: torch.Size([1152, 219])
  Source W_A_v shape: torch.Size([1152, 219])
  Unknown B matrix with dimensions [out=219, in=1152]
  Creating W_A_v with in_features=1152, out_features=219
  W_A_v transposing weight from torch.Size([1152, 219]) to torch.Size([219, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([219, 1152])
  Source W_B_q shape: torch.Size([1152, 95232])
  Source W_B_q shape: torch.Size([1152, 95232])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 23808 = 93 * 256 (actual q_head_dim)
  (Original tensor dimension was: 95232)
  Creating W_B_q with in_features=1152, out_features=23808
  WARNING: Weight shape torch.Size([1152, 95232]) doesn't match required Linear dimensions [23808, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([95232, 1152])
  Need to resize W_B_q from 95232 to 23808
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([23808, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([23808, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([23808, 1152])
  Source W_B_k shape: torch.Size([1152, 41472])
  Source W_B_k shape: torch.Size([1152, 41472])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 41472 = 162 * 256 (actual k_head_dim)
  (Original tensor dimension was: 41472)
  Creating W_B_k with in_features=1152, out_features=41472
  W_B_k transposing weight from torch.Size([1152, 41472]) to torch.Size([41472, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([41472, 1152])
  Source W_B_v shape: torch.Size([1152, 56064])
  Source W_B_v shape: torch.Size([1152, 56064])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 56064 = 219 * 256 (actual v_head_dim)
  (Original tensor dimension was: 56064)
  Creating W_B_v with in_features=1152, out_features=56064
  W_B_v transposing weight from torch.Size([1152, 56064]) to torch.Size([56064, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([56064, 1152])
  Found factorized module: model.layers.6.self_attn
  Layer index: 6
  Recorded ranks for layer 6: {'q_rank': 128, 'k_rank': 188, 'v_rank': 235}
  Source W_A_q shape: torch.Size([1152, 512])
  Source W_A_q shape: torch.Size([1152, 512])
  Unknown B matrix with dimensions [out=512, in=1152]
  Creating W_A_q with in_features=1152, out_features=512
  W_A_q transposing weight from torch.Size([1152, 512]) to torch.Size([512, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([512, 1152])
  Source W_A_k shape: torch.Size([1152, 188])
  Source W_A_k shape: torch.Size([1152, 188])
  Unknown B matrix with dimensions [out=188, in=1152]
  Creating W_A_k with in_features=1152, out_features=188
  W_A_k transposing weight from torch.Size([1152, 188]) to torch.Size([188, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([188, 1152])
  Source W_A_v shape: torch.Size([1152, 235])
  Source W_A_v shape: torch.Size([1152, 235])
  Unknown B matrix with dimensions [out=235, in=1152]
  Creating W_A_v with in_features=1152, out_features=235
  W_A_v transposing weight from torch.Size([1152, 235]) to torch.Size([235, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([235, 1152])
  Source W_B_q shape: torch.Size([1152, 131072])
  Source W_B_q shape: torch.Size([1152, 131072])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 32768 = 128 * 256 (actual q_head_dim)
  (Original tensor dimension was: 131072)
  Creating W_B_q with in_features=1152, out_features=32768
  WARNING: Weight shape torch.Size([1152, 131072]) doesn't match required Linear dimensions [32768, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([131072, 1152])
  Need to resize W_B_q from 131072 to 32768
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([32768, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([32768, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([32768, 1152])
  Source W_B_k shape: torch.Size([1152, 48128])
  Source W_B_k shape: torch.Size([1152, 48128])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 48128 = 188 * 256 (actual k_head_dim)
  (Original tensor dimension was: 48128)
  Creating W_B_k with in_features=1152, out_features=48128
  W_B_k transposing weight from torch.Size([1152, 48128]) to torch.Size([48128, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([48128, 1152])
  Source W_B_v shape: torch.Size([1152, 60160])
  Source W_B_v shape: torch.Size([1152, 60160])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 60160 = 235 * 256 (actual v_head_dim)
  (Original tensor dimension was: 60160)
  Creating W_B_v with in_features=1152, out_features=60160
  W_B_v transposing weight from torch.Size([1152, 60160]) to torch.Size([60160, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([60160, 1152])
  Found factorized module: model.layers.7.self_attn
  Layer index: 7
  Recorded ranks for layer 7: {'q_rank': 119, 'k_rank': 180, 'v_rank': 230}
  Source W_A_q shape: torch.Size([1152, 476])
  Source W_A_q shape: torch.Size([1152, 476])
  Unknown B matrix with dimensions [out=476, in=1152]
  Creating W_A_q with in_features=1152, out_features=476
  W_A_q transposing weight from torch.Size([1152, 476]) to torch.Size([476, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([476, 1152])
  Source W_A_k shape: torch.Size([1152, 180])
  Source W_A_k shape: torch.Size([1152, 180])
  Unknown B matrix with dimensions [out=180, in=1152]
  Creating W_A_k with in_features=1152, out_features=180
  W_A_k transposing weight from torch.Size([1152, 180]) to torch.Size([180, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([180, 1152])
  Source W_A_v shape: torch.Size([1152, 230])
  Source W_A_v shape: torch.Size([1152, 230])
  Unknown B matrix with dimensions [out=230, in=1152]
  Creating W_A_v with in_features=1152, out_features=230
  W_A_v transposing weight from torch.Size([1152, 230]) to torch.Size([230, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([230, 1152])
  Source W_B_q shape: torch.Size([1152, 121856])
  Source W_B_q shape: torch.Size([1152, 121856])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 30464 = 119 * 256 (actual q_head_dim)
  (Original tensor dimension was: 121856)
  Creating W_B_q with in_features=1152, out_features=30464
  WARNING: Weight shape torch.Size([1152, 121856]) doesn't match required Linear dimensions [30464, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([121856, 1152])
  Need to resize W_B_q from 121856 to 30464
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([30464, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([30464, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([30464, 1152])
  Source W_B_k shape: torch.Size([1152, 46080])
  Source W_B_k shape: torch.Size([1152, 46080])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 46080 = 180 * 256 (actual k_head_dim)
  (Original tensor dimension was: 46080)
  Creating W_B_k with in_features=1152, out_features=46080
  W_B_k transposing weight from torch.Size([1152, 46080]) to torch.Size([46080, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([46080, 1152])
  Source W_B_v shape: torch.Size([1152, 58880])
  Source W_B_v shape: torch.Size([1152, 58880])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 58880 = 230 * 256 (actual v_head_dim)
  (Original tensor dimension was: 58880)
  Creating W_B_v with in_features=1152, out_features=58880
  W_B_v transposing weight from torch.Size([1152, 58880]) to torch.Size([58880, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([58880, 1152])
  Found factorized module: model.layers.8.self_attn
  Layer index: 8
  Recorded ranks for layer 8: {'q_rank': 108, 'k_rank': 162, 'v_rank': 231}
  Source W_A_q shape: torch.Size([1152, 432])
  Source W_A_q shape: torch.Size([1152, 432])
  Unknown B matrix with dimensions [out=432, in=1152]
  Creating W_A_q with in_features=1152, out_features=432
  W_A_q transposing weight from torch.Size([1152, 432]) to torch.Size([432, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([432, 1152])
  Source W_A_k shape: torch.Size([1152, 162])
  Source W_A_k shape: torch.Size([1152, 162])
  Unknown B matrix with dimensions [out=162, in=1152]
  Creating W_A_k with in_features=1152, out_features=162
  W_A_k transposing weight from torch.Size([1152, 162]) to torch.Size([162, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([162, 1152])
  Source W_A_v shape: torch.Size([1152, 231])
  Source W_A_v shape: torch.Size([1152, 231])
  Unknown B matrix with dimensions [out=231, in=1152]
  Creating W_A_v with in_features=1152, out_features=231
  W_A_v transposing weight from torch.Size([1152, 231]) to torch.Size([231, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([231, 1152])
  Source W_B_q shape: torch.Size([1152, 110592])
  Source W_B_q shape: torch.Size([1152, 110592])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 27648 = 108 * 256 (actual q_head_dim)
  (Original tensor dimension was: 110592)
  Creating W_B_q with in_features=1152, out_features=27648
  WARNING: Weight shape torch.Size([1152, 110592]) doesn't match required Linear dimensions [27648, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([110592, 1152])
  Need to resize W_B_q from 110592 to 27648
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([27648, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([27648, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([27648, 1152])
  Source W_B_k shape: torch.Size([1152, 41472])
  Source W_B_k shape: torch.Size([1152, 41472])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 41472 = 162 * 256 (actual k_head_dim)
  (Original tensor dimension was: 41472)
  Creating W_B_k with in_features=1152, out_features=41472
  W_B_k transposing weight from torch.Size([1152, 41472]) to torch.Size([41472, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([41472, 1152])
  Source W_B_v shape: torch.Size([1152, 59136])
  Source W_B_v shape: torch.Size([1152, 59136])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 59136 = 231 * 256 (actual v_head_dim)
  (Original tensor dimension was: 59136)
  Creating W_B_v with in_features=1152, out_features=59136
  W_B_v transposing weight from torch.Size([1152, 59136]) to torch.Size([59136, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([59136, 1152])
  Found factorized module: model.layers.9.self_attn
  Layer index: 9
  Recorded ranks for layer 9: {'q_rank': 116, 'k_rank': 172, 'v_rank': 231}
  Source W_A_q shape: torch.Size([1152, 464])
  Source W_A_q shape: torch.Size([1152, 464])
  Unknown B matrix with dimensions [out=464, in=1152]
  Creating W_A_q with in_features=1152, out_features=464
  W_A_q transposing weight from torch.Size([1152, 464]) to torch.Size([464, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([464, 1152])
  Source W_A_k shape: torch.Size([1152, 172])
  Source W_A_k shape: torch.Size([1152, 172])
  Unknown B matrix with dimensions [out=172, in=1152]
  Creating W_A_k with in_features=1152, out_features=172
  W_A_k transposing weight from torch.Size([1152, 172]) to torch.Size([172, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([172, 1152])
  Source W_A_v shape: torch.Size([1152, 231])
  Source W_A_v shape: torch.Size([1152, 231])
  Unknown B matrix with dimensions [out=231, in=1152]
  Creating W_A_v with in_features=1152, out_features=231
  W_A_v transposing weight from torch.Size([1152, 231]) to torch.Size([231, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([231, 1152])
  Source W_B_q shape: torch.Size([1152, 118784])
  Source W_B_q shape: torch.Size([1152, 118784])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 29696 = 116 * 256 (actual q_head_dim)
  (Original tensor dimension was: 118784)
  Creating W_B_q with in_features=1152, out_features=29696
  WARNING: Weight shape torch.Size([1152, 118784]) doesn't match required Linear dimensions [29696, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([118784, 1152])
  Need to resize W_B_q from 118784 to 29696
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([29696, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([29696, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([29696, 1152])
  Source W_B_k shape: torch.Size([1152, 44032])
  Source W_B_k shape: torch.Size([1152, 44032])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 44032 = 172 * 256 (actual k_head_dim)
  (Original tensor dimension was: 44032)
  Creating W_B_k with in_features=1152, out_features=44032
  W_B_k transposing weight from torch.Size([1152, 44032]) to torch.Size([44032, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([44032, 1152])
  Source W_B_v shape: torch.Size([1152, 59136])
  Source W_B_v shape: torch.Size([1152, 59136])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 59136 = 231 * 256 (actual v_head_dim)
  (Original tensor dimension was: 59136)
  Creating W_B_v with in_features=1152, out_features=59136
  W_B_v transposing weight from torch.Size([1152, 59136]) to torch.Size([59136, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([59136, 1152])
  Found factorized module: model.layers.10.self_attn
  Layer index: 10
  Recorded ranks for layer 10: {'q_rank': 119, 'k_rank': 178, 'v_rank': 229}
  Source W_A_q shape: torch.Size([1152, 476])
  Source W_A_q shape: torch.Size([1152, 476])
  Unknown B matrix with dimensions [out=476, in=1152]
  Creating W_A_q with in_features=1152, out_features=476
  W_A_q transposing weight from torch.Size([1152, 476]) to torch.Size([476, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([476, 1152])
  Source W_A_k shape: torch.Size([1152, 178])
  Source W_A_k shape: torch.Size([1152, 178])
  Unknown B matrix with dimensions [out=178, in=1152]
  Creating W_A_k with in_features=1152, out_features=178
  W_A_k transposing weight from torch.Size([1152, 178]) to torch.Size([178, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([178, 1152])
  Source W_A_v shape: torch.Size([1152, 229])
  Source W_A_v shape: torch.Size([1152, 229])
  Unknown B matrix with dimensions [out=229, in=1152]
  Creating W_A_v with in_features=1152, out_features=229
  W_A_v transposing weight from torch.Size([1152, 229]) to torch.Size([229, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([229, 1152])
  Source W_B_q shape: torch.Size([1152, 121856])
  Source W_B_q shape: torch.Size([1152, 121856])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 30464 = 119 * 256 (actual q_head_dim)
  (Original tensor dimension was: 121856)
  Creating W_B_q with in_features=1152, out_features=30464
  WARNING: Weight shape torch.Size([1152, 121856]) doesn't match required Linear dimensions [30464, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([121856, 1152])
  Need to resize W_B_q from 121856 to 30464
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([30464, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([30464, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([30464, 1152])
  Source W_B_k shape: torch.Size([1152, 45568])
  Source W_B_k shape: torch.Size([1152, 45568])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 45568 = 178 * 256 (actual k_head_dim)
  (Original tensor dimension was: 45568)
  Creating W_B_k with in_features=1152, out_features=45568
  W_B_k transposing weight from torch.Size([1152, 45568]) to torch.Size([45568, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([45568, 1152])
  Source W_B_v shape: torch.Size([1152, 58624])
  Source W_B_v shape: torch.Size([1152, 58624])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 58624 = 229 * 256 (actual v_head_dim)
  (Original tensor dimension was: 58624)
  Creating W_B_v with in_features=1152, out_features=58624
  W_B_v transposing weight from torch.Size([1152, 58624]) to torch.Size([58624, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([58624, 1152])
  Found factorized module: model.layers.11.self_attn
  Layer index: 11
  Recorded ranks for layer 11: {'q_rank': 85, 'k_rank': 147, 'v_rank': 223}
  Source W_A_q shape: torch.Size([1152, 340])
  Source W_A_q shape: torch.Size([1152, 340])
  Unknown B matrix with dimensions [out=340, in=1152]
  Creating W_A_q with in_features=1152, out_features=340
  W_A_q transposing weight from torch.Size([1152, 340]) to torch.Size([340, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([340, 1152])
  Source W_A_k shape: torch.Size([1152, 147])
  Source W_A_k shape: torch.Size([1152, 147])
  Unknown B matrix with dimensions [out=147, in=1152]
  Creating W_A_k with in_features=1152, out_features=147
  W_A_k transposing weight from torch.Size([1152, 147]) to torch.Size([147, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([147, 1152])
  Source W_A_v shape: torch.Size([1152, 223])
  Source W_A_v shape: torch.Size([1152, 223])
  Unknown B matrix with dimensions [out=223, in=1152]
  Creating W_A_v with in_features=1152, out_features=223
  W_A_v transposing weight from torch.Size([1152, 223]) to torch.Size([223, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([223, 1152])
  Source W_B_q shape: torch.Size([1152, 87040])
  Source W_B_q shape: torch.Size([1152, 87040])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 21760 = 85 * 256 (actual q_head_dim)
  (Original tensor dimension was: 87040)
  Creating W_B_q with in_features=1152, out_features=21760
  WARNING: Weight shape torch.Size([1152, 87040]) doesn't match required Linear dimensions [21760, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([87040, 1152])
  Need to resize W_B_q from 87040 to 21760
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([21760, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([21760, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([21760, 1152])
  Source W_B_k shape: torch.Size([1152, 37632])
  Source W_B_k shape: torch.Size([1152, 37632])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 37632 = 147 * 256 (actual k_head_dim)
  (Original tensor dimension was: 37632)
  Creating W_B_k with in_features=1152, out_features=37632
  W_B_k transposing weight from torch.Size([1152, 37632]) to torch.Size([37632, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([37632, 1152])
  Source W_B_v shape: torch.Size([1152, 57088])
  Source W_B_v shape: torch.Size([1152, 57088])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 57088 = 223 * 256 (actual v_head_dim)
  (Original tensor dimension was: 57088)
  Creating W_B_v with in_features=1152, out_features=57088
  W_B_v transposing weight from torch.Size([1152, 57088]) to torch.Size([57088, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([57088, 1152])
  Found factorized module: model.layers.12.self_attn
  Layer index: 12
  Recorded ranks for layer 12: {'q_rank': 111, 'k_rank': 173, 'v_rank': 230}
  Source W_A_q shape: torch.Size([1152, 444])
  Source W_A_q shape: torch.Size([1152, 444])
  Unknown B matrix with dimensions [out=444, in=1152]
  Creating W_A_q with in_features=1152, out_features=444
  W_A_q transposing weight from torch.Size([1152, 444]) to torch.Size([444, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([444, 1152])
  Source W_A_k shape: torch.Size([1152, 173])
  Source W_A_k shape: torch.Size([1152, 173])
  Unknown B matrix with dimensions [out=173, in=1152]
  Creating W_A_k with in_features=1152, out_features=173
  W_A_k transposing weight from torch.Size([1152, 173]) to torch.Size([173, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([173, 1152])
  Source W_A_v shape: torch.Size([1152, 230])
  Source W_A_v shape: torch.Size([1152, 230])
  Unknown B matrix with dimensions [out=230, in=1152]
  Creating W_A_v with in_features=1152, out_features=230
  W_A_v transposing weight from torch.Size([1152, 230]) to torch.Size([230, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([230, 1152])
  Source W_B_q shape: torch.Size([1152, 113664])
  Source W_B_q shape: torch.Size([1152, 113664])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 28416 = 111 * 256 (actual q_head_dim)
  (Original tensor dimension was: 113664)
  Creating W_B_q with in_features=1152, out_features=28416
  WARNING: Weight shape torch.Size([1152, 113664]) doesn't match required Linear dimensions [28416, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([113664, 1152])
  Need to resize W_B_q from 113664 to 28416
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([28416, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([28416, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([28416, 1152])
  Source W_B_k shape: torch.Size([1152, 44288])
  Source W_B_k shape: torch.Size([1152, 44288])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 44288 = 173 * 256 (actual k_head_dim)
  (Original tensor dimension was: 44288)
  Creating W_B_k with in_features=1152, out_features=44288
  W_B_k transposing weight from torch.Size([1152, 44288]) to torch.Size([44288, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([44288, 1152])
  Source W_B_v shape: torch.Size([1152, 58880])
  Source W_B_v shape: torch.Size([1152, 58880])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 58880 = 230 * 256 (actual v_head_dim)
  (Original tensor dimension was: 58880)
  Creating W_B_v with in_features=1152, out_features=58880
  W_B_v transposing weight from torch.Size([1152, 58880]) to torch.Size([58880, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([58880, 1152])
  Found factorized module: model.layers.13.self_attn
  Layer index: 13
  Recorded ranks for layer 13: {'q_rank': 109, 'k_rank': 167, 'v_rank': 230}
  Source W_A_q shape: torch.Size([1152, 436])
  Source W_A_q shape: torch.Size([1152, 436])
  Unknown B matrix with dimensions [out=436, in=1152]
  Creating W_A_q with in_features=1152, out_features=436
  W_A_q transposing weight from torch.Size([1152, 436]) to torch.Size([436, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([436, 1152])
  Source W_A_k shape: torch.Size([1152, 167])
  Source W_A_k shape: torch.Size([1152, 167])
  Unknown B matrix with dimensions [out=167, in=1152]
  Creating W_A_k with in_features=1152, out_features=167
  W_A_k transposing weight from torch.Size([1152, 167]) to torch.Size([167, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([167, 1152])
  Source W_A_v shape: torch.Size([1152, 230])
  Source W_A_v shape: torch.Size([1152, 230])
  Unknown B matrix with dimensions [out=230, in=1152]
  Creating W_A_v with in_features=1152, out_features=230
  W_A_v transposing weight from torch.Size([1152, 230]) to torch.Size([230, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([230, 1152])
  Source W_B_q shape: torch.Size([1152, 111616])
  Source W_B_q shape: torch.Size([1152, 111616])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 27904 = 109 * 256 (actual q_head_dim)
  (Original tensor dimension was: 111616)
  Creating W_B_q with in_features=1152, out_features=27904
  WARNING: Weight shape torch.Size([1152, 111616]) doesn't match required Linear dimensions [27904, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([111616, 1152])
  Need to resize W_B_q from 111616 to 27904
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([27904, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([27904, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([27904, 1152])
  Source W_B_k shape: torch.Size([1152, 42752])
  Source W_B_k shape: torch.Size([1152, 42752])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 42752 = 167 * 256 (actual k_head_dim)
  (Original tensor dimension was: 42752)
  Creating W_B_k with in_features=1152, out_features=42752
  W_B_k transposing weight from torch.Size([1152, 42752]) to torch.Size([42752, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([42752, 1152])
  Source W_B_v shape: torch.Size([1152, 58880])
  Source W_B_v shape: torch.Size([1152, 58880])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 58880 = 230 * 256 (actual v_head_dim)
  (Original tensor dimension was: 58880)
  Creating W_B_v with in_features=1152, out_features=58880
  W_B_v transposing weight from torch.Size([1152, 58880]) to torch.Size([58880, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([58880, 1152])
  Found factorized module: model.layers.14.self_attn
  Layer index: 14
  Recorded ranks for layer 14: {'q_rank': 93, 'k_rank': 160, 'v_rank': 226}
  Source W_A_q shape: torch.Size([1152, 372])
  Source W_A_q shape: torch.Size([1152, 372])
  Unknown B matrix with dimensions [out=372, in=1152]
  Creating W_A_q with in_features=1152, out_features=372
  W_A_q transposing weight from torch.Size([1152, 372]) to torch.Size([372, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([372, 1152])
  Source W_A_k shape: torch.Size([1152, 160])
  Source W_A_k shape: torch.Size([1152, 160])
  Unknown B matrix with dimensions [out=160, in=1152]
  Creating W_A_k with in_features=1152, out_features=160
  W_A_k transposing weight from torch.Size([1152, 160]) to torch.Size([160, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([160, 1152])
  Source W_A_v shape: torch.Size([1152, 226])
  Source W_A_v shape: torch.Size([1152, 226])
  Unknown B matrix with dimensions [out=226, in=1152]
  Creating W_A_v with in_features=1152, out_features=226
  W_A_v transposing weight from torch.Size([1152, 226]) to torch.Size([226, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([226, 1152])
  Source W_B_q shape: torch.Size([1152, 95232])
  Source W_B_q shape: torch.Size([1152, 95232])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 23808 = 93 * 256 (actual q_head_dim)
  (Original tensor dimension was: 95232)
  Creating W_B_q with in_features=1152, out_features=23808
  WARNING: Weight shape torch.Size([1152, 95232]) doesn't match required Linear dimensions [23808, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([95232, 1152])
  Need to resize W_B_q from 95232 to 23808
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([23808, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([23808, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([23808, 1152])
  Source W_B_k shape: torch.Size([1152, 40960])
  Source W_B_k shape: torch.Size([1152, 40960])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 40960 = 160 * 256 (actual k_head_dim)
  (Original tensor dimension was: 40960)
  Creating W_B_k with in_features=1152, out_features=40960
  W_B_k transposing weight from torch.Size([1152, 40960]) to torch.Size([40960, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([40960, 1152])
  Source W_B_v shape: torch.Size([1152, 57856])
  Source W_B_v shape: torch.Size([1152, 57856])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 57856 = 226 * 256 (actual v_head_dim)
  (Original tensor dimension was: 57856)
  Creating W_B_v with in_features=1152, out_features=57856
  W_B_v transposing weight from torch.Size([1152, 57856]) to torch.Size([57856, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([57856, 1152])
  Found factorized module: model.layers.15.self_attn
  Layer index: 15
  Recorded ranks for layer 15: {'q_rank': 113, 'k_rank': 175, 'v_rank': 224}
  Source W_A_q shape: torch.Size([1152, 452])
  Source W_A_q shape: torch.Size([1152, 452])
  Unknown B matrix with dimensions [out=452, in=1152]
  Creating W_A_q with in_features=1152, out_features=452
  W_A_q transposing weight from torch.Size([1152, 452]) to torch.Size([452, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([452, 1152])
  Source W_A_k shape: torch.Size([1152, 175])
  Source W_A_k shape: torch.Size([1152, 175])
  Unknown B matrix with dimensions [out=175, in=1152]
  Creating W_A_k with in_features=1152, out_features=175
  W_A_k transposing weight from torch.Size([1152, 175]) to torch.Size([175, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([175, 1152])
  Source W_A_v shape: torch.Size([1152, 224])
  Source W_A_v shape: torch.Size([1152, 224])
  Unknown B matrix with dimensions [out=224, in=1152]
  Creating W_A_v with in_features=1152, out_features=224
  W_A_v transposing weight from torch.Size([1152, 224]) to torch.Size([224, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([224, 1152])
  Source W_B_q shape: torch.Size([1152, 115712])
  Source W_B_q shape: torch.Size([1152, 115712])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 28928 = 113 * 256 (actual q_head_dim)
  (Original tensor dimension was: 115712)
  Creating W_B_q with in_features=1152, out_features=28928
  WARNING: Weight shape torch.Size([1152, 115712]) doesn't match required Linear dimensions [28928, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([115712, 1152])
  Need to resize W_B_q from 115712 to 28928
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([28928, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([28928, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([28928, 1152])
  Source W_B_k shape: torch.Size([1152, 44800])
  Source W_B_k shape: torch.Size([1152, 44800])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 44800 = 175 * 256 (actual k_head_dim)
  (Original tensor dimension was: 44800)
  Creating W_B_k with in_features=1152, out_features=44800
  W_B_k transposing weight from torch.Size([1152, 44800]) to torch.Size([44800, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([44800, 1152])
  Source W_B_v shape: torch.Size([1152, 57344])
  Source W_B_v shape: torch.Size([1152, 57344])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 57344 = 224 * 256 (actual v_head_dim)
  (Original tensor dimension was: 57344)
  Creating W_B_v with in_features=1152, out_features=57344
  W_B_v transposing weight from torch.Size([1152, 57344]) to torch.Size([57344, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([57344, 1152])
  Found factorized module: model.layers.16.self_attn
  Layer index: 16
  Recorded ranks for layer 16: {'q_rank': 122, 'k_rank': 186, 'v_rank': 229}
  Source W_A_q shape: torch.Size([1152, 488])
  Source W_A_q shape: torch.Size([1152, 488])
  Unknown B matrix with dimensions [out=488, in=1152]
  Creating W_A_q with in_features=1152, out_features=488
  W_A_q transposing weight from torch.Size([1152, 488]) to torch.Size([488, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([488, 1152])
  Source W_A_k shape: torch.Size([1152, 186])
  Source W_A_k shape: torch.Size([1152, 186])
  Unknown B matrix with dimensions [out=186, in=1152]
  Creating W_A_k with in_features=1152, out_features=186
  W_A_k transposing weight from torch.Size([1152, 186]) to torch.Size([186, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([186, 1152])
  Source W_A_v shape: torch.Size([1152, 229])
  Source W_A_v shape: torch.Size([1152, 229])
  Unknown B matrix with dimensions [out=229, in=1152]
  Creating W_A_v with in_features=1152, out_features=229
  W_A_v transposing weight from torch.Size([1152, 229]) to torch.Size([229, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([229, 1152])
  Source W_B_q shape: torch.Size([1152, 124928])
  Source W_B_q shape: torch.Size([1152, 124928])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 31232 = 122 * 256 (actual q_head_dim)
  (Original tensor dimension was: 124928)
  Creating W_B_q with in_features=1152, out_features=31232
  WARNING: Weight shape torch.Size([1152, 124928]) doesn't match required Linear dimensions [31232, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([124928, 1152])
  Need to resize W_B_q from 124928 to 31232
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([31232, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([31232, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([31232, 1152])
  Source W_B_k shape: torch.Size([1152, 47616])
  Source W_B_k shape: torch.Size([1152, 47616])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 47616 = 186 * 256 (actual k_head_dim)
  (Original tensor dimension was: 47616)
  Creating W_B_k with in_features=1152, out_features=47616
  W_B_k transposing weight from torch.Size([1152, 47616]) to torch.Size([47616, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([47616, 1152])
  Source W_B_v shape: torch.Size([1152, 58624])
  Source W_B_v shape: torch.Size([1152, 58624])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 58624 = 229 * 256 (actual v_head_dim)
  (Original tensor dimension was: 58624)
  Creating W_B_v with in_features=1152, out_features=58624
  W_B_v transposing weight from torch.Size([1152, 58624]) to torch.Size([58624, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([58624, 1152])
  Found factorized module: model.layers.17.self_attn
  Layer index: 17
  Recorded ranks for layer 17: {'q_rank': 88, 'k_rank': 124, 'v_rank': 226}
  Source W_A_q shape: torch.Size([1152, 352])
  Source W_A_q shape: torch.Size([1152, 352])
  Unknown B matrix with dimensions [out=352, in=1152]
  Creating W_A_q with in_features=1152, out_features=352
  W_A_q transposing weight from torch.Size([1152, 352]) to torch.Size([352, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([352, 1152])
  Source W_A_k shape: torch.Size([1152, 124])
  Source W_A_k shape: torch.Size([1152, 124])
  Unknown B matrix with dimensions [out=124, in=1152]
  Creating W_A_k with in_features=1152, out_features=124
  W_A_k transposing weight from torch.Size([1152, 124]) to torch.Size([124, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([124, 1152])
  Source W_A_v shape: torch.Size([1152, 226])
  Source W_A_v shape: torch.Size([1152, 226])
  Unknown B matrix with dimensions [out=226, in=1152]
  Creating W_A_v with in_features=1152, out_features=226
  W_A_v transposing weight from torch.Size([1152, 226]) to torch.Size([226, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([226, 1152])
  Source W_B_q shape: torch.Size([1152, 90112])
  Source W_B_q shape: torch.Size([1152, 90112])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 22528 = 88 * 256 (actual q_head_dim)
  (Original tensor dimension was: 90112)
  Creating W_B_q with in_features=1152, out_features=22528
  WARNING: Weight shape torch.Size([1152, 90112]) doesn't match required Linear dimensions [22528, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([90112, 1152])
  Need to resize W_B_q from 90112 to 22528
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([22528, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([22528, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([22528, 1152])
  Source W_B_k shape: torch.Size([1152, 31744])
  Source W_B_k shape: torch.Size([1152, 31744])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 31744 = 124 * 256 (actual k_head_dim)
  (Original tensor dimension was: 31744)
  Creating W_B_k with in_features=1152, out_features=31744
  W_B_k transposing weight from torch.Size([1152, 31744]) to torch.Size([31744, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([31744, 1152])
  Source W_B_v shape: torch.Size([1152, 57856])
  Source W_B_v shape: torch.Size([1152, 57856])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 57856 = 226 * 256 (actual v_head_dim)
  (Original tensor dimension was: 57856)
  Creating W_B_v with in_features=1152, out_features=57856
  W_B_v transposing weight from torch.Size([1152, 57856]) to torch.Size([57856, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([57856, 1152])
  Found factorized module: model.layers.18.self_attn
  Layer index: 18
  Recorded ranks for layer 18: {'q_rank': 92, 'k_rank': 152, 'v_rank': 226}
  Source W_A_q shape: torch.Size([1152, 368])
  Source W_A_q shape: torch.Size([1152, 368])
  Unknown B matrix with dimensions [out=368, in=1152]
  Creating W_A_q with in_features=1152, out_features=368
  W_A_q transposing weight from torch.Size([1152, 368]) to torch.Size([368, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([368, 1152])
  Source W_A_k shape: torch.Size([1152, 152])
  Source W_A_k shape: torch.Size([1152, 152])
  Unknown B matrix with dimensions [out=152, in=1152]
  Creating W_A_k with in_features=1152, out_features=152
  W_A_k transposing weight from torch.Size([1152, 152]) to torch.Size([152, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([152, 1152])
  Source W_A_v shape: torch.Size([1152, 226])
  Source W_A_v shape: torch.Size([1152, 226])
  Unknown B matrix with dimensions [out=226, in=1152]
  Creating W_A_v with in_features=1152, out_features=226
  W_A_v transposing weight from torch.Size([1152, 226]) to torch.Size([226, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([226, 1152])
  Source W_B_q shape: torch.Size([1152, 94208])
  Source W_B_q shape: torch.Size([1152, 94208])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 23552 = 92 * 256 (actual q_head_dim)
  (Original tensor dimension was: 94208)
  Creating W_B_q with in_features=1152, out_features=23552
  WARNING: Weight shape torch.Size([1152, 94208]) doesn't match required Linear dimensions [23552, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([94208, 1152])
  Need to resize W_B_q from 94208 to 23552
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([23552, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([23552, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([23552, 1152])
  Source W_B_k shape: torch.Size([1152, 38912])
  Source W_B_k shape: torch.Size([1152, 38912])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 38912 = 152 * 256 (actual k_head_dim)
  (Original tensor dimension was: 38912)
  Creating W_B_k with in_features=1152, out_features=38912
  W_B_k transposing weight from torch.Size([1152, 38912]) to torch.Size([38912, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([38912, 1152])
  Source W_B_v shape: torch.Size([1152, 57856])
  Source W_B_v shape: torch.Size([1152, 57856])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 57856 = 226 * 256 (actual v_head_dim)
  (Original tensor dimension was: 57856)
  Creating W_B_v with in_features=1152, out_features=57856
  W_B_v transposing weight from torch.Size([1152, 57856]) to torch.Size([57856, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([57856, 1152])
  Found factorized module: model.layers.19.self_attn
  Layer index: 19
  Recorded ranks for layer 19: {'q_rank': 138, 'k_rank': 206, 'v_rank': 227}
  Source W_A_q shape: torch.Size([1152, 552])
  Source W_A_q shape: torch.Size([1152, 552])
  Unknown B matrix with dimensions [out=552, in=1152]
  Creating W_A_q with in_features=1152, out_features=552
  W_A_q transposing weight from torch.Size([1152, 552]) to torch.Size([552, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([552, 1152])
  Source W_A_k shape: torch.Size([1152, 206])
  Source W_A_k shape: torch.Size([1152, 206])
  Unknown B matrix with dimensions [out=206, in=1152]
  Creating W_A_k with in_features=1152, out_features=206
  W_A_k transposing weight from torch.Size([1152, 206]) to torch.Size([206, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([206, 1152])
  Source W_A_v shape: torch.Size([1152, 227])
  Source W_A_v shape: torch.Size([1152, 227])
  Unknown B matrix with dimensions [out=227, in=1152]
  Creating W_A_v with in_features=1152, out_features=227
  W_A_v transposing weight from torch.Size([1152, 227]) to torch.Size([227, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([227, 1152])
  Source W_B_q shape: torch.Size([1152, 141312])
  Source W_B_q shape: torch.Size([1152, 141312])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 35328 = 138 * 256 (actual q_head_dim)
  (Original tensor dimension was: 141312)
  Creating W_B_q with in_features=1152, out_features=35328
  WARNING: Weight shape torch.Size([1152, 141312]) doesn't match required Linear dimensions [35328, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([141312, 1152])
  Need to resize W_B_q from 141312 to 35328
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([35328, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([35328, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([35328, 1152])
  Source W_B_k shape: torch.Size([1152, 52736])
  Source W_B_k shape: torch.Size([1152, 52736])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 52736 = 206 * 256 (actual k_head_dim)
  (Original tensor dimension was: 52736)
  Creating W_B_k with in_features=1152, out_features=52736
  W_B_k transposing weight from torch.Size([1152, 52736]) to torch.Size([52736, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([52736, 1152])
  Source W_B_v shape: torch.Size([1152, 58112])
  Source W_B_v shape: torch.Size([1152, 58112])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 58112 = 227 * 256 (actual v_head_dim)
  (Original tensor dimension was: 58112)
  Creating W_B_v with in_features=1152, out_features=58112
  W_B_v transposing weight from torch.Size([1152, 58112]) to torch.Size([58112, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([58112, 1152])
  Found factorized module: model.layers.20.self_attn
  Layer index: 20
  Recorded ranks for layer 20: {'q_rank': 123, 'k_rank': 183, 'v_rank': 232}
  Source W_A_q shape: torch.Size([1152, 492])
  Source W_A_q shape: torch.Size([1152, 492])
  Unknown B matrix with dimensions [out=492, in=1152]
  Creating W_A_q with in_features=1152, out_features=492
  W_A_q transposing weight from torch.Size([1152, 492]) to torch.Size([492, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([492, 1152])
  Source W_A_k shape: torch.Size([1152, 183])
  Source W_A_k shape: torch.Size([1152, 183])
  Unknown B matrix with dimensions [out=183, in=1152]
  Creating W_A_k with in_features=1152, out_features=183
  W_A_k transposing weight from torch.Size([1152, 183]) to torch.Size([183, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([183, 1152])
  Source W_A_v shape: torch.Size([1152, 232])
  Source W_A_v shape: torch.Size([1152, 232])
  Unknown B matrix with dimensions [out=232, in=1152]
  Creating W_A_v with in_features=1152, out_features=232
  W_A_v transposing weight from torch.Size([1152, 232]) to torch.Size([232, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([232, 1152])
  Source W_B_q shape: torch.Size([1152, 125952])
  Source W_B_q shape: torch.Size([1152, 125952])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 31488 = 123 * 256 (actual q_head_dim)
  (Original tensor dimension was: 125952)
  Creating W_B_q with in_features=1152, out_features=31488
  WARNING: Weight shape torch.Size([1152, 125952]) doesn't match required Linear dimensions [31488, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([125952, 1152])
  Need to resize W_B_q from 125952 to 31488
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([31488, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([31488, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([31488, 1152])
  Source W_B_k shape: torch.Size([1152, 46848])
  Source W_B_k shape: torch.Size([1152, 46848])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 46848 = 183 * 256 (actual k_head_dim)
  (Original tensor dimension was: 46848)
  Creating W_B_k with in_features=1152, out_features=46848
  W_B_k transposing weight from torch.Size([1152, 46848]) to torch.Size([46848, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([46848, 1152])
  Source W_B_v shape: torch.Size([1152, 59392])
  Source W_B_v shape: torch.Size([1152, 59392])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 59392 = 232 * 256 (actual v_head_dim)
  (Original tensor dimension was: 59392)
  Creating W_B_v with in_features=1152, out_features=59392
  W_B_v transposing weight from torch.Size([1152, 59392]) to torch.Size([59392, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([59392, 1152])
  Found factorized module: model.layers.21.self_attn
  Layer index: 21
  Recorded ranks for layer 21: {'q_rank': 126, 'k_rank': 192, 'v_rank': 228}
  Source W_A_q shape: torch.Size([1152, 504])
  Source W_A_q shape: torch.Size([1152, 504])
  Unknown B matrix with dimensions [out=504, in=1152]
  Creating W_A_q with in_features=1152, out_features=504
  W_A_q transposing weight from torch.Size([1152, 504]) to torch.Size([504, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([504, 1152])
  Source W_A_k shape: torch.Size([1152, 192])
  Source W_A_k shape: torch.Size([1152, 192])
  Unknown B matrix with dimensions [out=192, in=1152]
  Creating W_A_k with in_features=1152, out_features=192
  W_A_k transposing weight from torch.Size([1152, 192]) to torch.Size([192, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([192, 1152])
  Source W_A_v shape: torch.Size([1152, 228])
  Source W_A_v shape: torch.Size([1152, 228])
  Unknown B matrix with dimensions [out=228, in=1152]
  Creating W_A_v with in_features=1152, out_features=228
  W_A_v transposing weight from torch.Size([1152, 228]) to torch.Size([228, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([228, 1152])
  Source W_B_q shape: torch.Size([1152, 129024])
  Source W_B_q shape: torch.Size([1152, 129024])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 32256 = 126 * 256 (actual q_head_dim)
  (Original tensor dimension was: 129024)
  Creating W_B_q with in_features=1152, out_features=32256
  WARNING: Weight shape torch.Size([1152, 129024]) doesn't match required Linear dimensions [32256, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([129024, 1152])
  Need to resize W_B_q from 129024 to 32256
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([32256, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([32256, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([32256, 1152])
  Source W_B_k shape: torch.Size([1152, 49152])
  Source W_B_k shape: torch.Size([1152, 49152])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 49152 = 192 * 256 (actual k_head_dim)
  (Original tensor dimension was: 49152)
  Creating W_B_k with in_features=1152, out_features=49152
  W_B_k transposing weight from torch.Size([1152, 49152]) to torch.Size([49152, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([49152, 1152])
  Source W_B_v shape: torch.Size([1152, 58368])
  Source W_B_v shape: torch.Size([1152, 58368])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 58368 = 228 * 256 (actual v_head_dim)
  (Original tensor dimension was: 58368)
  Creating W_B_v with in_features=1152, out_features=58368
  W_B_v transposing weight from torch.Size([1152, 58368]) to torch.Size([58368, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([58368, 1152])
  Found factorized module: model.layers.22.self_attn
  Layer index: 22
  Recorded ranks for layer 22: {'q_rank': 130, 'k_rank': 205, 'v_rank': 233}
  Source W_A_q shape: torch.Size([1152, 520])
  Source W_A_q shape: torch.Size([1152, 520])
  Unknown B matrix with dimensions [out=520, in=1152]
  Creating W_A_q with in_features=1152, out_features=520
  W_A_q transposing weight from torch.Size([1152, 520]) to torch.Size([520, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([520, 1152])
  Source W_A_k shape: torch.Size([1152, 205])
  Source W_A_k shape: torch.Size([1152, 205])
  Unknown B matrix with dimensions [out=205, in=1152]
  Creating W_A_k with in_features=1152, out_features=205
  W_A_k transposing weight from torch.Size([1152, 205]) to torch.Size([205, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([205, 1152])
  Source W_A_v shape: torch.Size([1152, 233])
  Source W_A_v shape: torch.Size([1152, 233])
  Unknown B matrix with dimensions [out=233, in=1152]
  Creating W_A_v with in_features=1152, out_features=233
  W_A_v transposing weight from torch.Size([1152, 233]) to torch.Size([233, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([233, 1152])
  Source W_B_q shape: torch.Size([1152, 133120])
  Source W_B_q shape: torch.Size([1152, 133120])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 33280 = 130 * 256 (actual q_head_dim)
  (Original tensor dimension was: 133120)
  Creating W_B_q with in_features=1152, out_features=33280
  WARNING: Weight shape torch.Size([1152, 133120]) doesn't match required Linear dimensions [33280, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([133120, 1152])
  Need to resize W_B_q from 133120 to 33280
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([33280, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([33280, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([33280, 1152])
  Source W_B_k shape: torch.Size([1152, 52480])
  Source W_B_k shape: torch.Size([1152, 52480])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 52480 = 205 * 256 (actual k_head_dim)
  (Original tensor dimension was: 52480)
  Creating W_B_k with in_features=1152, out_features=52480
  W_B_k transposing weight from torch.Size([1152, 52480]) to torch.Size([52480, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([52480, 1152])
  Source W_B_v shape: torch.Size([1152, 59648])
  Source W_B_v shape: torch.Size([1152, 59648])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 59648 = 233 * 256 (actual v_head_dim)
  (Original tensor dimension was: 59648)
  Creating W_B_v with in_features=1152, out_features=59648
  W_B_v transposing weight from torch.Size([1152, 59648]) to torch.Size([59648, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([59648, 1152])
  Found factorized module: model.layers.23.self_attn
  Layer index: 23
  Recorded ranks for layer 23: {'q_rank': 104, 'k_rank': 163, 'v_rank': 222}
  Source W_A_q shape: torch.Size([1152, 416])
  Source W_A_q shape: torch.Size([1152, 416])
  Unknown B matrix with dimensions [out=416, in=1152]
  Creating W_A_q with in_features=1152, out_features=416
  W_A_q transposing weight from torch.Size([1152, 416]) to torch.Size([416, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([416, 1152])
  Source W_A_k shape: torch.Size([1152, 163])
  Source W_A_k shape: torch.Size([1152, 163])
  Unknown B matrix with dimensions [out=163, in=1152]
  Creating W_A_k with in_features=1152, out_features=163
  W_A_k transposing weight from torch.Size([1152, 163]) to torch.Size([163, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([163, 1152])
  Source W_A_v shape: torch.Size([1152, 222])
  Source W_A_v shape: torch.Size([1152, 222])
  Unknown B matrix with dimensions [out=222, in=1152]
  Creating W_A_v with in_features=1152, out_features=222
  W_A_v transposing weight from torch.Size([1152, 222]) to torch.Size([222, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([222, 1152])
  Source W_B_q shape: torch.Size([1152, 106496])
  Source W_B_q shape: torch.Size([1152, 106496])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 26624 = 104 * 256 (actual q_head_dim)
  (Original tensor dimension was: 106496)
  Creating W_B_q with in_features=1152, out_features=26624
  WARNING: Weight shape torch.Size([1152, 106496]) doesn't match required Linear dimensions [26624, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([106496, 1152])
  Need to resize W_B_q from 106496 to 26624
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([26624, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([26624, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([26624, 1152])
  Source W_B_k shape: torch.Size([1152, 41728])
  Source W_B_k shape: torch.Size([1152, 41728])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 41728 = 163 * 256 (actual k_head_dim)
  (Original tensor dimension was: 41728)
  Creating W_B_k with in_features=1152, out_features=41728
  W_B_k transposing weight from torch.Size([1152, 41728]) to torch.Size([41728, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([41728, 1152])
  Source W_B_v shape: torch.Size([1152, 56832])
  Source W_B_v shape: torch.Size([1152, 56832])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 56832 = 222 * 256 (actual v_head_dim)
  (Original tensor dimension was: 56832)
  Creating W_B_v with in_features=1152, out_features=56832
  W_B_v transposing weight from torch.Size([1152, 56832]) to torch.Size([56832, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([56832, 1152])
  Found factorized module: model.layers.24.self_attn
  Layer index: 24
  Recorded ranks for layer 24: {'q_rank': 131, 'k_rank': 194, 'v_rank': 229}
  Source W_A_q shape: torch.Size([1152, 524])
  Source W_A_q shape: torch.Size([1152, 524])
  Unknown B matrix with dimensions [out=524, in=1152]
  Creating W_A_q with in_features=1152, out_features=524
  W_A_q transposing weight from torch.Size([1152, 524]) to torch.Size([524, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([524, 1152])
  Source W_A_k shape: torch.Size([1152, 194])
  Source W_A_k shape: torch.Size([1152, 194])
  Unknown B matrix with dimensions [out=194, in=1152]
  Creating W_A_k with in_features=1152, out_features=194
  W_A_k transposing weight from torch.Size([1152, 194]) to torch.Size([194, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([194, 1152])
  Source W_A_v shape: torch.Size([1152, 229])
  Source W_A_v shape: torch.Size([1152, 229])
  Unknown B matrix with dimensions [out=229, in=1152]
  Creating W_A_v with in_features=1152, out_features=229
  W_A_v transposing weight from torch.Size([1152, 229]) to torch.Size([229, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([229, 1152])
  Source W_B_q shape: torch.Size([1152, 134144])
  Source W_B_q shape: torch.Size([1152, 134144])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 33536 = 131 * 256 (actual q_head_dim)
  (Original tensor dimension was: 134144)
  Creating W_B_q with in_features=1152, out_features=33536
  WARNING: Weight shape torch.Size([1152, 134144]) doesn't match required Linear dimensions [33536, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([134144, 1152])
  Need to resize W_B_q from 134144 to 33536
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([33536, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([33536, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([33536, 1152])
  Source W_B_k shape: torch.Size([1152, 49664])
  Source W_B_k shape: torch.Size([1152, 49664])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 49664 = 194 * 256 (actual k_head_dim)
  (Original tensor dimension was: 49664)
  Creating W_B_k with in_features=1152, out_features=49664
  W_B_k transposing weight from torch.Size([1152, 49664]) to torch.Size([49664, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([49664, 1152])
  Source W_B_v shape: torch.Size([1152, 58624])
  Source W_B_v shape: torch.Size([1152, 58624])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 58624 = 229 * 256 (actual v_head_dim)
  (Original tensor dimension was: 58624)
  Creating W_B_v with in_features=1152, out_features=58624
  W_B_v transposing weight from torch.Size([1152, 58624]) to torch.Size([58624, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([58624, 1152])
  Found factorized module: model.layers.25.self_attn
  Layer index: 25
  Recorded ranks for layer 25: {'q_rank': 116, 'k_rank': 193, 'v_rank': 225}
  Source W_A_q shape: torch.Size([1152, 464])
  Source W_A_q shape: torch.Size([1152, 464])
  Unknown B matrix with dimensions [out=464, in=1152]
  Creating W_A_q with in_features=1152, out_features=464
  W_A_q transposing weight from torch.Size([1152, 464]) to torch.Size([464, 1152])
  Converted W_A_q to dtype torch.bfloat16
  Created W_A_q with shape torch.Size([464, 1152])
  Source W_A_k shape: torch.Size([1152, 193])
  Source W_A_k shape: torch.Size([1152, 193])
  Unknown B matrix with dimensions [out=193, in=1152]
  Creating W_A_k with in_features=1152, out_features=193
  W_A_k transposing weight from torch.Size([1152, 193]) to torch.Size([193, 1152])
  Converted W_A_k to dtype torch.bfloat16
  Created W_A_k with shape torch.Size([193, 1152])
  Source W_A_v shape: torch.Size([1152, 225])
  Source W_A_v shape: torch.Size([1152, 225])
  Unknown B matrix with dimensions [out=225, in=1152]
  Creating W_A_v with in_features=1152, out_features=225
  W_A_v transposing weight from torch.Size([1152, 225]) to torch.Size([225, 1152])
  Converted W_A_v to dtype torch.bfloat16
  Created W_A_v with shape torch.Size([225, 1152])
  Source W_B_q shape: torch.Size([1152, 118784])
  Source W_B_q shape: torch.Size([1152, 118784])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_q Linear using explicit dimensions: 29696 = 116 * 256 (actual q_head_dim)
  (Original tensor dimension was: 118784)
  Creating W_B_q with in_features=1152, out_features=29696
  WARNING: Weight shape torch.Size([1152, 118784]) doesn't match required Linear dimensions [29696, 1152] for contextual factorization
  Using optimally derived W_B_q projection weights from Tucker decomposition
  Original derived W_B_q from factorized_weights has shape torch.Size([118784, 1152])
  Need to resize W_B_q from 118784 to 29696
  Detected 4 heads worth of weights, extracting first head portion
  Extracted weights with shape torch.Size([29696, 1152])
  W_B_q resized weight to match required dimensions: torch.Size([29696, 1152])
  Converted W_B_q to dtype torch.bfloat16
  Created W_B_q with shape torch.Size([29696, 1152])
  Source W_B_k shape: torch.Size([1152, 49408])
  Source W_B_k shape: torch.Size([1152, 49408])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_k Linear using explicit dimensions: 49408 = 193 * 256 (actual k_head_dim)
  (Original tensor dimension was: 49408)
  Creating W_B_k with in_features=1152, out_features=49408
  W_B_k transposing weight from torch.Size([1152, 49408]) to torch.Size([49408, 1152])
  Converted W_B_k to dtype torch.bfloat16
  Created W_B_k with shape torch.Size([49408, 1152])
  Source W_B_v shape: torch.Size([1152, 57600])
  Source W_B_v shape: torch.Size([1152, 57600])
  Creating Linear layer for B matrix in TPA contextual factorization
  W_B_v Linear using explicit dimensions: 57600 = 225 * 256 (actual v_head_dim)
  (Original tensor dimension was: 57600)
  Creating W_B_v with in_features=1152, out_features=57600
  W_B_v transposing weight from torch.Size([1152, 57600]) to torch.Size([57600, 1152])
  Converted W_B_v to dtype torch.bfloat16
  Created W_B_v with shape torch.Size([57600, 1152])
Storing layer-specific ranks in model config: [{'q_rank': 134, 'k_rank': 202, 'v_rank': 220}, {'q_rank': 129, 'k_rank': 212, 'v_rank': 228}, {'q_rank': 138, 'k_rank': 206, 'v_rank': 229}, {'q_rank': 132, 'k_rank': 192, 'v_rank': 233}, {'q_rank': 119, 'k_rank': 189, 'v_rank': 230}, {'q_rank': 93, 'k_rank': 162, 'v_rank': 219}, {'q_rank': 128, 'k_rank': 188, 'v_rank': 235}, {'q_rank': 119, 'k_rank': 180, 'v_rank': 230}, {'q_rank': 108, 'k_rank': 162, 'v_rank': 231}, {'q_rank': 116, 'k_rank': 172, 'v_rank': 231}, {'q_rank': 119, 'k_rank': 178, 'v_rank': 229}, {'q_rank': 85, 'k_rank': 147, 'v_rank': 223}, {'q_rank': 111, 'k_rank': 173, 'v_rank': 230}, {'q_rank': 109, 'k_rank': 167, 'v_rank': 230}, {'q_rank': 93, 'k_rank': 160, 'v_rank': 226}, {'q_rank': 113, 'k_rank': 175, 'v_rank': 224}, {'q_rank': 122, 'k_rank': 186, 'v_rank': 229}, {'q_rank': 88, 'k_rank': 124, 'v_rank': 226}, {'q_rank': 92, 'k_rank': 152, 'v_rank': 226}, {'q_rank': 138, 'k_rank': 206, 'v_rank': 227}, {'q_rank': 123, 'k_rank': 183, 'v_rank': 232}, {'q_rank': 126, 'k_rank': 192, 'v_rank': 228}, {'q_rank': 130, 'k_rank': 205, 'v_rank': 233}, {'q_rank': 104, 'k_rank': 163, 'v_rank': 222}, {'q_rank': 131, 'k_rank': 194, 'v_rank': 229}, {'q_rank': 116, 'k_rank': 193, 'v_rank': 225}]
  Found factorized module: model.layers.0.self_attn
  Found factorized module: model.layers.1.self_attn
  Found factorized module: model.layers.2.self_attn
  Found factorized module: model.layers.3.self_attn
  Found factorized module: model.layers.4.self_attn
  Found factorized module: model.layers.5.self_attn
  Found factorized module: model.layers.6.self_attn
  Found factorized module: model.layers.7.self_attn
  Found factorized module: model.layers.8.self_attn
  Found factorized module: model.layers.9.self_attn
  Found factorized module: model.layers.10.self_attn
  Found factorized module: model.layers.11.self_attn
  Found factorized module: model.layers.12.self_attn
  Found factorized module: model.layers.13.self_attn
  Found factorized module: model.layers.14.self_attn
  Found factorized module: model.layers.15.self_attn
  Found factorized module: model.layers.16.self_attn
  Found factorized module: model.layers.17.self_attn
  Found factorized module: model.layers.18.self_attn
  Found factorized module: model.layers.19.self_attn
  Found factorized module: model.layers.20.self_attn
  Found factorized module: model.layers.21.self_attn
  Found factorized module: model.layers.22.self_attn
  Found factorized module: model.layers.23.self_attn
  Found factorized module: model.layers.24.self_attn
  Found factorized module: model.layers.25.self_attn
TPA model creation co5mplete in 54.37 seconds
Converted GQA -> TPA successfully.
Conversion to TPA took 57.11 seconds
Saving TPA model to tpa_model.pt ...
TPA model saved successfully.
Model is on device=cuda and ready (load time 69.53s).
Inference with temperature=0.9, top_p=0.95, top_k=64 ...
DEBUG TPA FORWARD: input_token_ids shape: torch.Size([1, 2]), device: cuda:0
DEBUG TPA FORWARD: input_positions: tensor([0, 1], device='cuda:0'), device: cuda:0
Moving freqs_cis from cpu to cuda:0
DEBUG TPA FORWARD: freqs_cis shapes - LOCAL: torch.Size([2, 128]), GLOBAL: torch.Size([2, 128])
Moving embedding weights from cpu to cuda:0
DEBUG TPA FORWARD: After embedding, hidden_states shape: torch.Size([1, 2, 1152]), mean: 0.000923, std: 0.029297
Moving model from cpu to cuda:0
DEBUG TPA FORWARD: After normalization, hidden_states shape: torch.Size([1, 2, 1152]), mean: 0.031250, std: 0.996094
DEBUG TPA FORWARD: kv_caches length: 26
DEBUG TPA FORWARD: First KV cache shapes: k=torch.Size([1, 102, 4, 256]), v=torch.Size([1, 102, 4, 256])
DEBUG TPA FORWARD: mask shape: torch.Size([1, 1, 2, 102]), device: cuda:0
Traceback (most recent call last):
  File "/workspace/gemma_pytorch_tpa/scripts/run_tpa.py", line 352, in <module>
    app.run(main)
  File "/venv/py312/lib/python3.12/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/venv/py312/lib/python3.12/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
             ^^^^^^^^^^
  File "/workspace/gemma_pytorch_tpa/scripts/run_tpa.py", line 316, in main
    output_text = model.generate(
                  ^^^^^^^^^^^^^^^
  File "/workspace/gemma_pytorch_tpa/gemma/tpa/gemma3_tpa_model.py", line 711, in generate
    next_token_ids, _ = self(
                        ^^^^^
  File "/venv/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/gemma_pytorch_tpa/gemma/tpa/gemma3_tpa_model.py", line 568, in forward
    hidden_states = self.model(
                    ^^^^^^^^^^^
  File "/venv/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/gemma_pytorch_tpa/gemma/tpa/gemma3_tpa_model.py", line 454, in forward
    hidden_states = layer(
                    ^^^^^^
  File "/venv/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/gemma_pytorch_tpa/gemma/tpa/gemma3_tpa_model.py", line 396, in forward
    hidden_states = self.self_attn(
                    ^^^^^^^^^^^^^^^
  File "/venv/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/gemma_pytorch_tpa/gemma/tpa/gemma3_tpa_model.py", line 268, in forward
    attn_weights = attn_weights + mask
                   ~~~~~~~~~~~~~^~~~~~
RuntimeError: The size of tensor a (2) must match the size of tensor b (102) at non-singleton dimension 3
